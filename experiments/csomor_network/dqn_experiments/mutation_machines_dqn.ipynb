{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ERC Starting Grant on COeXISTENCE between humans and machines in urban mobility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/img_mileston1.png\" alt=\"Milestone 1 Image\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Machine training using Double DQN algorithm\n",
    "## Name: Anastasia\n",
    "### Date: July 11, 2024\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "> In this notebook, we implement the training of independent machine agents using the DQN algorithm.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "> The purpose of this notebook is to understand whether DQN algorithm can train effectively our RL agents.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Summary\n",
    "\n",
    "### Network Architecture\n",
    "- Csomor network\n",
    "---\n",
    "\n",
    "### Agents\n",
    "| **Type**          |           |\n",
    "|-------------------|---------------------|\n",
    "| **Number**        | 5 machines |\n",
    "| **Total demand** | random |\n",
    "---\n",
    "\n",
    "\n",
    "### Origin and Destination Details\n",
    "| **Origin Count**      | 2                            |\n",
    "|-----------------------|------------------------------|\n",
    "| **Destination Count** | 2                            |\n",
    "| **Origin Pairing**    | 279952229#0, 115604053       |\n",
    "| **Destination Pairing**| -115602933#2, -441496282#1     |\n",
    "---\n",
    "\n",
    "### Execution time\n",
    "- 8 min 28 sec\n",
    "\n",
    "### Hardware Utilized for Experiment Execution\n",
    "| **Type of Machine** | Personal computer (or server) |\n",
    "|----------------------|-------------------------------|\n",
    "| **CPU**              | 12th Gen Intel(R) Core(TM) i7-1255U |\n",
    "|                      | Cores: 10                   |\n",
    "|                      | Sockets: 1                  |\n",
    "|                      | Base Speed: 1.70 GHz        |\n",
    "| **Memory**           | 16GB                          |\n",
    "| **Disc (SSD)**       | 477 GB                        |\n",
    "| **Operating System** | Windows 11                    |\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "import torch\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.modules import MLP, QValueActor\n",
    "from torchrl.data import CompositeSpec\n",
    "from torchrl.modules import EGreedyModule\n",
    "from torchrl.objectives import DQNLoss, HardUpdate, SoftUpdate\n",
    "from torchrl.record.loggers import generate_exp_name, get_logger\n",
    "from torchrl.envs.transforms import RenameTransform\n",
    "from torchrl.modules.tensordict_module import QValueModule\n",
    "from torchrl.trainers import (\n",
    "    LogReward,\n",
    "    Recorder,\n",
    "    ReplayBufferTrainer,\n",
    "    Trainer,\n",
    "    UpdateWeights,\n",
    ")\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_of_parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir))\n",
    "sys.path.append(parent_of_parent_dir)\n",
    "\n",
    "from environment import TrafficEnvironment\n",
    "from keychain import Keychain as kc\n",
    "from services.plotter import Plotter\n",
    "from utilities import get_params\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "vmas_device = device  # The device where the simulator is run\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 4  # Number of team frames collected per training iteration\n",
    "n_iters = 4  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 100  # Number of optimization steps per training iteration\n",
    "minibatch_size = 2  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# DQN\n",
    "gamma = 0.99  # discount factor\n",
    "hard_update_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(kc.PARAMS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(params[kc.RUNNER], params[kc.ENVIRONMENT], params[kc.SIMULATOR], params[kc.AGENT_GEN], params[kc.AGENTS], params[kc.PHASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode [] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the simulator\n"
     ]
    }
   ],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True,\n",
    "    group_map=None,\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3', 'episode_reward')]\n"
     ]
    }
   ],
   "source": [
    "out_keys = []\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "    out_keys.append((group, \"episode_reward\"))\n",
    "\n",
    "print(out_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(\n",
    "        in_keys=env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(env.group_map.keys()),\n",
    "        out_keys = out_keys\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 'reward')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['3'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.group_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 14:23:06,227 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the simulator\n"
     ]
    }
   ],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = False \n",
    "\n",
    "    mlp = MultiAgentMLP(\n",
    "        n_agent_inputs =env.observation_spec[group, \"observation\"].shape[-1],  \n",
    "        n_agent_outputs = env.full_action_spec[group, \"action\"].space.n, \n",
    "        n_agents = len(agents),\n",
    "        centralised=False,  \n",
    "        share_params = share_parameters_policy,\n",
    "        device = device,\n",
    "        depth = 4,\n",
    "        num_cells = 64,\n",
    "        activation_class=torch.nn.ReLU,\n",
    "    )\n",
    "\n",
    "    module = TensorDictModule(mlp, \n",
    "                              in_keys=[(group, \"observation\")],\n",
    "                              out_keys=[(group,\"action_value\")],\n",
    "    )\n",
    "\n",
    "    modules[group] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_modules = {}\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "\n",
    "    q_value_module = QValueModule(\n",
    "            action_value_key=(group, \"action_value\"),\n",
    "            out_keys=[\n",
    "                (group, \"action\"),\n",
    "                (group, \"action_value\"),\n",
    "                (group, \"chosen_action_value\"),\n",
    "            ],\n",
    "            spec=env.full_action_spec[group, \"action\"],\n",
    "            action_space=None,\n",
    "        )\n",
    "\n",
    "    q_value_modules[group] = q_value_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TensorDictSequential(*modules.values(), *q_value_modules.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a random rollout to ensure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, agents in env.group_map.items():\n",
    "\n",
    "    tensordict = env.fake_tensordict()\n",
    "    policy(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_module = {}\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "\n",
    "    greedy_module[group] = EGreedyModule(\n",
    "        action_key = (group, \"action\"),\n",
    "        spec=env.full_action_spec[group, \"action\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate the greedy module inside the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_policy = {}\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "    col_policy[group] = TensorDictSequential(policy, greedy_module[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_policies = TensorDictSequential(*col_policy.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the simulator\n"
     ]
    }
   ],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    col_policies,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    reset_at_each_iter=False,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffers = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    replay_buffers[group] = ReplayBuffer(\n",
    "        storage=LazyTensorStorage(\n",
    "            frames_per_batch, device=device\n",
    "        ), \n",
    "        batch_size=minibatch_size, \n",
    "    )\n",
    "\n",
    "replay_buffer = replay_buffers[group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "optimizers = {}\n",
    "target_net_updaters = {}\n",
    "\n",
    "\n",
    "for group, _agents in env.group_map.items():\n",
    "    loss_module = DQNLoss(\n",
    "        value_network=col_policies,\n",
    "        loss_function=\"l2\",\n",
    "        double_dqn = False,\n",
    "        delay_value=True,\n",
    "        action_space = \"categorical\"\n",
    "    )\n",
    "\n",
    "    loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "        reward=(group, \"reward\"),  \n",
    "        action_value=(group, \"action_value\"),\n",
    "        action=(group, \"action\"), \n",
    "        done=(group, \"done\"),\n",
    "        terminated=(group, \"terminated\"),\n",
    "        value=(group, \"chosen_action_value\"),\n",
    "    )\n",
    "\n",
    "    loss_module.make_value_estimator(gamma=gamma)\n",
    "\n",
    "    target_net_updaters[group] = SoftUpdate(\n",
    "        loss_module, eps=0.98\n",
    "    )    \n",
    "\n",
    "    losses[group] = loss_module\n",
    "\n",
    "    optimizer = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "    \n",
    "    optimizers[group] = optimizer\n",
    "\n",
    "# Access loss module for the first group for example\n",
    "group = next(iter(env.group_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to implement the Trainer class ~ probably not working for multi agent scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"n_optim = 8\n",
    "trainer = Trainer(\n",
    "    collector=collector,\n",
    "    total_frames=total_frames,\n",
    "    frame_skip=1,\n",
    "    loss_module=loss_module,\n",
    "    optimizer=optimizer,\n",
    "    optim_steps_per_batch=n_optim,\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"log_keys = []\n",
    "out_keys = {}\n",
    "\n",
    "for group, _agents in env.group_map.items():\n",
    "    log_keys.append((\"next\", group, \"reward\"))\n",
    "    out_keys[(\"next\", group, \"reward\")] = \"rewards\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"buffer_hook = ReplayBufferTrainer(\n",
    "    replay_buffer,\n",
    "    flatten_tensordicts=False,\n",
    ")\n",
    "#buffer_hook.register(trainer)\n",
    "weight_updater = UpdateWeights(collector, update_weights_interval=1)\n",
    "weight_updater.register(trainer)\n",
    "recorder = Recorder(\n",
    "    record_interval=1,  # log every 100 optimization steps\n",
    "    record_frames=1,  # maximum number of frames in the record\n",
    "    frame_skip=1,\n",
    "    policy_exploration=col_policies,\n",
    "    environment=env,\n",
    "    #exploration_type=ExplorationType.MODE,\n",
    "    log_keys=log_keys,\n",
    "    out_keys=out_keys,\n",
    "    log_pbar=True,\n",
    ")\n",
    "recorder.register(trainer)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['6'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  6 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['6'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  6 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['6'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  6 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['6'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  6 \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['6'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  6 \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rewards: -1.7333:  25%|██▌       | 4/16 [00:06<00:18,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------STEP------------------\n",
      "\n"
     ]
    },
    {
     "ename": "FatalTraCIError",
     "evalue": "Not connected.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\trainers\\trainers.py:447\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_frames)\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pbar_str \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 447\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollector\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_frames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:953\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_frames:\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 953\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tensordict_out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total_frames:\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\_utils.py:470\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[1;32m--> 470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:1071\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1070\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[1;32m-> 1071\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\common.py:2576\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_and_maybe_reset\u001b[39m(\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase\n\u001b[0;32m   2536\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001b[39;00m\n\u001b[0;32m   2538\u001b[0m \n\u001b[0;32m   2539\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;124;03m            is_shared=False)\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2576\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m     tensordict_ \u001b[38;5;241m=\u001b[39m step_mdp(\n\u001b[0;32m   2580\u001b[0m         tensordict,\n\u001b[0;32m   2581\u001b[0m         keep_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2586\u001b[0m         done_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_keys,\n\u001b[0;32m   2587\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\common.py:1408\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[0;32m   1407\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1408\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1409\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\transforms\\transforms.py:738\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    736\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    737\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[1;32m--> 738\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    744\u001b[0m         next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    745\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:616\u001b[0m, in \u001b[0;36mPettingZooWrapper._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     (\n\u001b[0;32m    603\u001b[0m         observation_dict,\n\u001b[0;32m    604\u001b[0m         rewards_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         info_dict,\n\u001b[0;32m    608\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_parallel(tensordict)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m     (\n\u001b[0;32m    611\u001b[0m         observation_dict,\n\u001b[0;32m    612\u001b[0m         rewards_dict,\n\u001b[0;32m    613\u001b[0m         terminations_dict,\n\u001b[0;32m    614\u001b[0m         truncations_dict,\n\u001b[0;32m    615\u001b[0m         info_dict,\n\u001b[1;32m--> 616\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_aec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# We start with zeroed data and fill in the data for alive agents\u001b[39;00m\n\u001b[0;32m    620\u001b[0m tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_step_output_zero\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:766\u001b[0m, in \u001b[0;36mPettingZooWrapper._step_aec\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    763\u001b[0m         action \u001b[38;5;241m=\u001b[39m group_action_np[agent_index]\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m terminations_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mterminations\n\u001b[0;32m    768\u001b[0m truncations_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mtruncations\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\OneDrive - Uniwersytet Jagielloński\\Documents\\torch-rl-trials\\Milestone-One\\environment\\environment.py:220\u001b[0m, in \u001b[0;36mTrafficEnvironment.step\u001b[1;34m(self, machine_action)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# The cumulative reward of the last agent must be 0\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cumulative_rewards[agent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmachine_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Collect rewards if it is the last agent to act\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_selector\u001b[38;5;241m.\u001b[39mis_last(): \n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\OneDrive - Uniwersytet Jagielloński\\Documents\\torch-rl-trials\\Milestone-One\\environment\\environment.py:462\u001b[0m, in \u001b[0;36mTrafficEnvironment.simulation_loop\u001b[1;34m(self, machine_action, machine_id)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# If all machines that have start time as the simulator timestep acted\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine_same_start_time: \n\u001b[1;32m--> 462\u001b[0m     travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelp_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_dict \u001b[38;5;129;01min\u001b[39;00m travel_times:\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtravel_times_list\u001b[38;5;241m.\u001b[39mappend(agent_dict)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\OneDrive - Uniwersytet Jagielloński\\Documents\\torch-rl-trials\\Milestone-One\\environment\\environment.py:340\u001b[0m, in \u001b[0;36mTrafficEnvironment.help_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39madd_vehice(action_dict)\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_actions[agent\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m action_dict\n\u001b[1;32m--> 340\u001b[0m timestep, arrivals, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdet_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m veh_id \u001b[38;5;129;01min\u001b[39;00m arrivals:\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\OneDrive - Uniwersytet Jagielloński\\Documents\\torch-rl-trials\\Milestone-One\\environment\\simulator.py:83\u001b[0m, in \u001b[0;36mSumoSimulator.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 83\u001b[0m     arrivals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetArrivedIDList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo_connection\u001b[38;5;241m.\u001b[39msimulationStep()\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_simulation.py:335\u001b[0m, in \u001b[0;36mSimulationDomain.getArrivedIDList\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetArrivedIDList\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"getArrivedIDList() -> list(string)\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    Returns a list of ids of vehicles which arrived (have reached their destination and are removed from the road\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    network) in this time step.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAR_ARRIVED_VEHICLES_IDS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:147\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:151\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getCmd\u001b[39m(\u001b[38;5;28mself\u001b[39m, varID, objID, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mvalues):\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_sendCmd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmdGetID, varID, objID, \u001b[38;5;28mformat\u001b[39m, \u001b[38;5;241m*\u001b[39mvalues)\n\u001b[0;32m    153\u001b[0m     r\u001b[38;5;241m.\u001b[39mreadLength()\n",
      "\u001b[1;31mFatalTraCIError\u001b[0m: Not connected."
     ]
    }
   ],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manastasiapsarou123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>dqn\\wandb\\run-20240719_180830-m30vq9iu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation/runs/m30vq9iu' target=\"_blank\">DQN_TrafficEnv_82a45c94_24_07_19-18_08_30</a></strong> to <a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation' target=\"_blank\">https://wandb.ai/anastasiapsarou123/2_machines_mutation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation/runs/m30vq9iu' target=\"_blank\">https://wandb.ai/anastasiapsarou123/2_machines_mutation/runs/m30vq9iu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = None\n",
    "\n",
    "exp_name = generate_exp_name(\"DQN\", f\"TrafficEnv\")\n",
    "logger = get_logger(\n",
    "    \"wandb\",\n",
    "    logger_name=\"dqn\",\n",
    "    experiment_name=exp_name,\n",
    "    wandb_kwargs={\n",
    "        \"project\": \"2_machines_mutation\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "\n",
    "collected_frames = 0\n",
    "start_time = time.time()\n",
    "num_updates = 5\n",
    "batch_size = 10\n",
    "test_interval = 5\n",
    "max_grad = 1\n",
    "num_test_episodes = 5\n",
    "frames_per_batch = frames_per_batch\n",
    "init_random_frames = 5\n",
    "n_optim = 8\n",
    "sampling_start = time.time()\n",
    "q_losses = torch.zeros(num_updates, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:10,  1.34s/it]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:17,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n",
      "------------------STEP------------------\n",
      "\n",
      "Reset episode ['3'] \n",
      "\n",
      "\n",
      "\n",
      "Resetting the simulator\n",
      "self.agent_selection is:  3 \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:24,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 24.63 seconds to finish\n"
     ]
    }
   ],
   "source": [
    "q_losses_loop = {group: [] for group in env.group_map.keys()}\n",
    "\n",
    "pbar = tqdm.tqdm(total=n_iters)\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "\n",
    "    for group, _agents in env.group_map.items():\n",
    "        tensordict_data.set(\n",
    "            (\"next\", group, \"done\"),\n",
    "            tensordict_data.get((\"next\", \"done\"))\n",
    "            .unsqueeze(-1)\n",
    "            .expand(tensordict_data.get_item_shape((\"next\", group, \"reward\"))),  # Adjust index to start from 0\n",
    "        )\n",
    "        tensordict_data.set(\n",
    "            (\"next\", group, \"terminated\"),\n",
    "            tensordict_data.get((\"next\", \"terminated\"))\n",
    "            .unsqueeze(-1)\n",
    "            .expand(tensordict_data.get_item_shape((\"next\", group, \"reward\"))),  # Adjust index to start from 0\n",
    "        )\n",
    "\n",
    "    log_info = {}\n",
    "    sampling_time = time.time() - sampling_start\n",
    "    pbar.update(tensordict_data.numel())\n",
    "\n",
    "    data = tensordict_data.reshape(-1)\n",
    "    current_frames = data.numel()\n",
    "    collected_frames += current_frames\n",
    "\n",
    "    for group, agents in env.group_map.items():\n",
    "        replay_buffers[group].extend(data)\n",
    "        greedy_module[group].step(current_frames)\n",
    "\n",
    "    # Get and log training rewards and episode lengths\n",
    "    \n",
    "        episode_rewards = data[\"next\", group, \"episode_reward\"][data[\"next\", group, \"done\"]]\n",
    "        if len(episode_rewards) > 0:\n",
    "            episode_reward_mean = episode_rewards.mean().item()\n",
    "            #episode_length = data[\"next\", group, \"step_count\"][data[\"next\", group, \"done\"]]\n",
    "            #episode_length_mean = episode_length.sum().item() / len(episode_length)\n",
    "            log_info.update(\n",
    "                {\n",
    "                    f\"train/episode_reward_{group}\": episode_reward_mean,\n",
    "                    #\"train/episode_length\": episode_length_mean,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        \"\"\"if collected_frames < init_random_frames:\n",
    "            if logger:\n",
    "                for key, value in log_info.items():\n",
    "                    logger.log_scalar(key, value, step=collected_frames)\n",
    "            continue\"\"\"\n",
    "\n",
    "\n",
    "    # optimization steps\n",
    "    training_start = time.time()\n",
    "    for group, agent in env.group_map.items():\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "\n",
    "            sampled_tensordict = replay_buffers[group].sample()\n",
    "            sampled_tensordict = sampled_tensordict.to(device)\n",
    "\n",
    "            loss_td = losses[group](sampled_tensordict)\n",
    "            q_loss = loss_td[\"loss\"]\n",
    "\n",
    "            q_losses_loop[group].append(q_loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(losses[group].parameters()), max_norm=max_grad\n",
    "            )\n",
    "            \n",
    "            optimizers[group].step()\n",
    "            target_net_updaters[group].step()\n",
    "\n",
    "            training_time = time.time() - training_start\n",
    "\n",
    "            # Get and log q-values, loss, epsilon, sampling time and training time\n",
    "            log_info.update(\n",
    "                {\n",
    "                    f\"train/q_values_{group}\": (data[group, \"action_value\"] * data[group, \"action\"]).sum().item()\n",
    "                    / frames_per_batch,\n",
    "                    f\"train/q_loss_{group}\": torch.stack(q_losses_loop[group]).mean().item(),\n",
    "                    f\"train/epsilon_{group}\": greedy_module[group].eps,\n",
    "                    \"train/sampling_time\": sampling_time,\n",
    "                    \"train/training_time\": training_time,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            \"\"\"if logger:\n",
    "                for key, value in log_info.items():\n",
    "                    logger.log_scalar(key, value, step=collected_frames)\"\"\"\n",
    "\n",
    "            \n",
    "            # update weights of the inference policy\n",
    "            collector.update_policy_weights_()\n",
    "            sampling_start = time.time()\n",
    "\n",
    "\n",
    "collector.shutdown()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Training took {execution_time:.2f} seconds to finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<services.plotter.Plotter at 0x29278d1b910>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from services import plotter\n",
    "\n",
    "plotter(params[kc.PLOTTER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![actions.png](../../results/humans_mutation_dqn/actions.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![actions_shifts.png](../../results/humans_mutation_dqn/actions_shifts.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![rewards.png](../../results/humans_mutation_dqn/rewards.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![simulation_length.png](../../results/humans_mutation_dqn/simulation_length.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![travel_times.png](../../results/humans_mutation_dqn/travel_times.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![tt_dist.png](../../results/humans_mutation_dqn/tt_dist.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Path to the images directory\n",
    "images_dir = '../../results/humans_mutation_dqn'\n",
    "\n",
    "# List all image files in the directory\n",
    "images = [f for f in os.listdir(images_dir) if os.path.isfile(os.path.join(images_dir, f))]\n",
    "\n",
    "# Generate and display Markdown for each image\n",
    "for image in images:\n",
    "    markdown_image = f\"![{image}]({images_dir}/{image})\"\n",
    "    display(Markdown(markdown_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
