{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ERC Starting Grant on COeXISTENCE between humans and machines in urban mobility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/img_mileston1.png\" alt=\"Milestone 1 Image\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Machine training using Double DQN algorithm\n",
    "## Name: Anastasia\n",
    "### Date: July 11, 2024\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "> In this notebook, we implement the training of independent machine agents using the DQN algorithm.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "> The purpose of this notebook is to understand whether DQN algorithm can train effectively our RL agents.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Summary\n",
    "\n",
    "### Network Architecture\n",
    "- Csomor network\n",
    "---\n",
    "\n",
    "### Agents\n",
    "| **Type**          |           |\n",
    "|-------------------|---------------------|\n",
    "| **Number**        | 5 machines |\n",
    "| **Total demand** | random |\n",
    "---\n",
    "\n",
    "\n",
    "### Origin and Destination Details\n",
    "| **Origin Count**      | 2                            |\n",
    "|-----------------------|------------------------------|\n",
    "| **Destination Count** | 2                            |\n",
    "| **Origin Pairing**    | 279952229#0, 115604053       |\n",
    "| **Destination Pairing**| -115602933#2, -441496282#1     |\n",
    "---\n",
    "\n",
    "### Execution time\n",
    "- 8 min 28 sec\n",
    "\n",
    "### Hardware Utilized for Experiment Execution\n",
    "| **Type of Machine** | Personal computer (or server) |\n",
    "|----------------------|-------------------------------|\n",
    "| **CPU**              | 12th Gen Intel(R) Core(TM) i7-1255U |\n",
    "|                      | Cores: 10                   |\n",
    "|                      | Sockets: 1                  |\n",
    "|                      | Base Speed: 1.70 GHz        |\n",
    "| **Memory**           | 16GB                          |\n",
    "| **Disc (SSD)**       | 477 GB                        |\n",
    "| **Operating System** | Windows 11                    |\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "import torch\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.modules import MLP, QValueActor\n",
    "from torchrl.data import CompositeSpec\n",
    "from torchrl.modules import EGreedyModule\n",
    "from torchrl.objectives import DQNLoss, HardUpdate, SoftUpdate\n",
    "from torchrl.record.loggers import generate_exp_name, get_logger\n",
    "from torchrl.envs.transforms import RenameTransform\n",
    "from torchrl.modules.tensordict_module import QValueModule\n",
    "from torchrl.trainers import (\n",
    "    LogReward,\n",
    "    Recorder,\n",
    "    ReplayBufferTrainer,\n",
    "    Trainer,\n",
    "    UpdateWeights,\n",
    ")\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_of_parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir))\n",
    "sys.path.append(parent_of_parent_dir)\n",
    "\n",
    "from environment import TrafficEnvironment\n",
    "from keychain import Keychain as kc\n",
    "from services.plotter import Plotter\n",
    "from utilities import get_params\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "vmas_device = device  # The device where the simulator is run\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 20  # Number of team frames collected per training iteration\n",
    "n_iters = 40  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 100  # Number of optimization steps per training iteration\n",
    "minibatch_size = 2  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# DQN\n",
    "gamma = 0.99  # discount factor\n",
    "hard_update_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(kc.PARAMS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(params[kc.RUNNER], params[kc.ENVIRONMENT], params[kc.SIMULATOR], params[kc.AGENT_GEN], params[kc.AGENTS], params[kc.PHASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True,\n",
    "    group_map=None,\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2', 'episode_reward'), ('6', 'episode_reward'), ('4', 'episode_reward'), ('5', 'episode_reward')]\n"
     ]
    }
   ],
   "source": [
    "out_keys = []\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "    out_keys.append((group, \"episode_reward\"))\n",
    "\n",
    "print(out_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(\n",
    "        in_keys=env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(env.group_map.keys()),\n",
    "        out_keys = out_keys\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 'reward'), ('4', 'reward'), ('5', 'reward'), ('6', 'reward')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2', '6', '4', '5'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.group_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 12:11:28,431 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module is:  TensorDictModule(\n",
      "    module=MultiAgentMLP(\n",
      "      (agent_networks): ModuleList(\n",
      "        (0): MLP(\n",
      "          (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (5): ReLU()\n",
      "          (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (7): ReLU()\n",
      "          (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    device=cpu,\n",
      "    in_keys=[('2', 'observation')],\n",
      "    out_keys=[('2', 'action_value')]) \n",
      "\n",
      "\n",
      "\n",
      "module is:  TensorDictModule(\n",
      "    module=MultiAgentMLP(\n",
      "      (agent_networks): ModuleList(\n",
      "        (0): MLP(\n",
      "          (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (5): ReLU()\n",
      "          (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (7): ReLU()\n",
      "          (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    device=cpu,\n",
      "    in_keys=[('6', 'observation')],\n",
      "    out_keys=[('6', 'action_value')]) \n",
      "\n",
      "\n",
      "\n",
      "module is:  TensorDictModule(\n",
      "    module=MultiAgentMLP(\n",
      "      (agent_networks): ModuleList(\n",
      "        (0): MLP(\n",
      "          (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (5): ReLU()\n",
      "          (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (7): ReLU()\n",
      "          (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    device=cpu,\n",
      "    in_keys=[('4', 'observation')],\n",
      "    out_keys=[('4', 'action_value')]) \n",
      "\n",
      "\n",
      "\n",
      "module is:  TensorDictModule(\n",
      "    module=MultiAgentMLP(\n",
      "      (agent_networks): ModuleList(\n",
      "        (0): MLP(\n",
      "          (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (5): ReLU()\n",
      "          (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (7): ReLU()\n",
      "          (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    device=cpu,\n",
      "    in_keys=[('5', 'observation')],\n",
      "    out_keys=[('5', 'action_value')]) \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = False \n",
    "\n",
    "    mlp = MultiAgentMLP(\n",
    "        n_agent_inputs =env.observation_spec[group, \"observation\"].shape[-1],  \n",
    "        n_agent_outputs = env.full_action_spec[group, \"action\"].space.n, \n",
    "        n_agents = len(agents),\n",
    "        centralised=False,  \n",
    "        share_params = share_parameters_policy,\n",
    "        device = device,\n",
    "        depth = 4,\n",
    "        num_cells = 64,\n",
    "        activation_class=torch.nn.ReLU,\n",
    "    )\n",
    "\n",
    "    module = TensorDictModule(mlp, \n",
    "                              in_keys=[(group, \"observation\")],\n",
    "                              out_keys=[(group,\"action_value\")],\n",
    "    )\n",
    "\n",
    "    modules[group] = module\n",
    "\n",
    "    print(\"module is: \", module, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_modules = {}\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "\n",
    "    q_value_module = QValueModule(\n",
    "            action_value_key=(group, \"action_value\"),\n",
    "            out_keys=[\n",
    "                (group, \"action\"),\n",
    "                (group, \"action_value\"),\n",
    "                (group, \"chosen_action_value\"),\n",
    "            ],\n",
    "            spec=env.full_action_spec[group, \"action\"],\n",
    "            action_space=None,\n",
    "        )\n",
    "\n",
    "    q_value_modules[group] = q_value_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TensorDictSequential(*modules.values(), *q_value_modules.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a random rollout to ensure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, agents in env.group_map.items():\n",
    "\n",
    "    tensordict = env.fake_tensordict()\n",
    "    policy(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_module = {}\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "\n",
    "    greedy_module[group] = EGreedyModule(\n",
    "        action_key = (group, \"action\"),\n",
    "        spec=env.full_action_spec[group, \"action\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate the greedy module inside the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_policy = {}\n",
    "\n",
    "for group, agents in env.group_map.items():\n",
    "    col_policy[group] = TensorDictSequential(policy, greedy_module[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_policies = TensorDictSequential(*col_policy.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    col_policies,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    reset_at_each_iter=False,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffers = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    replay_buffers[group] = ReplayBuffer(\n",
    "        storage=LazyTensorStorage(\n",
    "            frames_per_batch, device=device\n",
    "        ), \n",
    "        batch_size=minibatch_size, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "optimizers = {}\n",
    "target_net_updaters = {}\n",
    "\n",
    "\n",
    "for group, _agents in env.group_map.items():\n",
    "    loss_module = DQNLoss(\n",
    "        value_network=col_policies,\n",
    "        loss_function=\"l2\",\n",
    "        double_dqn = False,\n",
    "        delay_value=True,\n",
    "        action_space = \"categorical\"\n",
    "    )\n",
    "\n",
    "    loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "        reward=(group, \"reward\"),  \n",
    "        action_value=(group, \"action_value\"),\n",
    "        action=(group, \"action\"), \n",
    "        done=(group, \"done\"),\n",
    "        terminated=(group, \"terminated\"),\n",
    "        value=(group, \"chosen_action_value\"),\n",
    "    )\n",
    "\n",
    "    loss_module.make_value_estimator(gamma=gamma)\n",
    "\n",
    "    target_net_updaters[group] = SoftUpdate(\n",
    "        loss_module, eps=0.98\n",
    "    )    \n",
    "\n",
    "    losses[group] = loss_module\n",
    "\n",
    "    optimizer = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "    \n",
    "    optimizers[group] = optimizer\n",
    "\n",
    "# Access loss module for the first group for example\n",
    "group = next(iter(env.group_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_optim = 8\n",
    "trainer = Trainer(\n",
    "    collector=collector,\n",
    "    total_frames=total_frames,\n",
    "    frame_skip=1,\n",
    "    loss_module=loss_module,\n",
    "    optimizer=optimizer,\n",
    "    optim_steps_per_batch=n_optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manastasiapsarou123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>dqn\\wandb\\run-20240719_180830-m30vq9iu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation/runs/m30vq9iu' target=\"_blank\">DQN_TrafficEnv_82a45c94_24_07_19-18_08_30</a></strong> to <a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation' target=\"_blank\">https://wandb.ai/anastasiapsarou123/2_machines_mutation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anastasiapsarou123/2_machines_mutation/runs/m30vq9iu' target=\"_blank\">https://wandb.ai/anastasiapsarou123/2_machines_mutation/runs/m30vq9iu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = None\n",
    "\n",
    "exp_name = generate_exp_name(\"DQN\", f\"TrafficEnv\")\n",
    "logger = get_logger(\n",
    "    \"wandb\",\n",
    "    logger_name=\"dqn\",\n",
    "    experiment_name=exp_name,\n",
    "    wandb_kwargs={\n",
    "        \"project\": \"2_machines_mutation\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "\n",
    "collected_frames = 0\n",
    "start_time = time.time()\n",
    "num_updates = 5\n",
    "batch_size = 10\n",
    "test_interval = 5\n",
    "max_grad = 1\n",
    "num_test_episodes = 5\n",
    "frames_per_batch = frames_per_batch\n",
    "init_random_frames = 5\n",
    "n_optim = 8\n",
    "sampling_start = time.time()\n",
    "q_losses = torch.zeros(num_updates, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [08:24,  1.56it/s]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 508.01 seconds to finish\n"
     ]
    }
   ],
   "source": [
    "q_losses_loop = {group: [] for group in env.group_map.keys()}\n",
    "\n",
    "pbar = tqdm.tqdm(total=n_iters)\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "\n",
    "    for group, _agents in env.group_map.items():\n",
    "        tensordict_data.set(\n",
    "            (\"next\", group, \"done\"),\n",
    "            tensordict_data.get((\"next\", \"done\"))\n",
    "            .unsqueeze(-1)\n",
    "            .expand(tensordict_data.get_item_shape((\"next\", group, \"reward\"))),  # Adjust index to start from 0\n",
    "        )\n",
    "        tensordict_data.set(\n",
    "            (\"next\", group, \"terminated\"),\n",
    "            tensordict_data.get((\"next\", \"terminated\"))\n",
    "            .unsqueeze(-1)\n",
    "            .expand(tensordict_data.get_item_shape((\"next\", group, \"reward\"))),  # Adjust index to start from 0\n",
    "        )\n",
    "\n",
    "    log_info = {}\n",
    "    sampling_time = time.time() - sampling_start\n",
    "    pbar.update(tensordict_data.numel())\n",
    "\n",
    "    data = tensordict_data.reshape(-1)\n",
    "    current_frames = data.numel()\n",
    "    collected_frames += current_frames\n",
    "\n",
    "    for group, agents in env.group_map.items():\n",
    "        replay_buffers[group].extend(data)\n",
    "        greedy_module[group].step(current_frames)\n",
    "\n",
    "    # Get and log training rewards and episode lengths\n",
    "    \n",
    "        episode_rewards = data[\"next\", group, \"episode_reward\"][data[\"next\", group, \"done\"]]\n",
    "        if len(episode_rewards) > 0:\n",
    "            episode_reward_mean = episode_rewards.mean().item()\n",
    "            #episode_length = data[\"next\", group, \"step_count\"][data[\"next\", group, \"done\"]]\n",
    "            #episode_length_mean = episode_length.sum().item() / len(episode_length)\n",
    "            log_info.update(\n",
    "                {\n",
    "                    f\"train/episode_reward_{group}\": episode_reward_mean,\n",
    "                    #\"train/episode_length\": episode_length_mean,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if collected_frames < init_random_frames:\n",
    "            if logger:\n",
    "                for key, value in log_info.items():\n",
    "                    logger.log_scalar(key, value, step=collected_frames)\n",
    "            continue\n",
    "\n",
    "\n",
    "    # optimization steps\n",
    "    training_start = time.time()\n",
    "    for group, agent in env.group_map.items():\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "\n",
    "            sampled_tensordict = replay_buffers[group].sample()\n",
    "            sampled_tensordict = sampled_tensordict.to(device)\n",
    "\n",
    "            loss_td = losses[group](sampled_tensordict)\n",
    "            q_loss = loss_td[\"loss\"]\n",
    "\n",
    "            q_losses_loop[group].append(q_loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(losses[group].parameters()), max_norm=max_grad\n",
    "            )\n",
    "            \n",
    "            optimizers[group].step()\n",
    "            target_net_updaters[group].step()\n",
    "\n",
    "            training_time = time.time() - training_start\n",
    "\n",
    "            # Get and log q-values, loss, epsilon, sampling time and training time\n",
    "            log_info.update(\n",
    "                {\n",
    "                    f\"train/q_values_{group}\": (data[group, \"action_value\"] * data[group, \"action\"]).sum().item()\n",
    "                    / frames_per_batch,\n",
    "                    f\"train/q_loss_{group}\": torch.stack(q_losses_loop[group]).mean().item(),\n",
    "                    f\"train/epsilon_{group}\": greedy_module[group].eps,\n",
    "                    \"train/sampling_time\": sampling_time,\n",
    "                    \"train/training_time\": training_time,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if logger:\n",
    "                for key, value in log_info.items():\n",
    "                    logger.log_scalar(key, value, step=collected_frames)\n",
    "\n",
    "            \n",
    "            # update weights of the inference policy\n",
    "            collector.update_policy_weights_()\n",
    "            sampling_start = time.time()\n",
    "\n",
    "\n",
    "collector.shutdown()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Training took {execution_time:.2f} seconds to finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<services.plotter.Plotter at 0x29278d1b910>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from services import plotter\n",
    "\n",
    "plotter(params[kc.PLOTTER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![actions.png](../../results/humans_mutation_dqn/actions.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![actions_shifts.png](../../results/humans_mutation_dqn/actions_shifts.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![rewards.png](../../results/humans_mutation_dqn/rewards.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![simulation_length.png](../../results/humans_mutation_dqn/simulation_length.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![travel_times.png](../../results/humans_mutation_dqn/travel_times.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![tt_dist.png](../../results/humans_mutation_dqn/tt_dist.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Path to the images directory\n",
    "images_dir = '../../results/humans_mutation_dqn'\n",
    "\n",
    "# List all image files in the directory\n",
    "images = [f for f in os.listdir(images_dir) if os.path.isfile(os.path.join(images_dir, f))]\n",
    "\n",
    "# Generate and display Markdown for each image\n",
    "for image in images:\n",
    "    markdown_image = f\"![{image}]({images_dir}/{image})\"\n",
    "    display(Markdown(markdown_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
