{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchrl.collectors import MultiaSyncDataCollector, SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, MultiStep, TensorDictReplayBuffer\n",
    "from torchrl.envs import (\n",
    "    EnvCreator,\n",
    "    ExplorationType,\n",
    "    ParallelEnv,\n",
    "    RewardScaling,\n",
    "    StepCounter,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.transforms import (\n",
    "    CatFrames,\n",
    "    Compose,\n",
    "    GrayScale,\n",
    "    ObservationNorm,\n",
    "    Resize,\n",
    "    ToTensorImage,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.modules import DuelingCnnDQNet, EGreedyModule, QValueActor\n",
    "from tensordict.nn import TensorDictSequential\n",
    "\n",
    "\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from torchrl.record.loggers.csv import CSVLogger\n",
    "from torchrl.trainers import (\n",
    "    LogReward,\n",
    "    Recorder,\n",
    "    ReplayBufferTrainer,\n",
    "    Trainer,\n",
    "    UpdateWeights,\n",
    ")\n",
    "\n",
    "\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == \"ZMQInteractiveShell\":\n",
    "            return True  # Jupyter notebook or qtconsole\n",
    "        elif shell == \"TerminalInteractiveShell\":\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False  # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(\n",
    "    parallel=False,\n",
    "    obs_norm_sd=None,\n",
    "    num_workers=1,\n",
    "):\n",
    "    if obs_norm_sd is None:\n",
    "        obs_norm_sd = {\"standard_normal\": True}\n",
    "    if parallel:\n",
    "\n",
    "        def maker():\n",
    "            return GymEnv(\n",
    "                \"CartPole-v1\",\n",
    "                from_pixels=True,\n",
    "                pixels_only=True,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "        base_env = ParallelEnv(\n",
    "            num_workers,\n",
    "            EnvCreator(maker),\n",
    "            # Don't create a sub-process if we have only one worker\n",
    "            serial_for_single=True,\n",
    "        )\n",
    "    else:\n",
    "        base_env = GymEnv(\n",
    "            \"CartPole-v1\",\n",
    "            from_pixels=True,\n",
    "            pixels_only=True,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    env = TransformedEnv(\n",
    "        base_env,\n",
    "        Compose(\n",
    "            StepCounter(),  # to count the steps of each trajectory\n",
    "            ToTensorImage(),\n",
    "            RewardScaling(loc=0.0, scale=0.1),\n",
    "            GrayScale(),\n",
    "            Resize(64, 64),\n",
    "            CatFrames(4, in_keys=[\"pixels\"], dim=-3),\n",
    "            ObservationNorm(in_keys=[\"pixels\"], **obs_norm_sd),\n",
    "        ),\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_stats():\n",
    "    test_env = make_env()\n",
    "    test_env.transform[-1].init_stats(\n",
    "        num_iter=1000, cat_dim=0, reduce_dim=[-1, -2, -4], keep_dims=(-1, -2)\n",
    "    )\n",
    "    obs_norm_sd = test_env.transform[-1].state_dict()\n",
    "    # let's check that normalizing constants have a size of ``[C, 1, 1]`` where\n",
    "    # ``C=4`` (because of :class:`~torchrl.envs.CatFrames`).\n",
    "    print(\"state dict of the observation norm:\", obs_norm_sd)\n",
    "    test_env.close()\n",
    "    del test_env\n",
    "    return obs_norm_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dummy_env):\n",
    "    cnn_kwargs = {\n",
    "        \"num_cells\": [32, 64, 64],\n",
    "        \"kernel_sizes\": [6, 4, 3],\n",
    "        \"strides\": [2, 2, 1],\n",
    "        \"activation_class\": nn.ELU,\n",
    "        # This can be used to reduce the size of the last layer of the CNN\n",
    "        # \"squeeze_output\": True,\n",
    "        # \"aggregator_class\": nn.AdaptiveAvgPool2d,\n",
    "        # \"aggregator_kwargs\": {\"output_size\": (1, 1)},\n",
    "    }\n",
    "    mlp_kwargs = {\n",
    "        \"depth\": 2,\n",
    "        \"num_cells\": [\n",
    "            64,\n",
    "            64,\n",
    "        ],\n",
    "        \"activation_class\": nn.ELU,\n",
    "    }\n",
    "    net = DuelingCnnDQNet(\n",
    "        dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs\n",
    "    ).to(device)\n",
    "    net.value[-1].bias.data.fill_(init_bias)\n",
    "    \n",
    "    print(\"net is: \", net)\n",
    "\n",
    "    actor = QValueActor(net, in_keys=[\"pixels\"], spec=dummy_env.action_spec).to(device)\n",
    "\n",
    "    print(\"QValueActor is: \", actor, \"\\n\\n\")\n",
    "\n",
    "    # init actor: because the model is composed of lazy conv/linear layers,\n",
    "    # we must pass a fake batch of data through it to instantiate them.\n",
    "    tensordict = dummy_env.fake_tensordict()\n",
    "    actor(tensordict)\n",
    "\n",
    "    # we join our actor with an EGreedyModule for data collection\n",
    "    exploration_module = EGreedyModule(\n",
    "        spec=dummy_env.action_spec,\n",
    "        annealing_num_steps=total_frames,\n",
    "        eps_init=eps_greedy_val,\n",
    "        eps_end=eps_greedy_val_env,\n",
    "    )\n",
    "\n",
    "    print(\"actor is: \", actor)\n",
    "    actor_explore = TensorDictSequential(actor, exploration_module)\n",
    "\n",
    "    return actor, actor_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replay_buffer(buffer_size, n_optim, batch_size):\n",
    "    replay_buffer = TensorDictReplayBuffer(\n",
    "        batch_size=batch_size,\n",
    "        storage=LazyMemmapStorage(buffer_size),\n",
    "        prefetch=n_optim,\n",
    "    )\n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collector(\n",
    "    stats,\n",
    "    num_collectors,\n",
    "    actor_explore,\n",
    "    frames_per_batch,\n",
    "    total_frames,\n",
    "    device,\n",
    "):\n",
    "    # We can't use nested child processes with mp_start_method=\"fork\"\n",
    "   \n",
    "    cls = SyncDataCollector\n",
    "    env_arg = make_env(parallel=True, obs_norm_sd=stats, num_workers=num_workers)\n",
    "\n",
    "    data_collector = cls(\n",
    "        env_arg,\n",
    "        policy=actor_explore,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "        # this is the default behaviour: the collector runs in ``\"random\"`` (or explorative) mode\n",
    "        exploration_type=ExplorationType.RANDOM,\n",
    "        # We set the all the devices to be identical. Below is an example of\n",
    "        # heterogeneous devices\n",
    "        device=device,\n",
    "        storing_device=device,\n",
    "        split_trajs=False,\n",
    "        postproc=MultiStep(gamma=gamma, n_steps=5),\n",
    "    )\n",
    "    return data_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_module(actor, gamma):\n",
    "    loss_module = DQNLoss(actor, delay_value=True)\n",
    "    loss_module.make_value_estimator(gamma=gamma)\n",
    "    target_updater = SoftUpdate(loss_module, eps=0.995)\n",
    "    return loss_module, target_updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the learning rate of the optimizer\n",
    "lr = 2e-3\n",
    "# weight decay\n",
    "wd = 1e-5\n",
    "# the beta parameters of Adam\n",
    "betas = (0.9, 0.999)\n",
    "# Optimization steps per batch collected (aka UPD or updates per data)\n",
    "n_optim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "tau = 0.02\n",
    "total_frames = 5_000  # 500000\n",
    "init_random_frames = 100  # 1000\n",
    "frames_per_batch = 32  # 128\n",
    "batch_size = 32  # 256\n",
    "buffer_size = min(total_frames, 100000)\n",
    "num_workers = 2  # 8\n",
    "num_collectors = 2  # 4\n",
    "eps_greedy_val = 0.1\n",
    "eps_greedy_val_env = 0.005\n",
    "init_bias = 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net is:  DuelingCnnDQNet(\n",
      "  (features): ConvNet(\n",
      "    (0): LazyConv2d(0, 32, kernel_size=(6, 6), stride=(2, 2))\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): SquashDims()\n",
      "  )\n",
      "  (advantage): MLP(\n",
      "    (0): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      "  (value): MLP(\n",
      "    (0): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "QValueActor is:  QValueActor(\n",
      "    module=ModuleList(\n",
      "      (0): TensorDictModule(\n",
      "          module=DuelingCnnDQNet(\n",
      "            (features): ConvNet(\n",
      "              (0): LazyConv2d(0, 32, kernel_size=(6, 6), stride=(2, 2))\n",
      "              (1): ELU(alpha=1.0)\n",
      "              (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "              (3): ELU(alpha=1.0)\n",
      "              (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "              (5): ELU(alpha=1.0)\n",
      "              (6): SquashDims()\n",
      "            )\n",
      "            (advantage): MLP(\n",
      "              (0): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "              (1): ELU(alpha=1.0)\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): ELU(alpha=1.0)\n",
      "              (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "            )\n",
      "            (value): MLP(\n",
      "              (0): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "              (1): ELU(alpha=1.0)\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): ELU(alpha=1.0)\n",
      "              (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "            )\n",
      "          ),\n",
      "          device=cpu,\n",
      "          in_keys=['pixels'],\n",
      "          out_keys=['action_value'])\n",
      "      (1): QValueModule()\n",
      "    ),\n",
      "    device=cpu,\n",
      "    in_keys=['pixels'],\n",
      "    out_keys=['action', 'action_value', 'chosen_action_value']) \n",
      "\n",
      "\n",
      "actor is:  QValueActor(\n",
      "    module=ModuleList(\n",
      "      (0): TensorDictModule(\n",
      "          module=DuelingCnnDQNet(\n",
      "            (features): ConvNet(\n",
      "              (0): Conv2d(4, 32, kernel_size=(6, 6), stride=(2, 2))\n",
      "              (1): ELU(alpha=1.0)\n",
      "              (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "              (3): ELU(alpha=1.0)\n",
      "              (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "              (5): ELU(alpha=1.0)\n",
      "              (6): SquashDims()\n",
      "            )\n",
      "            (advantage): MLP(\n",
      "              (0): Linear(in_features=9216, out_features=64, bias=True)\n",
      "              (1): ELU(alpha=1.0)\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): ELU(alpha=1.0)\n",
      "              (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "            )\n",
      "            (value): MLP(\n",
      "              (0): Linear(in_features=9216, out_features=64, bias=True)\n",
      "              (1): ELU(alpha=1.0)\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): ELU(alpha=1.0)\n",
      "              (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "            )\n",
      "          ),\n",
      "          device=cpu,\n",
      "          in_keys=['pixels'],\n",
      "          out_keys=['action_value'])\n",
      "      (1): QValueModule()\n",
      "    ),\n",
      "    device=cpu,\n",
      "    in_keys=['pixels'],\n",
      "    out_keys=['action', 'action_value', 'chosen_action_value'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "#stats = get_norm_stats()\n",
    "test_env = make_env(parallel=False)\n",
    "# Get model\n",
    "actor, actor_explore = make_model(test_env)\n",
    "loss_module, target_net_updater = get_loss_module(actor, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QValueActor(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=DuelingCnnDQNet(\n",
       "            (features): ConvNet(\n",
       "              (0): Conv2d(4, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "              (1): ELU(alpha=1.0)\n",
       "              (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "              (3): ELU(alpha=1.0)\n",
       "              (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "              (5): ELU(alpha=1.0)\n",
       "              (6): SquashDims()\n",
       "            )\n",
       "            (advantage): MLP(\n",
       "              (0): Linear(in_features=9216, out_features=64, bias=True)\n",
       "              (1): ELU(alpha=1.0)\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): ELU(alpha=1.0)\n",
       "              (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "            )\n",
       "            (value): MLP(\n",
       "              (0): Linear(in_features=9216, out_features=64, bias=True)\n",
       "              (1): ELU(alpha=1.0)\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): ELU(alpha=1.0)\n",
       "              (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "            )\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['pixels'],\n",
       "          out_keys=['action_value'])\n",
       "      (1): QValueModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['pixels'],\n",
       "    out_keys=['action', 'action_value', 'chosen_action_value'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): QValueActor(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=DuelingCnnDQNet(\n",
       "                  (features): ConvNet(\n",
       "                    (0): Conv2d(4, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "                    (1): ELU(alpha=1.0)\n",
       "                    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "                    (3): ELU(alpha=1.0)\n",
       "                    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "                    (5): ELU(alpha=1.0)\n",
       "                    (6): SquashDims()\n",
       "                  )\n",
       "                  (advantage): MLP(\n",
       "                    (0): Linear(in_features=9216, out_features=64, bias=True)\n",
       "                    (1): ELU(alpha=1.0)\n",
       "                    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (3): ELU(alpha=1.0)\n",
       "                    (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "                  )\n",
       "                  (value): MLP(\n",
       "                    (0): Linear(in_features=9216, out_features=64, bias=True)\n",
       "                    (1): ELU(alpha=1.0)\n",
       "                    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (3): ELU(alpha=1.0)\n",
       "                    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "                  )\n",
       "                ),\n",
       "                device=cpu,\n",
       "                in_keys=['pixels'],\n",
       "                out_keys=['action_value'])\n",
       "            (1): QValueModule()\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['pixels'],\n",
       "          out_keys=['action', 'action_value', 'chosen_action_value'])\n",
       "      (1): EGreedyModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['pixels'],\n",
       "    out_keys=['action_value', 'chosen_action_value', 'action'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNLoss(\n",
       "  (value_network_params): TensorDictParams(params=TensorDict(\n",
       "      fields={\n",
       "          module: TensorDict(\n",
       "              fields={\n",
       "                  0: TensorDict(\n",
       "                      fields={\n",
       "                          module: TensorDict(\n",
       "                              fields={\n",
       "                                  advantage: TensorDict(\n",
       "                                      fields={\n",
       "                                          0: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 9216]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          1: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          2: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          3: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          4: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([2, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False)},\n",
       "                                      batch_size=torch.Size([]),\n",
       "                                      device=None,\n",
       "                                      is_shared=False),\n",
       "                                  features: TensorDict(\n",
       "                                      fields={\n",
       "                                          0: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([32]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([32, 4, 6, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          1: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          2: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 32, 4, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          3: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          4: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 64, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          5: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          6: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False)},\n",
       "                                      batch_size=torch.Size([]),\n",
       "                                      device=None,\n",
       "                                      is_shared=False),\n",
       "                                  value: TensorDict(\n",
       "                                      fields={\n",
       "                                          0: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 9216]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          1: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          2: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          3: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          4: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([1, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False)},\n",
       "                                      batch_size=torch.Size([]),\n",
       "                                      device=None,\n",
       "                                      is_shared=False)},\n",
       "                              batch_size=torch.Size([]),\n",
       "                              device=None,\n",
       "                              is_shared=False)},\n",
       "                      batch_size=torch.Size([]),\n",
       "                      device=None,\n",
       "                      is_shared=False),\n",
       "                  1: TensorDict(\n",
       "                      fields={\n",
       "                      },\n",
       "                      batch_size=torch.Size([]),\n",
       "                      device=None,\n",
       "                      is_shared=False)},\n",
       "              batch_size=torch.Size([]),\n",
       "              device=None,\n",
       "              is_shared=False)},\n",
       "      batch_size=torch.Size([]),\n",
       "      device=None,\n",
       "      is_shared=False))\n",
       "  (target_value_network_params): TensorDictParams(params=TensorDict(\n",
       "      fields={\n",
       "          module: TensorDict(\n",
       "              fields={\n",
       "                  0: TensorDict(\n",
       "                      fields={\n",
       "                          module: TensorDict(\n",
       "                              fields={\n",
       "                                  advantage: TensorDict(\n",
       "                                      fields={\n",
       "                                          0: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 9216]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          1: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          2: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          3: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          4: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([2, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False)},\n",
       "                                      batch_size=torch.Size([]),\n",
       "                                      device=None,\n",
       "                                      is_shared=False),\n",
       "                                  features: TensorDict(\n",
       "                                      fields={\n",
       "                                          0: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([32]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([32, 4, 6, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          1: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          2: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 32, 4, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          3: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          4: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 64, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          5: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          6: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False)},\n",
       "                                      batch_size=torch.Size([]),\n",
       "                                      device=None,\n",
       "                                      is_shared=False),\n",
       "                                  value: TensorDict(\n",
       "                                      fields={\n",
       "                                          0: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 9216]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          1: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          2: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([64, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          3: TensorDict(\n",
       "                                              fields={\n",
       "                                              },\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False),\n",
       "                                          4: TensorDict(\n",
       "                                              fields={\n",
       "                                                  bias: Parameter(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                  weight: Parameter(shape=torch.Size([1, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                              batch_size=torch.Size([]),\n",
       "                                              device=None,\n",
       "                                              is_shared=False)},\n",
       "                                      batch_size=torch.Size([]),\n",
       "                                      device=None,\n",
       "                                      is_shared=False)},\n",
       "                              batch_size=torch.Size([]),\n",
       "                              device=None,\n",
       "                              is_shared=False)},\n",
       "                      batch_size=torch.Size([]),\n",
       "                      device=None,\n",
       "                      is_shared=False),\n",
       "                  1: TensorDict(\n",
       "                      fields={\n",
       "                      },\n",
       "                      batch_size=torch.Size([]),\n",
       "                      device=None,\n",
       "                      is_shared=False)},\n",
       "              batch_size=torch.Size([]),\n",
       "              device=None,\n",
       "              is_shared=False)},\n",
       "      batch_size=torch.Size([]),\n",
       "      device=None,\n",
       "      is_shared=False))\n",
       "  (_value_estimator): TD0Estimator()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.keys to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.keys` for environment variables or `env.get_wrapper_attr('keys')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torchrl.envs.transforms.transforms.ObservationNorm() argument after ** must be a mapping, not TransformedEnv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m collector \u001b[38;5;241m=\u001b[39m \u001b[43mget_collector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_collectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_collectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_explore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_explore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframes_per_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[0;32m     10\u001b[0m     loss_module\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd, betas\u001b[38;5;241m=\u001b[39mbetas\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m exp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn_exp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;241m.\u001b[39muuid1()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mget_collector\u001b[1;34m(stats, num_collectors, actor_explore, frames_per_batch, total_frames, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_collector\u001b[39m(\n\u001b[0;32m      2\u001b[0m     stats,\n\u001b[0;32m      3\u001b[0m     num_collectors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m ):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# We can't use nested child processes with mp_start_method=\"fork\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m SyncDataCollector\n\u001b[1;32m---> 12\u001b[0m     env_arg \u001b[38;5;241m=\u001b[39m \u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_norm_sd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     data_collector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     15\u001b[0m         env_arg,\n\u001b[0;32m     16\u001b[0m         policy\u001b[38;5;241m=\u001b[39mactor_explore,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m         postproc\u001b[38;5;241m=\u001b[39mMultiStep(gamma\u001b[38;5;241m=\u001b[39mgamma, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_collector\n",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m, in \u001b[0;36mmake_env\u001b[1;34m(parallel, obs_norm_sd, num_workers)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     base_env \u001b[38;5;241m=\u001b[39m GymEnv(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m         from_pixels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     28\u001b[0m         pixels_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     29\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     32\u001b[0m env \u001b[38;5;241m=\u001b[39m TransformedEnv(\n\u001b[0;32m     33\u001b[0m     base_env,\n\u001b[0;32m     34\u001b[0m     Compose(\n\u001b[0;32m     35\u001b[0m         StepCounter(),  \u001b[38;5;66;03m# to count the steps of each trajectory\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         ToTensorImage(),\n\u001b[0;32m     37\u001b[0m         RewardScaling(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m     38\u001b[0m         GrayScale(),\n\u001b[0;32m     39\u001b[0m         Resize(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     40\u001b[0m         CatFrames(\u001b[38;5;241m4\u001b[39m, in_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixels\u001b[39m\u001b[38;5;124m\"\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m---> 41\u001b[0m         ObservationNorm(in_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixels\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobs_norm_sd),\n\u001b[0;32m     42\u001b[0m     ),\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "\u001b[1;31mTypeError\u001b[0m: torchrl.envs.transforms.transforms.ObservationNorm() argument after ** must be a mapping, not TransformedEnv"
     ]
    }
   ],
   "source": [
    "collector = get_collector(\n",
    "    stats=test_env,\n",
    "    num_collectors=num_collectors,\n",
    "    actor_explore=actor_explore,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    device=device,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    loss_module.parameters(), lr=lr, weight_decay=wd, betas=betas\n",
    ")\n",
    "exp_name = f\"dqn_exp_{uuid.uuid1()}\"\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "logger = CSVLogger(exp_name=exp_name, log_dir=tmpdir.name)\n",
    "warnings.warn(f\"log dir: {logger.experiment.log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\anastasia\\anaconda3\\envs\\torchrl\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\anastasia\\anaconda3\\envs\\torchrl\\lib\\site-packages (from gymnasium[classic-control]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\anastasia\\anaconda3\\envs\\torchrl\\lib\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\anastasia\\anaconda3\\envs\\torchrl\\lib\\site-packages (from gymnasium[classic-control]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\anastasia\\anaconda3\\envs\\torchrl\\lib\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Collecting pygame>=2.1.3 (from gymnasium[classic-control])\n",
      "  Downloading pygame-2.6.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.6.0-cp311-cp311-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.8 MB 487.6 kB/s eta 0:00:22\n",
      "    --------------------------------------- 0.2/10.8 MB 1.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.6/10.8 MB 3.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/10.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/10.8 MB 4.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.5/10.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/10.8 MB 4.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.2/10.8 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/10.8 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/10.8 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.6/10.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.2/10.8 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.9/10.8 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.7/10.8 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.5/10.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.2/10.8 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.8 MB 11.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/10.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 12.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[classic-control]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
