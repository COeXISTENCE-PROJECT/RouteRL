{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to our PettingZoo Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have created a framework that integrates reinforcement learning (RL) with a microscopic traffic simulation tool to explore the potential of RL in optimizing urban route choice.\n",
    "\n",
    "> We use [SUMO](https://sumo.dlr.de/docs/index.html), an open-source, microscopic and continuous traffic simulation.\n",
    "\n",
    "## Related work\n",
    "\n",
    "> Some methods have utilized RL for optimal route choice (Thomasini et al. [2023](https://alaworkshop2023.github.io/papers/ALA2023_paper_69.pdf/)). These approaches\n",
    "are typically based on macroscopic traffic simulations, which model relationships among traffic\n",
    "flow characteristics such as density, flow, and mean speed of a traffic stream. In contrast, our\n",
    "problem employs a microscopic model, which focuses on interactions between individual vehicles.\n",
    "\n",
    "> Additionally, a method proposed by (Tavares and Bazzan [2012](https://www.researchgate.net/publication/235219033_Reinforcement_learning_for_route_choice_in_an_abstract_traffic_scenario)) addresses optimal route choice at the microscopic level, where rewards are generated through a predefined function. In contrast, in our approach, rewards are provided dynamically by a continuous traffic simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from keychain_main import Keychain as kc\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "from RouteRL.environment.environment import TrafficEnvironment\n",
    "from RouteRL.services import plotter\n",
    "\n",
    "from RouteRL.create_agents import create_agent_objects\n",
    "from RouteRL.utilities import check_device\n",
    "from RouteRL.utilities import get_params\n",
    "from RouteRL.utilities import set_seeds\n",
    "\n",
    "from pettingzoo.test import api_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "check_device()\n",
    "set_seeds()\n",
    "params = get_params(kc.PARAMS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(params[kc.RUNNER], params[kc.ENVIRONMENT], params[kc.SIMULATOR], params[kc.AGENT_GEN], params[kc.AGENTS], params[kc.PHASE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  20 \n",
      "\n",
      "Agents are:  [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19] \n",
      "\n",
      "Number of human agents is:  20 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mutation: a portion of human agents are converted into machine agents (autonomous vehicles). You can adjust the number of agents to be mutated in the <code style=\"color:white\">/params.json</code> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  20 \n",
      "\n",
      "Agents are:  [Machine 2, Machine 17, Machine 9, Machine 8, Machine 7, Machine 4, Machine 10, Machine 15, Machine 0, Machine 1, Human 3, Human 5, Human 6, Human 11, Human 12, Human 13, Human 14, Human 16, Human 18, Human 19] \n",
      "\n",
      "Number of human agents is:  10 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Machine 2,\n",
       " Machine 17,\n",
       " Machine 9,\n",
       " Machine 8,\n",
       " Machine 7,\n",
       " Machine 4,\n",
       " Machine 10,\n",
       " Machine 15,\n",
       " Machine 0,\n",
       " Machine 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.machine_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 8 is going to step with action 0\n",
      "\n",
      "Agent 9 is going to step with action 1\n",
      "\n",
      "Agent 0 is going to step with action 1\n",
      "\n",
      "Agent 15 is going to step with action 0\n",
      "\n",
      "Agent 2 is going to step with action 0\n",
      "\n",
      "Agent 17 is going to step with action 1\n",
      "\n",
      "Agent 7 is going to step with action 1\n",
      "\n",
      "Agent 4 is going to step with action 0\n",
      "\n",
      "Agent 10 is going to step with action 0\n",
      "\n",
      "Agent 1 is going to step with action 0\n",
      "\n",
      "Agent 8 is terminating\n",
      "\n",
      "Agent 8 is going to step with action None\n",
      "\n",
      "Agent 9 is terminating\n",
      "\n",
      "Agent 9 is going to step with action None\n",
      "\n",
      "Agent 0 is terminating\n",
      "\n",
      "Agent 0 is going to step with action None\n",
      "\n",
      "Agent 15 is terminating\n",
      "\n",
      "Agent 15 is going to step with action None\n",
      "\n",
      "Agent 2 is terminating\n",
      "\n",
      "Agent 2 is going to step with action None\n",
      "\n",
      "Agent 17 is terminating\n",
      "\n",
      "Agent 17 is going to step with action None\n",
      "\n",
      "Agent 7 is terminating\n",
      "\n",
      "Agent 7 is going to step with action None\n",
      "\n",
      "Agent 4 is terminating\n",
      "\n",
      "Agent 4 is going to step with action None\n",
      "\n",
      "Agent 10 is terminating\n",
      "\n",
      "Agent 10 is going to step with action None\n",
      "\n",
      "Agent 1 is terminating\n",
      "\n",
      "Agent 1 is going to step with action None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "\n",
    "for episode in range(episodes):\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if termination or truncation:\n",
    "            print(f\"Agent {agent} is terminating\\n\")\n",
    "            action = None\n",
    "        else:\n",
    "            # this is where you would insert your policy\n",
    "            action = env.action_space(agent).sample()\n",
    "\n",
    "        print(f\"Agent {agent} is going to step with action {action}\\n\")\n",
    "        env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"color:white\">agent_iter(max_iter=2**63)</code> returns an iterator that yields the current agent of the environment. It terminates when all agents in the environment are done or when max_iter (steps have been executed).\n",
    "\n",
    "<code style=\"color:white\">last(observe=True)</code> returns observation, reward, done, and info for the agent currently able to act. The returned reward is the cumulative reward that the agent has received since it last acted. If observe is set to False, the observation will not be computed, and None will be returned in its place. Note that a single agent being done does not imply the environment is done.\n",
    "\n",
    "<code style=\"color:white\">reset()</code> resets the environment and sets it up for use when called the first time. This method must be called before any other method.\n",
    "\n",
    "<code style=\"color:white\">step(action)</code> takes and executes the action of the agent in the environment, automatically switches control to the next agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Close SUMO connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RouteRL.services.plotter.Plotter at 0x28af775cbd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RouteRL.services import plotter\n",
    "plotter(params[kc.PLOTTER])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check the folder RouteRL/network_and_config/agents_data.csv and adjust the number of agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be nice if we can have AVs have different colour from human drivers and every time we run sumo to understand which vehicle is which."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
