{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "import torch\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from tqdm import tqdm\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import nn\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "from torchrl.modules import EGreedyModule, QValueModule, SafeSequential\n",
    "from torchrl.modules.models.multiagent import MultiAgentMLP, QMixer, VDNMixer\n",
    "from torchrl.objectives import SoftUpdate, ValueEstimators\n",
    "from torchrl.objectives.multiagent.qmixer import QMixerLoss\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from keychain import Keychain as kc\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "from RouteRL.environment.environment import TrafficEnvironment\n",
    "from RouteRL.services.plotter import Plotter\n",
    "from RouteRL.utilities import get_params\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"device is: \", device)\n",
    "vmas_device = device  # The device where the simulator is run\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 40  # Number of team frames collected per training iteration\n",
    "n_iters = 50  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 100  # Number of optimization steps per training iteration\n",
    "minibatch_size = 2  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "memory_size = 1000  # Size of the replay buffer\n",
    "tau =  0.005\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'params.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc.PARAMS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../RouteRL/network_and_config/two_route_yield/detectors.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc.PATHS_CSV_SAVE_DETECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(kc.PARAMS_PATH)\n",
    "\n",
    "env = TrafficEnvironment(params[kc.RUNNER], params[kc.ENVIRONMENT], params[kc.SIMULATOR], params[kc.AGENT_GEN], params[kc.AGENTS], params[kc.PHASE])\n",
    "\n",
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_list = []\n",
    "for machines in env.machine_agents:\n",
    "    machine_list.append(str(machines.id))\n",
    "      \n",
    "group = {'agents': machine_list}\n",
    "      \n",
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True,\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False,\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        action: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        reward: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "done_spec: CompositeSpec(\n",
      "    done: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    truncated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    agents: CompositeSpec(\n",
      "        done: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        terminated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        truncated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "observation_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        observation: BoundedTensorSpec(\n",
      "            shape=torch.Size([10, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        mask: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "env.group is:  {'agents': ['14', '8', '1', '4', '17', '11', '10', '19', '9', '18']}\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", env.full_action_spec)\n",
    "print(\"reward_spec:\", env.full_reward_spec)\n",
    "print(\"done_spec:\", env.full_done_spec)\n",
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"env.group is: \", env.group_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 15:42:21,472 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs=env.action_spec.space.n,\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=True,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=nn.Tanh,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = TensorDictModule(\n",
    "        net, in_keys=[(\"agents\", \"observation\")], out_keys=[(\"agents\", \"action_value\")]\n",
    "    )\n",
    "\n",
    "value_module = QValueModule(\n",
    "    action_value_key=(\"agents\", \"action_value\"),\n",
    "    out_keys=[\n",
    "        env.action_key,\n",
    "        (\"agents\", \"action_value\"),\n",
    "        (\"agents\", \"chosen_action_value\"),\n",
    "    ],\n",
    "    spec=env.action_spec,\n",
    "    action_space=None,\n",
    ")\n",
    "\n",
    "qnet = SafeSequential(module, value_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_explore = TensorDictSequential(\n",
    "        qnet,\n",
    "        EGreedyModule(\n",
    "            eps_init=0.3,\n",
    "            eps_end=0,\n",
    "            annealing_num_steps=int(total_frames * (1 / 2)),\n",
    "            action_key=env.action_key,\n",
    "            spec=env.action_spec,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixer = TensorDictModule(\n",
    "            module=QMixer(\n",
    "                state_shape=env.observation_spec[\n",
    "                    \"agents\", \"observation\"\n",
    "                ].shape,\n",
    "                mixing_embed_dim=32,\n",
    "                n_agents=env.n_agents,\n",
    "                device=device,\n",
    "            ),\n",
    "            in_keys=[(\"agents\", \"chosen_action_value\"), (\"agents\", \"observation\")],\n",
    "            out_keys=[\"chosen_action_value\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "        env,\n",
    "        qnet_explore,\n",
    "        device=device,\n",
    "        storing_device=device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyTensorStorage(memory_size, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qmix loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = QMixerLoss(qnet, mixer, delay_value=True)\n",
    "loss_module.set_keys(\n",
    "    action_value=(\"agents\", \"action_value\"),\n",
    "    local_value=(\"agents\", \"chosen_action_value\"),\n",
    "    global_value=\"chosen_action_value\",\n",
    "    action=env.action_key,\n",
    ")\n",
    "loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "target_net_updater = SoftUpdate(loss_module, eps=1 - tau)\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 15:42:22,568 [torchrl][INFO] \n",
      "Iteration 0\n",
      "2024-09-30 15:42:49,022 [torchrl][INFO] \n",
      "Iteration 1\n",
      "2024-09-30 15:43:15,743 [torchrl][INFO] \n",
      "Iteration 2\n",
      "2024-09-30 15:43:42,777 [torchrl][INFO] \n",
      "Iteration 3\n",
      "2024-09-30 15:44:09,211 [torchrl][INFO] \n",
      "Iteration 4\n",
      "2024-09-30 15:44:35,059 [torchrl][INFO] \n",
      "Iteration 5\n",
      "2024-09-30 15:45:01,105 [torchrl][INFO] \n",
      "Iteration 6\n",
      "2024-09-30 15:45:29,605 [torchrl][INFO] \n",
      "Iteration 7\n",
      "2024-09-30 15:45:58,858 [torchrl][INFO] \n",
      "Iteration 8\n",
      "2024-09-30 15:46:27,544 [torchrl][INFO] \n",
      "Iteration 9\n",
      "2024-09-30 15:46:57,955 [torchrl][INFO] \n",
      "Iteration 10\n",
      "2024-09-30 15:47:27,132 [torchrl][INFO] \n",
      "Iteration 11\n",
      "2024-09-30 15:47:55,331 [torchrl][INFO] \n",
      "Iteration 12\n",
      "2024-09-30 15:48:24,789 [torchrl][INFO] \n",
      "Iteration 13\n",
      "2024-09-30 15:49:04,054 [torchrl][INFO] \n",
      "Iteration 14\n",
      "2024-09-30 15:49:40,437 [torchrl][INFO] \n",
      "Iteration 15\n",
      "2024-09-30 15:50:18,081 [torchrl][INFO] \n",
      "Iteration 16\n",
      "2024-09-30 15:50:50,701 [torchrl][INFO] \n",
      "Iteration 17\n",
      "2024-09-30 15:51:23,652 [torchrl][INFO] \n",
      "Iteration 18\n",
      "2024-09-30 15:51:52,585 [torchrl][INFO] \n",
      "Iteration 19\n",
      "2024-09-30 15:52:23,070 [torchrl][INFO] \n",
      "Iteration 20\n",
      "2024-09-30 15:52:54,130 [torchrl][INFO] \n",
      "Iteration 21\n",
      "2024-09-30 15:53:24,569 [torchrl][INFO] \n",
      "Iteration 22\n",
      "2024-09-30 15:53:55,170 [torchrl][INFO] \n",
      "Iteration 23\n",
      "2024-09-30 15:54:26,020 [torchrl][INFO] \n",
      "Iteration 24\n",
      "2024-09-30 15:54:55,989 [torchrl][INFO] \n",
      "Iteration 25\n",
      "2024-09-30 15:55:24,239 [torchrl][INFO] \n",
      "Iteration 26\n",
      "2024-09-30 15:55:51,259 [torchrl][INFO] \n",
      "Iteration 27\n",
      "2024-09-30 15:56:18,517 [torchrl][INFO] \n",
      "Iteration 28\n",
      "2024-09-30 15:56:45,276 [torchrl][INFO] \n",
      "Iteration 29\n",
      "2024-09-30 15:57:14,199 [torchrl][INFO] \n",
      "Iteration 30\n",
      "2024-09-30 15:57:41,603 [torchrl][INFO] \n",
      "Iteration 31\n",
      "2024-09-30 15:58:08,611 [torchrl][INFO] \n",
      "Iteration 32\n",
      "2024-09-30 15:58:35,122 [torchrl][INFO] \n",
      "Iteration 33\n",
      "2024-09-30 15:59:02,819 [torchrl][INFO] \n",
      "Iteration 34\n",
      "2024-09-30 15:59:40,221 [torchrl][INFO] \n",
      "Iteration 35\n",
      "2024-09-30 16:00:16,488 [torchrl][INFO] \n",
      "Iteration 36\n",
      "2024-09-30 16:00:51,630 [torchrl][INFO] \n",
      "Iteration 37\n",
      "2024-09-30 16:01:29,005 [torchrl][INFO] \n",
      "Iteration 38\n",
      "2024-09-30 16:02:04,892 [torchrl][INFO] \n",
      "Iteration 39\n",
      "2024-09-30 16:02:42,948 [torchrl][INFO] \n",
      "Iteration 40\n",
      "2024-09-30 16:03:20,787 [torchrl][INFO] \n",
      "Iteration 41\n",
      "2024-09-30 16:04:00,577 [torchrl][INFO] \n",
      "Iteration 42\n",
      "2024-09-30 16:04:39,075 [torchrl][INFO] \n",
      "Iteration 43\n",
      "2024-09-30 16:05:13,092 [torchrl][INFO] \n",
      "Iteration 44\n",
      "2024-09-30 16:05:42,141 [torchrl][INFO] \n",
      "Iteration 45\n",
      "2024-09-30 16:06:08,992 [torchrl][INFO] \n",
      "Iteration 46\n",
      "2024-09-30 16:06:35,681 [torchrl][INFO] \n",
      "Iteration 47\n",
      "2024-09-30 16:07:03,181 [torchrl][INFO] \n",
      "Iteration 48\n",
      "2024-09-30 16:07:31,186 [torchrl][INFO] \n",
      "Iteration 49\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total_time = 0\n",
    "total_frames = 0\n",
    "sampling_start = time.time()\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    torchrl_logger.info(f\"\\nIteration {i}\")\n",
    "\n",
    "    sampling_time = time.time() - sampling_start\n",
    "\n",
    "    # Remove agent dimension from reward (since it is shared in QMIX/VDN)\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"reward\"), tensordict_data.get((\"next\", env.reward_key)).mean(-2)\n",
    "    )\n",
    "    del tensordict_data[\"next\", env.reward_key]\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"episode_reward\"),\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\")).mean(-2),\n",
    "    )\n",
    "    del tensordict_data[\"next\", \"agents\", \"episode_reward\"]\n",
    "\n",
    "    current_frames = tensordict_data.numel()\n",
    "    total_frames += current_frames\n",
    "    data_view = tensordict_data.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    training_tds = []\n",
    "    training_start = time.time()\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "            training_tds.append(loss_vals.detach())\n",
    "\n",
    "            loss_value = loss_vals[\"loss\"]\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )\n",
    "            training_tds[-1].set(\"grad_norm\", total_norm.mean())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            target_net_updater.step()\n",
    "\n",
    "    qnet_explore[1].step(frames=current_frames)  # Update exploration annealing\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "\n",
    "    iteration_time = sampling_time + training_time\n",
    "    total_time += iteration_time\n",
    "    training_tds = torch.stack(training_tds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RouteRL.services.plotter.Plotter at 0x1af1e755b50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RouteRL.services import plotter\n",
    "plotter(params[kc.PLOTTER])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
