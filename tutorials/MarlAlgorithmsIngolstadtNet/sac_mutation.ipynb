{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, we implement the multi-agent versions of the **[SAC](https://arxiv.org/pdf/1801.01290)** algorithm, **Independent SAC (ISAC)** and **Multi-Agent SAC (MASAC)** in our environment. SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy RL framework. \n",
    "\n",
    "\n",
    "> Tutorial based on [SAC TorchRL Tutorial](https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/sac.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We simulate our environment with an initial population of **100 human agents**. These agents navigate the environment and eventually converge on the fastest path. After this convergence, we will transition **23 of these human agents** into **machine agents**, specifically autonomous vehicles (AVs), which will then employ the QMIX reinforcement learning algorithms to further refine their learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run startup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torch.distributions import Categorical, OneHotCategorical\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.modules.models.multiagent import MultiAgentMLP\n",
    "from torchrl.objectives import DiscreteSACLoss, SACLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "from RouteRL import TrafficEnvironment\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"device is: \", device)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 100  # Number of team frames collected per training iteration\n",
    "n_iters = 50  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 1  # Number of optimization steps per training iteration\n",
    "minibatch_size = 16  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-2  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "memory_size =  1000  # Size of the replay buffer\n",
    "tau =  0.005\n",
    "gamma = 0.99  # discount factor\n",
    "\n",
    "policy_net_depth=3\n",
    "policy_net_num_cells=64\n",
    "\n",
    "critic_net_depth=2\n",
    "critic_net_num_cells=64\n",
    "\n",
    "human_learning_episodes = 100\n",
    "\n",
    "\n",
    "# Environment\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"num_agents\" : 100,\n",
    "        \"new_machines_after_mutation\": 20,\n",
    "        \"human_parameters\" : {\n",
    "            \"model\" : \"w_avg\"\n",
    "        },\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"ingolstadt\"\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes],\n",
    "        \"smooth_by\" : 50,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n",
      "TrafficEnvironment with 100 agents.            \n",
      "0 machines and 100 humans.            \n",
      "Machines: []            \n",
      "Humans: [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 24, Human 25, Human 26, Human 27, Human 28, Human 29, Human 30, Human 31, Human 32, Human 33, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 59, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 66, Human 67, Human 68, Human 69, Human 70, Human 71, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 85, Human 86, Human 87, Human 88, Human 89, Human 90, Human 91, Human 92, Human 93, Human 94, Human 95, Human 96, Human 97, Human 98, Human 99]\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(seed=42, **env_params)\n",
    "\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  100 \n",
      "\n",
      "Agents are:  [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 24, Human 25, Human 26, Human 27, Human 28, Human 29, Human 30, Human 31, Human 32, Human 33, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 59, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 66, Human 67, Human 68, Human 69, Human 70, Human 71, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 85, Human 86, Human 87, Human 88, Human 89, Human 90, Human 91, Human 92, Human 93, Human 94, Human 95, Human 96, Human 97, Human 98, Human 99] \n",
      "\n",
      "Number of human agents is:  100 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(human_learning_episodes):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\environment.py:270\u001b[0m, in \u001b[0;36mTrafficEnvironment.step\u001b[1;34m(self, machine_action)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_rewards()\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# If there are only humans in the system\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmachine_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmachine_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_rewards()\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\environment.py:513\u001b[0m, in \u001b[0;36mTrafficEnvironment.simulation_loop\u001b[1;34m(self, machine_action, machine_id)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# If all machines that have start time as the simulator timestep acted\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine_same_start_time: \n\u001b[1;32m--> 513\u001b[0m     travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_help_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_dict \u001b[38;5;129;01min\u001b[39;00m travel_times:\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtravel_times_list\u001b[38;5;241m.\u001b[39mappend(agent_dict)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\environment.py:392\u001b[0m, in \u001b[0;36mTrafficEnvironment._help_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39madd_vehice(action_dict)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_actions[agent\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m action_dict\n\u001b[1;32m--> 392\u001b[0m timestep, arrivals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m    394\u001b[0m travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m veh_id \u001b[38;5;129;01min\u001b[39;00m arrivals:\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\simulator.py:335\u001b[0m, in \u001b[0;36mSumoSimulator.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Advances the SUMO simulation by one timestep and retrieves information\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    about vehicle arrivals and detector data.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m        arrivals (list): List of vehicle IDs that arrived at their destinations during the current timestep.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m     arrivals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetArrivedIDList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo_connection\u001b[38;5;241m.\u001b[39msimulationStep()\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_simulation.py:335\u001b[0m, in \u001b[0;36mSimulationDomain.getArrivedIDList\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetArrivedIDList\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"getArrivedIDList() -> list(string)\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    Returns a list of ids of vehicles which arrived (have reached their destination and are removed from the road\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    network) in this time step.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAR_ARRIVED_VEHICLES_IDS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:147\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:152\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m r\u001b[38;5;241m.\u001b[39mreadLength()\n\u001b[0;32m    154\u001b[0m response, retVarID \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39msend(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\n\u001b[1;32m--> 131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceiving\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mgetDebugString())\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[0m, in \u001b[0;36mConnection._recvExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 109\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(human_learning_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mutation**: a portion of human agents are converted into machine agents (autonomous vehicles). You can adjust the number of agents to be mutated in the <code style=\"color:white\">/params.json</code> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a group that contains all the machine (RL) agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_list = []\n",
    "for machines in env.machine_agents:\n",
    "    machine_list.append(str(machines.id))\n",
    "      \n",
    "group = {'agents': machine_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True, # Whether to use the mask in the outputs. It is important for AEC environments to mask out non-acting agents.\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False, # Whether the environmentâ€™s done keys are set by aggregating the agent keys using any() (when True) or all() (when False).\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Agent group mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"env.group is: \", env.group_map, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can append any TorchRL transform we need to our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code style=\"color:white\">check_env_specs()</code> function runs a small rollout and compared it output against the environment specs. It will raise an error if the specs aren't properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instantiate an `MPL` that can be used in multi-agent contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = False \n",
    "\n",
    "actor_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs = env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs = env.action_spec.space.n,\n",
    "        n_agents = env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=policy_net_depth,\n",
    "        num_cells=policy_net_num_cells,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"logits\")],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[(\"agents\", \"logits\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=Categorical,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The critic reads the observations and returns the corresponding value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralised_critic = False\n",
    "shared_parameters = False\n",
    "\n",
    "module = MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=env.action_spec.space.n,\n",
    "            n_agents=env.n_agents,\n",
    "            centralised=centralised_critic,\n",
    "            share_params=shared_parameters,\n",
    "            device=device,\n",
    "            depth=critic_net_depth,\n",
    "            num_cells=critic_net_num_cells,\n",
    "            activation_class=nn.Tanh,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_module = ValueOperator(\n",
    "            module=module,\n",
    "            in_keys=[(\"agents\", \"observation\")],\n",
    "            out_keys=[(\"agents\", \"action_value\")],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyTensorStorage(memory_size, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAC loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = DiscreteSACLoss(\n",
    "            actor_network=policy,\n",
    "            qvalue_network=value_module,\n",
    "            delay_qvalue=True,\n",
    "            num_actions=env.action_spec.space.n,\n",
    "            action_space=env.action_spec,\n",
    "        )\n",
    "\n",
    "loss_module.set_keys(\n",
    "    action_value=(\"agents\", \"action_value\"),\n",
    "    action=env.action_key,\n",
    "    reward=env.reward_key,\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "target_net_updater = SoftUpdate(loss_module, eps=1 - tau)\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tensordict_data in tqdm(enumerate(collector), total=n_iters, desc=\"Training\"):\n",
    "\n",
    "\n",
    "    current_frames = tensordict_data.numel()\n",
    "    total_frames += current_frames\n",
    "    data_view = tensordict_data.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    training_tds = []\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "            training_tds.append(loss_vals.detach())\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_actor\"]\n",
    "                + loss_vals[\"loss_alpha\"]\n",
    "                + loss_vals[\"loss_qvalue\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )\n",
    "            training_tds[-1].set(\"grad_norm\", total_norm.mean())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            target_net_updater.step()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    training_tds = torch.stack(training_tds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Check `\\plots` directory to find the plots created from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
