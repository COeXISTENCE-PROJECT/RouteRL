{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, we implement the **[Independent Q learning]()** Multi Agent Reinforcement Leaning (MARL) algorithm in our environment. \n",
    "\n",
    "\n",
    "> Tutorial based on [IQL TorchRL Tutorial](https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/iql.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level overview of IQL algorithm\n",
    "\n",
    "In IQL a centralized state-action value function is used, Q<sub>tot</sub>, and each agent α learns an individual action-value function Q<sub>α</sub>, independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We simulate our environment with an initial population of **200 human agents**. These agents navigate the environment and eventually converge on the fastest path. After this convergence, we will transition **50 of these human agents** into **machine agents**, specifically autonomous vehicles (AVs), which will then employ the Independent Q learning reinforcement learning algorithm to further refine their learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run startup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torch import nn\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import EGreedyModule, QValueModule, SafeSequential\n",
    "from torchrl.modules.models.multiagent import MultiAgentMLP\n",
    "from torchrl.objectives import SoftUpdate, ValueEstimators, DQNLoss\n",
    "\n",
    "from RouteRL import TrafficEnvironment\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"device is: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "frames_per_batch = 100  # Number of team frames collected per training iteration\n",
    "n_iters = 50  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 1  # Number of optimization steps per training iteration \n",
    "minibatch_size = 16  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-3 # Learning rate\n",
    "max_grad_norm = 5.0  # Maximum norm for the gradients\n",
    "memory_size = 5_000  # Size of the replay buffer\n",
    "tau =  0.05\n",
    "gamma = 0.99  # discount factor\n",
    "exploration_fraction = 1/3 # Fraction of frames over which the exploration rate is annealed\n",
    "\n",
    "eps = 1 - tau\n",
    "eps_init = 0.99\n",
    "eps_end = 0\n",
    "\n",
    "mlp_depth = 2\n",
    "mlp_cells = 32\n",
    "\n",
    "# Human learning phase\n",
    "human_learning_episodes = 100\n",
    "\n",
    "# Environment\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"num_agents\" : 200,\n",
    "        \"new_machines_after_mutation\": 50,\n",
    "        \"human_parameters\" : {\n",
    "            \"model\" : \"w_avg\"\n",
    "        },\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"ingolstadt\"\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes],\n",
    "        \"smooth_by\" : 50,\n",
    "    },\n",
    "    \"path_generation_parameters\":\n",
    "    {\n",
    "        \"number_of_paths\" : 5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n",
      "TrafficEnvironment with 200 agents.            \n",
      "0 machines and 200 humans.            \n",
      "Machines: []            \n",
      "Humans: [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 24, Human 25, Human 26, Human 27, Human 28, Human 29, Human 30, Human 31, Human 32, Human 33, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 59, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 66, Human 67, Human 68, Human 69, Human 70, Human 71, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 85, Human 86, Human 87, Human 88, Human 89, Human 90, Human 91, Human 92, Human 93, Human 94, Human 95, Human 96, Human 97, Human 98, Human 99, Human 100, Human 101, Human 102, Human 103, Human 104, Human 105, Human 106, Human 107, Human 108, Human 109, Human 110, Human 111, Human 112, Human 113, Human 114, Human 115, Human 116, Human 117, Human 118, Human 119, Human 120, Human 121, Human 122, Human 123, Human 124, Human 125, Human 126, Human 127, Human 128, Human 129, Human 130, Human 131, Human 132, Human 133, Human 134, Human 135, Human 136, Human 137, Human 138, Human 139, Human 140, Human 141, Human 142, Human 143, Human 144, Human 145, Human 146, Human 147, Human 148, Human 149, Human 150, Human 151, Human 152, Human 153, Human 154, Human 155, Human 156, Human 157, Human 158, Human 159, Human 160, Human 161, Human 162, Human 163, Human 164, Human 165, Human 166, Human 167, Human 168, Human 169, Human 170, Human 171, Human 172, Human 173, Human 174, Human 175, Human 176, Human 177, Human 178, Human 179, Human 180, Human 181, Human 182, Human 183, Human 184, Human 185, Human 186, Human 187, Human 188, Human 189, Human 190, Human 191, Human 192, Human 193, Human 194, Human 195, Human 196, Human 197, Human 198, Human 199]\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(seed=42, **env_params)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(human_learning_episodes):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\environment.py:290\u001b[0m, in \u001b[0;36mTrafficEnvironment.step\u001b[1;34m(self, machine_action)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_rewards()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# If there are only humans in the system\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmachine_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmachine_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_rewards()\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\environment.py:532\u001b[0m, in \u001b[0;36mTrafficEnvironment.simulation_loop\u001b[1;34m(self, machine_action, machine_id)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# If all machines that have start time as the simulator timestep acted\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine_same_start_time: \n\u001b[1;32m--> 532\u001b[0m     travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_help_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_dict \u001b[38;5;129;01min\u001b[39;00m travel_times:\n\u001b[0;32m    535\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtravel_times_list\u001b[38;5;241m.\u001b[39mappend(agent_dict)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\environment.py:412\u001b[0m, in \u001b[0;36mTrafficEnvironment._help_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39madd_vehice(action_dict)\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_actions[agent\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m action_dict\n\u001b[1;32m--> 412\u001b[0m timestep, arrivals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m    414\u001b[0m travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m veh_id \u001b[38;5;129;01min\u001b[39;00m arrivals:\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL\\RouteRL\\environment\\simulator.py:333\u001b[0m, in \u001b[0;36mSumoSimulator.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Advances the SUMO simulation by one timestep and retrieves information\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03mabout vehicle arrivals and detector data.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m    arrivals (list): List of vehicle IDs that arrived at their destinations during the current timestep.\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m arrivals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo_connection\u001b[38;5;241m.\u001b[39msimulation\u001b[38;5;241m.\u001b[39mgetArrivedIDList()\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep, arrivals\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:368\u001b[0m, in \u001b[0;36mConnection.simulationStep\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m    367\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI change now handles step as floating point seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 368\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptionMapping\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    370\u001b[0m     subscriptionResults\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39msend(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\n\u001b[1;32m--> 131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceiving\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mgetDebugString())\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[0m, in \u001b[0;36mConnection._recvExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 109\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(human_learning_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mutation**: a portion of human agents are converted into machine agents (autonomous vehicles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrafficEnvironment with 200 agents.            \n",
      "50 machines and 150 humans.            \n",
      "Machines: [Machine 3, Machine 4, Machine 24, Machine 27, Machine 32, Machine 33, Machine 50, Machine 51, Machine 52, Machine 55, Machine 57, Machine 58, Machine 62, Machine 64, Machine 65, Machine 68, Machine 72, Machine 73, Machine 77, Machine 83, Machine 85, Machine 86, Machine 89, Machine 90, Machine 93, Machine 94, Machine 98, Machine 99, Machine 100, Machine 105, Machine 107, Machine 114, Machine 118, Machine 122, Machine 125, Machine 130, Machine 131, Machine 146, Machine 151, Machine 152, Machine 155, Machine 157, Machine 165, Machine 168, Machine 178, Machine 181, Machine 182, Machine 185, Machine 187, Machine 191]            \n",
      "Humans: [Human 0, Human 1, Human 2, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 25, Human 26, Human 28, Human 29, Human 30, Human 31, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 53, Human 54, Human 56, Human 59, Human 60, Human 61, Human 63, Human 66, Human 67, Human 69, Human 70, Human 71, Human 74, Human 75, Human 76, Human 78, Human 79, Human 80, Human 81, Human 82, Human 84, Human 87, Human 88, Human 91, Human 92, Human 95, Human 96, Human 97, Human 101, Human 102, Human 103, Human 104, Human 106, Human 108, Human 109, Human 110, Human 111, Human 112, Human 113, Human 115, Human 116, Human 117, Human 119, Human 120, Human 121, Human 123, Human 124, Human 126, Human 127, Human 128, Human 129, Human 132, Human 133, Human 134, Human 135, Human 136, Human 137, Human 138, Human 139, Human 140, Human 141, Human 142, Human 143, Human 144, Human 145, Human 147, Human 148, Human 149, Human 150, Human 153, Human 154, Human 156, Human 158, Human 159, Human 160, Human 161, Human 162, Human 163, Human 164, Human 166, Human 167, Human 169, Human 170, Human 171, Human 172, Human 173, Human 174, Human 175, Human 176, Human 177, Human 179, Human 180, Human 183, Human 184, Human 186, Human 188, Human 189, Human 190, Human 192, Human 193, Human 194, Human 195, Human 196, Human 197, Human 198, Human 199]\n"
     ]
    }
   ],
   "source": [
    "env.mutation()\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a group that contains all the machine (RL) agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = {'agents': [str(machine.id) for machine in env.machine_agents]}\n",
    "\n",
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True,\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False,\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instatiate a <code style=\"color:white\">RewardSum</code> transformer that will sum rewards over episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code style=\"color:white\">check_env_specs()</code> function runs a small rollout and compared it output against the environment specs. It will raise an error if the specs aren't properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 16:45:48,667 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        agents: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                mask: Tensor(shape=torch.Size([50]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([50, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([50]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_env_specs(env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instantiate an `MPL` that can be used in multi-agent contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs=env.action_spec.space.n,\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=False,\n",
    "        device=device,\n",
    "        depth=mlp_depth,\n",
    "        num_cells=mlp_cells,\n",
    "        activation_class=nn.ReLU,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = TensorDictModule(\n",
    "        net, in_keys=[(\"agents\", \"observation\")], out_keys=[(\"agents\", \"action_value\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_module = QValueModule(\n",
    "    action_value_key=(\"agents\", \"action_value\"),\n",
    "    out_keys=[\n",
    "        env.action_key,\n",
    "        (\"agents\", \"action_value\"),\n",
    "        (\"agents\", \"chosen_action_value\"),\n",
    "    ],\n",
    "    spec=env.action_spec,\n",
    "    action_space=None,\n",
    ")\n",
    "\n",
    "qnet = SafeSequential(module, value_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_explore = TensorDictSequential(\n",
    "    qnet,\n",
    "    EGreedyModule(\n",
    "        eps_init=eps_init,\n",
    "        eps_end=eps_end,\n",
    "        annealing_num_steps=int(total_frames * exploration_fraction),\n",
    "        action_key=env.action_key,\n",
    "        spec=env.action_spec,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "        env,\n",
    "        qnet_explore,\n",
    "        device=device,\n",
    "        storing_device=device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyTensorStorage(memory_size, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = DQNLoss(qnet, delay_value=True)\n",
    "\n",
    "loss_module.set_keys(\n",
    "        action_value=(\"agents\", \"action_value\"),\n",
    "        action=env.action_key,\n",
    "        value=(\"agents\", \"chosen_action_value\"),\n",
    "        reward=env.reward_key,\n",
    "        done=(\"agents\", \"done\"),\n",
    "        terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "target_net_updater = SoftUpdate(loss_module, eps=eps)\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 16:46:04,061 [torchrl][INFO] \n",
      "Iteration 0\n",
      "2025-02-04 16:46:17,957 [torchrl][INFO] \n",
      "Iteration 1\n",
      "2025-02-04 16:46:31,547 [torchrl][INFO] \n",
      "Iteration 2\n",
      "2025-02-04 16:46:45,297 [torchrl][INFO] \n",
      "Iteration 3\n",
      "2025-02-04 16:47:00,910 [torchrl][INFO] \n",
      "Iteration 4\n",
      "2025-02-04 16:47:14,157 [torchrl][INFO] \n",
      "Iteration 5\n",
      "2025-02-04 16:47:26,303 [torchrl][INFO] \n",
      "Iteration 6\n",
      "2025-02-04 16:47:39,802 [torchrl][INFO] \n",
      "Iteration 7\n",
      "2025-02-04 16:47:49,051 [torchrl][INFO] \n",
      "Iteration 8\n",
      "2025-02-04 16:47:57,282 [torchrl][INFO] \n",
      "Iteration 9\n",
      "2025-02-04 16:48:06,842 [torchrl][INFO] \n",
      "Iteration 10\n",
      "2025-02-04 16:48:17,159 [torchrl][INFO] \n",
      "Iteration 11\n",
      "2025-02-04 16:48:25,218 [torchrl][INFO] \n",
      "Iteration 12\n",
      "2025-02-04 16:48:34,644 [torchrl][INFO] \n",
      "Iteration 13\n",
      "2025-02-04 16:48:43,757 [torchrl][INFO] \n",
      "Iteration 14\n",
      "2025-02-04 16:48:53,119 [torchrl][INFO] \n",
      "Iteration 15\n",
      "2025-02-04 16:49:02,829 [torchrl][INFO] \n",
      "Iteration 16\n",
      "2025-02-04 16:49:11,830 [torchrl][INFO] \n",
      "Iteration 17\n",
      "2025-02-04 16:49:20,838 [torchrl][INFO] \n",
      "Iteration 18\n",
      "2025-02-04 16:49:29,754 [torchrl][INFO] \n",
      "Iteration 19\n",
      "2025-02-04 16:49:38,404 [torchrl][INFO] \n",
      "Iteration 20\n",
      "2025-02-04 16:49:47,417 [torchrl][INFO] \n",
      "Iteration 21\n",
      "2025-02-04 16:49:56,657 [torchrl][INFO] \n",
      "Iteration 22\n",
      "2025-02-04 16:50:05,380 [torchrl][INFO] \n",
      "Iteration 23\n",
      "2025-02-04 16:50:14,610 [torchrl][INFO] \n",
      "Iteration 24\n",
      "2025-02-04 16:50:23,370 [torchrl][INFO] \n",
      "Iteration 25\n",
      "2025-02-04 16:50:32,355 [torchrl][INFO] \n",
      "Iteration 26\n",
      "2025-02-04 16:50:41,517 [torchrl][INFO] \n",
      "Iteration 27\n",
      "2025-02-04 16:50:51,693 [torchrl][INFO] \n",
      "Iteration 28\n",
      "2025-02-04 16:51:00,815 [torchrl][INFO] \n",
      "Iteration 29\n",
      "2025-02-04 16:51:11,237 [torchrl][INFO] \n",
      "Iteration 30\n",
      "2025-02-04 16:51:22,708 [torchrl][INFO] \n",
      "Iteration 31\n",
      "2025-02-04 16:51:34,766 [torchrl][INFO] \n",
      "Iteration 32\n",
      "2025-02-04 16:51:55,552 [torchrl][INFO] \n",
      "Iteration 33\n",
      "2025-02-04 16:52:09,683 [torchrl][INFO] \n",
      "Iteration 34\n",
      "2025-02-04 16:52:17,141 [torchrl][INFO] \n",
      "Iteration 35\n",
      "2025-02-04 16:52:25,540 [torchrl][INFO] \n",
      "Iteration 36\n",
      "2025-02-04 16:52:33,731 [torchrl][INFO] \n",
      "Iteration 37\n",
      "2025-02-04 16:52:45,159 [torchrl][INFO] \n",
      "Iteration 38\n",
      "2025-02-04 16:52:55,919 [torchrl][INFO] \n",
      "Iteration 39\n",
      "2025-02-04 16:53:09,528 [torchrl][INFO] \n",
      "Iteration 40\n",
      "2025-02-04 16:53:25,189 [torchrl][INFO] \n",
      "Iteration 41\n",
      "2025-02-04 16:53:43,883 [torchrl][INFO] \n",
      "Iteration 42\n",
      "2025-02-04 16:53:56,145 [torchrl][INFO] \n",
      "Iteration 43\n",
      "2025-02-04 16:54:05,868 [torchrl][INFO] \n",
      "Iteration 44\n",
      "2025-02-04 16:54:14,481 [torchrl][INFO] \n",
      "Iteration 45\n",
      "2025-02-04 16:54:24,349 [torchrl][INFO] \n",
      "Iteration 46\n",
      "2025-02-04 16:54:33,827 [torchrl][INFO] \n",
      "Iteration 47\n",
      "2025-02-04 16:54:44,643 [torchrl][INFO] \n",
      "Iteration 48\n",
      "2025-02-04 16:54:54,767 [torchrl][INFO] \n",
      "Iteration 49\n"
     ]
    }
   ],
   "source": [
    "for i, tensordict_data in tqdm(enumerate(collector), total=n_iters, desc=\"Training\"):\n",
    "\n",
    "    current_frames = tensordict_data.numel()\n",
    "    data_view = tensordict_data.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "    \n",
    "    training_tds = []\n",
    "\n",
    "    ## Update the policies of the learning agents\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "            training_tds.append(loss_vals.detach())\n",
    "\n",
    "            loss_value = loss_vals[\"loss\"]\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            training_tds[-1].set(\"grad_norm\", total_norm.mean())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        target_net_updater.step()\n",
    "\n",
    "    qnet_explore[1].step(frames=current_frames)  # Update exploration annealing\n",
    "    collector.update_policy_weights_()\n",
    "    \n",
    "    training_tds = torch.stack(training_tds) \n",
    "\n",
    "collector.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plots of the training process are include in the **\\plots** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plots of the training process are include in the **\\plots** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plots of the training process are include in the **\\plots** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
