{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "import torch\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from tqdm import tqdm\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import nn\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "from torchrl.modules import EGreedyModule, QValueModule, SafeSequential\n",
    "from torchrl.modules.models.multiagent import MultiAgentMLP, QMixer, VDNMixer\n",
    "from torchrl.objectives import SoftUpdate, ValueEstimators\n",
    "from torchrl.objectives.multiagent.qmixer import QMixerLoss\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "from RouteRL.keychain import Keychain as kc\n",
    "from RouteRL.environment.environment import TrafficEnvironment\n",
    "from RouteRL.services.plotter import Plotter\n",
    "from RouteRL.utilities import get_params\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"device is: \", device)\n",
    "vmas_device = device  # The device where the simulator is run\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 40  # Number of team frames collected per training iteration\n",
    "n_iters = 50  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 100  # Number of optimization steps per training iteration\n",
    "minibatch_size = 2  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "memory_size = 1000  # Size of the replay buffer\n",
    "tau =  0.005\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(\"params.json\")\n",
    "\n",
    "env = TrafficEnvironment(params[kc.RUNNER], params[kc.ENVIRONMENT], params[kc.SIMULATOR], params[kc.AGENT_GEN], params[kc.AGENTS], params[kc.PLOTTER])\n",
    "\n",
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_list = []\n",
    "for machines in env.machine_agents:\n",
    "    machine_list.append(str(machines.id))\n",
    "      \n",
    "group = {'agents': machine_list}\n",
    "      \n",
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True,\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False,\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agents': ['15', '4', '5', '18', '14', '7', '2', '1', '19', '11']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        action: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        reward: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "done_spec: CompositeSpec(\n",
      "    done: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    truncated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    agents: CompositeSpec(\n",
      "        done: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        terminated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        truncated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "observation_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        observation: BoundedTensorSpec(\n",
      "            shape=torch.Size([10, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        mask: DiscreteTensorSpec(\n",
      "            shape=torch.Size([10]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([10])), device=cpu, shape=torch.Size([]))\n",
      "env.group is:  {'agents': ['15', '4', '5', '18', '14', '7', '2', '1', '19', '11']}\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", env.full_action_spec)\n",
    "print(\"reward_spec:\", env.full_reward_spec)\n",
    "print(\"done_spec:\", env.full_done_spec)\n",
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"env.group is: \", env.group_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 15:54:58,695 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs=env.action_spec.space.n,\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=True,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=nn.Tanh,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiAgentMLP(\n",
       "  (agent_networks): ModuleList(\n",
       "    (0): MLP(\n",
       "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): Tanh()\n",
       "      (4): Linear(in_features=256, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = TensorDictModule(\n",
    "        net, in_keys=[(\"agents\", \"observation\")], out_keys=[(\"agents\", \"action_value\")]\n",
    "    )\n",
    "\n",
    "value_module = QValueModule(\n",
    "    action_value_key=(\"agents\", \"action_value\"),\n",
    "    out_keys=[\n",
    "        env.action_key,\n",
    "        (\"agents\", \"action_value\"),\n",
    "        (\"agents\", \"chosen_action_value\"),\n",
    "    ],\n",
    "    spec=env.action_spec,\n",
    "    action_space=None,\n",
    ")\n",
    "\n",
    "qnet = SafeSequential(module, value_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_explore = TensorDictSequential(\n",
    "        qnet,\n",
    "        EGreedyModule(\n",
    "            eps_init=0.3,\n",
    "            eps_end=0,\n",
    "            annealing_num_steps=int(total_frames * (1 / 2)),\n",
    "            action_key=env.action_key,\n",
    "            spec=env.action_spec,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixer = TensorDictModule(\n",
    "            module=QMixer(\n",
    "                state_shape=env.observation_spec[\n",
    "                    \"agents\", \"observation\"\n",
    "                ].shape,\n",
    "                mixing_embed_dim=32,\n",
    "                n_agents=env.n_agents,\n",
    "                device=device,\n",
    "            ),\n",
    "            in_keys=[(\"agents\", \"chosen_action_value\"), (\"agents\", \"observation\")],\n",
    "            out_keys=[\"chosen_action_value\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictModule(\n",
       "    module=QMixer(\n",
       "      (hyper_w_1): Linear(in_features=20, out_features=320, bias=True)\n",
       "      (hyper_w_final): Linear(in_features=20, out_features=32, bias=True)\n",
       "      (hyper_b_1): Linear(in_features=20, out_features=32, bias=True)\n",
       "      (V): Sequential(\n",
       "        (0): Linear(in_features=20, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=[('agents', 'chosen_action_value'), ('agents', 'observation')],\n",
       "    out_keys=['chosen_action_value'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "        env,\n",
    "        qnet_explore,\n",
    "        device=device,\n",
    "        storing_device=device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyTensorStorage(memory_size, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qmix loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = QMixerLoss(qnet, mixer, delay_value=True)\n",
    "loss_module.set_keys(\n",
    "    action_value=(\"agents\", \"action_value\"),\n",
    "    local_value=(\"agents\", \"chosen_action_value\"),\n",
    "    global_value=\"chosen_action_value\",\n",
    "    action=env.action_key,\n",
    ")\n",
    "loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "target_net_updater = SoftUpdate(loss_module, eps=1 - tau)\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m total_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m sampling_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorchrl_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mIteration \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msampling_start\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:953\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_frames:\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 953\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tensordict_out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total_frames:\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\_utils.py:470\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[1;32m--> 470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:1071\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1070\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[1;32m-> 1071\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\common.py:2576\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_and_maybe_reset\u001b[39m(\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase\n\u001b[0;32m   2536\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001b[39;00m\n\u001b[0;32m   2538\u001b[0m \n\u001b[0;32m   2539\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;124;03m            is_shared=False)\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2576\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m     tensordict_ \u001b[38;5;241m=\u001b[39m step_mdp(\n\u001b[0;32m   2580\u001b[0m         tensordict,\n\u001b[0;32m   2581\u001b[0m         keep_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2586\u001b[0m         done_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_keys,\n\u001b[0;32m   2587\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\common.py:1408\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[0;32m   1407\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1408\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1409\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\transforms\\transforms.py:738\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    736\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    737\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[1;32m--> 738\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    744\u001b[0m         next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    745\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:616\u001b[0m, in \u001b[0;36mPettingZooWrapper._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     (\n\u001b[0;32m    603\u001b[0m         observation_dict,\n\u001b[0;32m    604\u001b[0m         rewards_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         info_dict,\n\u001b[0;32m    608\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_parallel(tensordict)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m     (\n\u001b[0;32m    611\u001b[0m         observation_dict,\n\u001b[0;32m    612\u001b[0m         rewards_dict,\n\u001b[0;32m    613\u001b[0m         terminations_dict,\n\u001b[0;32m    614\u001b[0m         truncations_dict,\n\u001b[0;32m    615\u001b[0m         info_dict,\n\u001b[1;32m--> 616\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_aec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# We start with zeroed data and fill in the data for alive agents\u001b[39;00m\n\u001b[0;32m    620\u001b[0m tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_step_output_zero\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:766\u001b[0m, in \u001b[0;36mPettingZooWrapper._step_aec\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    763\u001b[0m         action \u001b[38;5;241m=\u001b[39m group_action_np[agent_index]\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m terminations_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mterminations\n\u001b[0;32m    768\u001b[0m truncations_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mtruncations\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\OneDrive - Uniwersytet Jagielloński\\Documents\\torch-rl-trials\\Milestone-One\\RouteRL\\environment\\environment.py:224\u001b[0m, in \u001b[0;36mTrafficEnvironment.step\u001b[1;34m(self, machine_action)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# The cumulative reward of the last agent must be 0\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cumulative_rewards[agent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmachine_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Collect rewards if it is the last agent to act\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_selector\u001b[38;5;241m.\u001b[39mis_last(): \n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\OneDrive - Uniwersytet Jagielloński\\Documents\\torch-rl-trials\\Milestone-One\\RouteRL\\environment\\environment.py:518\u001b[0m, in \u001b[0;36mTrafficEnvironment.simulation_loop\u001b[1;34m(self, machine_action, machine_id)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;66;03m# If all machines that have start time as the simulator timestep acted\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine_same_start_time: \n\u001b[1;32m--> 518\u001b[0m     travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelp_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_dict \u001b[38;5;129;01min\u001b[39;00m travel_times:\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtravel_times_list\u001b[38;5;241m.\u001b[39mappend(agent_dict)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\OneDrive - Uniwersytet Jagielloński\\Documents\\torch-rl-trials\\Milestone-One\\RouteRL\\environment\\environment.py:382\u001b[0m, in \u001b[0;36mTrafficEnvironment.help_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m veh_id \u001b[38;5;129;01min\u001b[39;00m arrivals:\n\u001b[0;32m    381\u001b[0m     agent_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(veh_id)\n\u001b[1;32m--> 382\u001b[0m     travel_times[agent_id] \u001b[38;5;241m=\u001b[39m {kc\u001b[38;5;241m.\u001b[39mTRAVEL_TIME : (timestep \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m[kc\u001b[38;5;241m.\u001b[39mAGENT_START_TIME]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60.0\u001b[39m}\n\u001b[0;32m    383\u001b[0m     travel_times[agent_id]\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_actions[agent_id])\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m travel_times\u001b[38;5;241m.\u001b[39mvalues()\n",
      "\u001b[1;31mKeyError\u001b[0m: 12"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total_time = 0\n",
    "total_frames = 0\n",
    "sampling_start = time.time()\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    torchrl_logger.info(f\"\\nIteration {i}\")\n",
    "\n",
    "    sampling_time = time.time() - sampling_start\n",
    "\n",
    "    # Remove agent dimension from reward (since it is shared in QMIX/VDN)\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"reward\"), tensordict_data.get((\"next\", env.reward_key)).mean(-2)\n",
    "    )\n",
    "    del tensordict_data[\"next\", env.reward_key]\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"episode_reward\"),\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\")).mean(-2),\n",
    "    )\n",
    "    del tensordict_data[\"next\", \"agents\", \"episode_reward\"]\n",
    "\n",
    "    current_frames = tensordict_data.numel()\n",
    "    total_frames += current_frames\n",
    "    data_view = tensordict_data.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    training_tds = []\n",
    "    training_start = time.time()\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "            training_tds.append(loss_vals.detach())\n",
    "\n",
    "            loss_value = loss_vals[\"loss\"]\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )\n",
    "            training_tds[-1].set(\"grad_norm\", total_norm.mean())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            target_net_updater.step()\n",
    "\n",
    "    qnet_explore[1].step(frames=current_frames)  # Update exploration annealing\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "\n",
    "    iteration_time = sampling_time + training_time\n",
    "    total_time += iteration_time\n",
    "    training_tds = torch.stack(training_tds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RouteRL.services.plotter.Plotter at 0x1af1e755b50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RouteRL.services import plotter\n",
    "plotter(params[kc.PLOTTER])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
