{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RouteRL Quickstart\n",
    "\n",
    "We simulate a simple network topology where humans and later AVs make routing decisions to maximize their rewards (i.e., minimize travel times) over a sequence of days.\n",
    "\n",
    "* For the first 100 days, we model a human-driven system, where drivers update their routing policies using behavioral models to optimize rewards.\n",
    "* Each day, we simulate the impact of joint actions using the [`SUMO`](https://eclipse.dev/sumo/) traffic simulator, which returns the reward for each agent.\n",
    "* After 100 days, we introduce 10 `Autononmous Vehicles` as `Petting Zoo` agents, allowing them to use any `MARL` algorithm to maximise rewards.\n",
    "* Finally, we analyse basic results from the simulation.\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../../docs/img/two_route_net_1.png\" alt=\"Two-route network\" />\n",
    "  <img src=\"../../docs/img/two_route_net_1_2.png\" alt=\"Two-route network\" />\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "from routerl import TrafficEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters\n",
    "\n",
    "> Further parameters customization can take place by modifying the entries of the `routerl/environment/params.json`. Users can create a dictionary with their preferred adjustments and pass it as an argument to the `TrafficEnvironment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_learning_episodes = 100\n",
    "\n",
    "\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"num_agents\" : 100,\n",
    "        \"new_machines_after_mutation\": 10,\n",
    "        \"human_parameters\" : {\n",
    "            \"model\" : \"gawron\"\n",
    "        },\n",
    "        \"machine_parameters\" :\n",
    "        {\n",
    "            \"behavior\" : \"selfish\",\n",
    "        }\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"two_route_yield\"\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes],\n",
    "        \"smooth_by\" : 50,\n",
    "    },\n",
    "    \"path_generation_parameters\":\n",
    "    {\n",
    "        \"number_of_paths\" : 3,\n",
    "        \"beta\" : -5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our setup, road networks initially consist of human agents, with AVs introduced later.\n",
    "\n",
    "- The `TrafficEnvironment` environment is firstly initialized.\n",
    "- The traffic network is instantiated and the paths between designated origin and destination points are determined.\n",
    "- The drivers/agents objects are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(seed=42, **env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p >\n",
    "  <img src=\"plots_saved/0_0.png\" width=\"600\" />\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  100 \n",
      "\n",
      "Number of human agents is:  100 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(human_learning_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"plots_saved/human_learning.png\"/>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mutation: a portion of human agents are converted into machine agents (autonomous vehicles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  100 \n",
      "\n",
      "Number of human agents is:  90 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Machine 1,\n",
       " Machine 15,\n",
       " Machine 10,\n",
       " Machine 91,\n",
       " Machine 22,\n",
       " Machine 73,\n",
       " Machine 5,\n",
       " Machine 52,\n",
       " Machine 81,\n",
       " Machine 77]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.machine_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Human and AV agents interact with the environment over multiple episodes, with AVs following a random policy as defined in the PettingZoo environment [loop](https://pettingzoo.farama.org/content/basic_usage/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo stepping loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting episode 1\n",
      "\n",
      "Starting episode 2\n",
      "\n",
      "Starting episode 3\n",
      "\n",
      "Starting episode 4\n",
      "\n",
      "Starting episode 5\n",
      "\n",
      "Starting episode 6\n",
      "\n",
      "Starting episode 7\n",
      "\n",
      "Starting episode 8\n",
      "\n",
      "Starting episode 9\n",
      "\n",
      "Starting episode 10\n",
      "\n",
      "Starting episode 11\n",
      "\n",
      "Starting episode 12\n",
      "\n",
      "Starting episode 13\n",
      "\n",
      "Starting episode 14\n",
      "\n",
      "Starting episode 15\n",
      "\n",
      "Starting episode 16\n",
      "\n",
      "Starting episode 17\n",
      "\n",
      "Starting episode 18\n",
      "\n",
      "Starting episode 19\n",
      "\n",
      "Starting episode 20\n",
      "\n",
      "Starting episode 21\n",
      "\n",
      "Starting episode 22\n",
      "\n",
      "Starting episode 23\n",
      "\n",
      "Starting episode 24\n",
      "\n",
      "Starting episode 25\n",
      "\n",
      "Starting episode 26\n",
      "\n",
      "Starting episode 27\n",
      "\n",
      "Starting episode 28\n",
      "\n",
      "Starting episode 29\n",
      "\n",
      "Starting episode 30\n",
      "\n",
      "Starting episode 31\n",
      "\n",
      "Starting episode 32\n",
      "\n",
      "Starting episode 33\n",
      "\n",
      "Starting episode 34\n",
      "\n",
      "Starting episode 35\n",
      "\n",
      "Starting episode 36\n",
      "\n",
      "Starting episode 37\n",
      "\n",
      "Starting episode 38\n",
      "\n",
      "Starting episode 39\n",
      "\n",
      "Starting episode 40\n",
      "\n",
      "Starting episode 41\n",
      "\n",
      "Starting episode 42\n",
      "\n",
      "Starting episode 43\n",
      "\n",
      "Starting episode 44\n",
      "\n",
      "Starting episode 45\n",
      "\n",
      "Starting episode 46\n",
      "\n",
      "Starting episode 47\n",
      "\n",
      "Starting episode 48\n",
      "\n",
      "Starting episode 49\n",
      "\n",
      "Starting episode 50\n",
      "\n",
      "Starting episode 51\n",
      "\n",
      "Starting episode 52\n",
      "\n",
      "Starting episode 53\n",
      "\n",
      "Starting episode 54\n",
      "\n",
      "Starting episode 55\n",
      "\n",
      "Starting episode 56\n",
      "\n",
      "Starting episode 57\n",
      "\n",
      "Starting episode 58\n",
      "\n",
      "Starting episode 59\n",
      "\n",
      "Starting episode 60\n",
      "\n",
      "Starting episode 61\n",
      "\n",
      "Starting episode 62\n",
      "\n",
      "Starting episode 63\n",
      "\n",
      "Starting episode 64\n",
      "\n",
      "Starting episode 65\n",
      "\n",
      "Starting episode 66\n",
      "\n",
      "Starting episode 67\n",
      "\n",
      "Starting episode 68\n",
      "\n",
      "Starting episode 69\n",
      "\n",
      "Starting episode 70\n",
      "\n",
      "Starting episode 71\n",
      "\n",
      "Starting episode 72\n",
      "\n",
      "Starting episode 73\n",
      "\n",
      "Starting episode 74\n",
      "\n",
      "Starting episode 75\n",
      "\n",
      "Starting episode 76\n",
      "\n",
      "Starting episode 77\n",
      "\n",
      "Starting episode 78\n",
      "\n",
      "Starting episode 79\n",
      "\n",
      "Starting episode 80\n",
      "\n",
      "Starting episode 81\n",
      "\n",
      "Starting episode 82\n",
      "\n",
      "Starting episode 83\n",
      "\n",
      "Starting episode 84\n",
      "\n",
      "Starting episode 85\n",
      "\n",
      "Starting episode 86\n",
      "\n",
      "Starting episode 87\n",
      "\n",
      "Starting episode 88\n",
      "\n",
      "Starting episode 89\n",
      "\n",
      "Starting episode 90\n",
      "\n",
      "Starting episode 91\n",
      "\n",
      "Starting episode 92\n",
      "\n",
      "Starting episode 93\n",
      "\n",
      "Starting episode 94\n",
      "\n",
      "Starting episode 95\n",
      "\n",
      "Starting episode 96\n",
      "\n",
      "Starting episode 97\n",
      "\n",
      "Starting episode 98\n",
      "\n",
      "Starting episode 99\n",
      "\n",
      "Starting episode 100\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "\n",
    "for episode in range(episodes): # returns an iterator that yields the current agent of the environment\n",
    "    print(f\"\\nStarting episode {episode + 1}\")\n",
    "    env.reset() # reset the environment \n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last() # returns observation, reward etc for the agent able to act\n",
    "\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = env.action_space(agent).sample() # policy action or random sampling\n",
    "        #print(f\"Agent {agent} takes action: {action}\")\n",
    "        \n",
    "        env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results \n",
    "\n",
    ">This will be shown in the `\\plots` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  |\n",
    "|---------|---------|\n",
    "| **Action shifts of human and AV agents** ![](plots_saved/actions_shifts.png) | **Action shifts of all vehicles in the network** ![](plots_saved/actions.png) |\n",
    "| ![](plots_saved/rewards.png) | ![](plots_saved/travel_times.png) |\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"plots_saved/tt_dist.png\" width=\"700\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interrupt the connection with `SUMO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
