{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, we implement the multi-agent versions of the **[SAC](https://arxiv.org/pdf/1801.01290)** algorithm, **Independent SAC (ISAC)** and **Multi-Agent SAC (MASAC)** in our environment. SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy RL framework. \n",
    "\n",
    "\n",
    "> Tutorial based on [SAC TorchRL Tutorial](https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/sac.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We simulate our environment with an initial population of **200 human agents**. These agents navigate the environment and eventually converge on the fastest path. After this convergence, we will transition **50 of these human agents** into **machine agents**, specifically autonomous vehicles (AVs), which will then employ the QMIX reinforcement learning algorithms to further refine their learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run startup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torch.distributions import Categorical, OneHotCategorical\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.modules.models.multiagent import MultiAgentMLP\n",
    "from torchrl.objectives import DiscreteSACLoss, SACLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "from RouteRL import TrafficEnvironment\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"device is: \", device)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 100  # Number of team frames collected per training iteration\n",
    "n_iters = 50  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 1  # Number of optimization steps per training iteration\n",
    "minibatch_size = 16  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-2  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "memory_size =  1000  # Size of the replay buffer\n",
    "tau =  0.005\n",
    "gamma = 0.99  # discount factor\n",
    "\n",
    "policy_net_depth=3\n",
    "policy_net_num_cells=64\n",
    "\n",
    "critic_net_depth=2\n",
    "critic_net_num_cells=64\n",
    "\n",
    "human_learning_episodes = 100\n",
    "\n",
    "\n",
    "# Environment\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"num_agents\" : 200,\n",
    "        \"new_machines_after_mutation\": 50,\n",
    "        \"human_parameters\" : {\n",
    "            \"model\" : \"w_avg\"\n",
    "        },\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"csomor\"\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes],\n",
    "        \"smooth_by\" : 50,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n",
      "TrafficEnvironment with 200 agents.            \n",
      "0 machines and 200 humans.            \n",
      "Machines: []            \n",
      "Humans: [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 24, Human 25, Human 26, Human 27, Human 28, Human 29, Human 30, Human 31, Human 32, Human 33, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 59, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 66, Human 67, Human 68, Human 69, Human 70, Human 71, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 85, Human 86, Human 87, Human 88, Human 89, Human 90, Human 91, Human 92, Human 93, Human 94, Human 95, Human 96, Human 97, Human 98, Human 99, Human 100, Human 101, Human 102, Human 103, Human 104, Human 105, Human 106, Human 107, Human 108, Human 109, Human 110, Human 111, Human 112, Human 113, Human 114, Human 115, Human 116, Human 117, Human 118, Human 119, Human 120, Human 121, Human 122, Human 123, Human 124, Human 125, Human 126, Human 127, Human 128, Human 129, Human 130, Human 131, Human 132, Human 133, Human 134, Human 135, Human 136, Human 137, Human 138, Human 139, Human 140, Human 141, Human 142, Human 143, Human 144, Human 145, Human 146, Human 147, Human 148, Human 149, Human 150, Human 151, Human 152, Human 153, Human 154, Human 155, Human 156, Human 157, Human 158, Human 159, Human 160, Human 161, Human 162, Human 163, Human 164, Human 165, Human 166, Human 167, Human 168, Human 169, Human 170, Human 171, Human 172, Human 173, Human 174, Human 175, Human 176, Human 177, Human 178, Human 179, Human 180, Human 181, Human 182, Human 183, Human 184, Human 185, Human 186, Human 187, Human 188, Human 189, Human 190, Human 191, Human 192, Human 193, Human 194, Human 195, Human 196, Human 197, Human 198, Human 199]\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(seed=42, **env_params)\n",
    "\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  200 \n",
      "\n",
      "Agents are:  [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 24, Human 25, Human 26, Human 27, Human 28, Human 29, Human 30, Human 31, Human 32, Human 33, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 59, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 66, Human 67, Human 68, Human 69, Human 70, Human 71, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 85, Human 86, Human 87, Human 88, Human 89, Human 90, Human 91, Human 92, Human 93, Human 94, Human 95, Human 96, Human 97, Human 98, Human 99, Human 100, Human 101, Human 102, Human 103, Human 104, Human 105, Human 106, Human 107, Human 108, Human 109, Human 110, Human 111, Human 112, Human 113, Human 114, Human 115, Human 116, Human 117, Human 118, Human 119, Human 120, Human 121, Human 122, Human 123, Human 124, Human 125, Human 126, Human 127, Human 128, Human 129, Human 130, Human 131, Human 132, Human 133, Human 134, Human 135, Human 136, Human 137, Human 138, Human 139, Human 140, Human 141, Human 142, Human 143, Human 144, Human 145, Human 146, Human 147, Human 148, Human 149, Human 150, Human 151, Human 152, Human 153, Human 154, Human 155, Human 156, Human 157, Human 158, Human 159, Human 160, Human 161, Human 162, Human 163, Human 164, Human 165, Human 166, Human 167, Human 168, Human 169, Human 170, Human 171, Human 172, Human 173, Human 174, Human 175, Human 176, Human 177, Human 178, Human 179, Human 180, Human 181, Human 182, Human 183, Human 184, Human 185, Human 186, Human 187, Human 188, Human 189, Human 190, Human 191, Human 192, Human 193, Human 194, Human 195, Human 196, Human 197, Human 198, Human 199] \n",
      "\n",
      "Number of human agents is:  200 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(human_learning_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mutation**: a portion of human agents are converted into machine agents (autonomous vehicles). You can adjust the number of agents to be mutated in the <code style=\"color:white\">/params.json</code> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  200 \n",
      "\n",
      "Agents are:  [Machine 182, Machine 125, Machine 114, Machine 4, Machine 24, Machine 77, Machine 58, Machine 107, Machine 185, Machine 65, Machine 83, Machine 178, Machine 157, Machine 100, Machine 131, Machine 152, Machine 146, Machine 94, Machine 118, Machine 155, Machine 90, Machine 98, Machine 130, Machine 73, Machine 85, Machine 68, Machine 62, Machine 32, Machine 52, Machine 93, Machine 33, Machine 165, Machine 51, Machine 55, Machine 64, Machine 151, Machine 86, Machine 187, Machine 168, Machine 191, Machine 89, Machine 27, Machine 57, Machine 99, Machine 72, Machine 122, Machine 50, Machine 105, Machine 3, Machine 181, Human 0, Human 1, Human 2, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 25, Human 26, Human 28, Human 29, Human 30, Human 31, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 53, Human 54, Human 56, Human 59, Human 60, Human 61, Human 63, Human 66, Human 67, Human 69, Human 70, Human 71, Human 74, Human 75, Human 76, Human 78, Human 79, Human 80, Human 81, Human 82, Human 84, Human 87, Human 88, Human 91, Human 92, Human 95, Human 96, Human 97, Human 101, Human 102, Human 103, Human 104, Human 106, Human 108, Human 109, Human 110, Human 111, Human 112, Human 113, Human 115, Human 116, Human 117, Human 119, Human 120, Human 121, Human 123, Human 124, Human 126, Human 127, Human 128, Human 129, Human 132, Human 133, Human 134, Human 135, Human 136, Human 137, Human 138, Human 139, Human 140, Human 141, Human 142, Human 143, Human 144, Human 145, Human 147, Human 148, Human 149, Human 150, Human 153, Human 154, Human 156, Human 158, Human 159, Human 160, Human 161, Human 162, Human 163, Human 164, Human 166, Human 167, Human 169, Human 170, Human 171, Human 172, Human 173, Human 174, Human 175, Human 176, Human 177, Human 179, Human 180, Human 183, Human 184, Human 186, Human 188, Human 189, Human 190, Human 192, Human 193, Human 194, Human 195, Human 196, Human 197, Human 198, Human 199] \n",
      "\n",
      "Number of human agents is:  150 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  50 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a group that contains all the machine (RL) agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_list = []\n",
    "for machines in env.machine_agents:\n",
    "    machine_list.append(str(machines.id))\n",
    "      \n",
    "group = {'agents': machine_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True, # Whether to use the mask in the outputs. It is important for AEC environments to mask out non-acting agents.\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False, # Whether the environment’s done keys are set by aggregating the agent keys using any() (when True) or all() (when False).\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Agent group mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.group is:  {'agents': ['182', '125', '114', '4', '24', '77', '58', '107', '185', '65', '83', '178', '157', '100', '131', '152', '146', '94', '118', '155', '90', '98', '130', '73', '85', '68', '62', '32', '52', '93', '33', '165', '51', '55', '64', '151', '86', '187', '168', '191', '89', '27', '57', '99', '72', '122', '50', '105', '3', '181']} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"env.group is: \", env.group_map, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can append any TorchRL transform we need to our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code style=\"color:white\">check_env_specs()</code> function runs a small rollout and compared it output against the environment specs. It will raise an error if the specs aren't properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instantiate an `MPL` that can be used in multi-agent contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = False \n",
    "\n",
    "actor_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs = env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs = env.action_spec.space.n,\n",
    "        n_agents = env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=policy_net_depth,\n",
    "        num_cells=policy_net_num_cells,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"logits\")],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[(\"agents\", \"logits\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=Categorical,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The critic reads the observations and returns the corresponding value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralised_critic = True\n",
    "shared_parameters = True\n",
    "\n",
    "module = MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=env.action_spec.space.n,\n",
    "            n_agents=env.n_agents,\n",
    "            centralised=centralised_critic,\n",
    "            share_params=shared_parameters,\n",
    "            device=device,\n",
    "            depth=critic_net_depth,\n",
    "            num_cells=critic_net_num_cells,\n",
    "            activation_class=nn.Tanh,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_module = ValueOperator(\n",
    "            module=module,\n",
    "            in_keys=[(\"agents\", \"observation\")],\n",
    "            out_keys=[(\"agents\", \"action_value\")],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyTensorStorage(memory_size, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAC loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = DiscreteSACLoss(\n",
    "            actor_network=policy,\n",
    "            qvalue_network=value_module,\n",
    "            delay_qvalue=True,\n",
    "            num_actions=env.action_spec.space.n,\n",
    "            action_space=env.action_spec,\n",
    "        )\n",
    "\n",
    "loss_module.set_keys(\n",
    "    action_value=(\"agents\", \"action_value\"),\n",
    "    action=env.action_key,\n",
    "    reward=env.reward_key,\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "target_net_updater = SoftUpdate(loss_module, eps=1 - tau)\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [10:06<00:00, 12.13s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, tensordict_data in tqdm(enumerate(collector), total=n_iters, desc=\"Training\"):\n",
    "\n",
    "\n",
    "    current_frames = tensordict_data.numel()\n",
    "    total_frames += current_frames\n",
    "    data_view = tensordict_data.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    training_tds = []\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "            training_tds.append(loss_vals.detach())\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_actor\"]\n",
    "                + loss_vals[\"loss_alpha\"]\n",
    "                + loss_vals[\"loss_qvalue\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )\n",
    "            training_tds[-1].set(\"grad_norm\", total_norm.mean())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            target_net_updater.step()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    training_tds = torch.stack(training_tds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Check `\\plots` directory to find the plots created from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
