{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, we implement a state-of-the-art Multi Agent Reinforcement Leaning (MARL) algorithms **[SAC](https://arxiv.org/pdf/1801.01290)** in our environment. SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy RL framework. \n",
    "\n",
    "\n",
    "> Tutorial based on [SAC TorchRL Tutorial](https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/sac.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We simulate our environment with an initial population of **20 human agents**. These agents navigate the environment and eventually converge on the fastest path. After this convergence, we will transition **10 of these human agents** into **machine agents**, specifically autonomous vehicles (AVs), which will then employ the QMIX reinforcement learning algorithms to further refine their learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torch.distributions import Categorical, OneHotCategorical\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.modules.models.multiagent import MultiAgentMLP\n",
    "from torchrl.objectives import DiscreteSACLoss, SACLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "from RouteRL.keychain import Keychain as kc\n",
    "from RouteRL.environment.environment import TrafficEnvironment\n",
    "from RouteRL.services.plotter import Plotter\n",
    "from RouteRL.utilities import get_params\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(\"params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"device is: \", device)\n",
    "vmas_device = device  # The device where the simulator is run\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 150  # Number of team frames collected per training iteration\n",
    "n_iters = 100  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 1  # Number of optimization steps per training iteration\n",
    "minibatch_size = 16  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-2  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "memory_size =  1000  # Size of the replay buffer\n",
    "tau =  0.005\n",
    "gamma = 0.99  # discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: /opt/homebrew/opt/sumo/share/sumo/tools\n",
      "TrafficEnvironment with 100 agents.            \n",
      "0 machines and 100 humans.            \n",
      "Machines: []            \n",
      "Humans: [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 24, Human 25, Human 26, Human 27, Human 28, Human 29, Human 30, Human 31, Human 32, Human 33, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 59, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 66, Human 67, Human 68, Human 69, Human 70, Human 71, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 85, Human 86, Human 87, Human 88, Human 89, Human 90, Human 91, Human 92, Human 93, Human 94, Human 95, Human 96, Human 97, Human 98, Human 99]\n"
     ]
    }
   ],
   "source": [
    "params= {\n",
    "    \"agent_generation_parameters\" : {\n",
    "        \"num_agents\" : 100,\n",
    "        \"ratio_mutating\" : 0.3\n",
    "    }\n",
    "}\n",
    "env = TrafficEnvironment(params, generate_agent_data=True, generate_paths=True)\n",
    "\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  100 \n",
      "\n",
      "Agents are:  [Human 0, Human 1, Human 2, Human 3, Human 4, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 13, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 20, Human 21, Human 22, Human 23, Human 24, Human 25, Human 26, Human 27, Human 28, Human 29, Human 30, Human 31, Human 32, Human 33, Human 34, Human 35, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 43, Human 44, Human 45, Human 46, Human 47, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 59, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 66, Human 67, Human 68, Human 69, Human 70, Human 71, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 85, Human 86, Human 87, Human 88, Human 89, Human 90, Human 91, Human 92, Human 93, Human 94, Human 95, Human 96, Human 97, Human 98, Human 99] \n",
      "\n",
      "Number of human agents is:  100 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mutation**: a portion of human agents are converted into machine agents (autonomous vehicles). You can adjust the number of agents to be mutated in the <code style=\"color:white\">/params.json</code> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  100 \n",
      "\n",
      "Agents are:  [Machine 97, Machine 99, Machine 69, Machine 20, Machine 13, Machine 90, Machine 43, Machine 26, Machine 45, Machine 92, Machine 4, Machine 30, Machine 28, Machine 66, Machine 71, Machine 96, Machine 21, Machine 59, Machine 47, Machine 35, Machine 95, Machine 85, Machine 34, Machine 93, Machine 44, Human 0, Human 1, Human 2, Human 3, Human 5, Human 6, Human 7, Human 8, Human 9, Human 10, Human 11, Human 12, Human 14, Human 15, Human 16, Human 17, Human 18, Human 19, Human 22, Human 23, Human 24, Human 25, Human 27, Human 29, Human 31, Human 32, Human 33, Human 36, Human 37, Human 38, Human 39, Human 40, Human 41, Human 42, Human 46, Human 48, Human 49, Human 50, Human 51, Human 52, Human 53, Human 54, Human 55, Human 56, Human 57, Human 58, Human 60, Human 61, Human 62, Human 63, Human 64, Human 65, Human 67, Human 68, Human 70, Human 72, Human 73, Human 74, Human 75, Human 76, Human 77, Human 78, Human 79, Human 80, Human 81, Human 82, Human 83, Human 84, Human 86, Human 87, Human 88, Human 89, Human 91, Human 94, Human 98] \n",
      "\n",
      "Number of human agents is:  75 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Agents are: \", env.all_agents, \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a group that contains all the machine (RL) agents.\n",
    "\n",
    ">  **Hint:** As a feature of TorchRL multiagent, we are able to control the grouping of agents. We can group agents together (stacking their tensors) to leverage vectorization when passing them through the same neural network. We can split agents in different groups where they are heterogenous or should be processed by different neural netowkrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_list = []\n",
    "for machines in env.machine_agents:\n",
    "    machine_list.append(str(machines.id))\n",
    "      \n",
    "group = {'agents': machine_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True, # Whether to use the mask in the outputs. It is important for AEC environments to mask out non-acting agents.\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False, # Whether the environmentâ€™s done keys are set by aggregating the agent keys using any() (when True) or all() (when False).\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The environment is defined by a series of metadata that describe what can be expected during its execution. \n",
    "\n",
    "There are four specs to look at:\n",
    "\n",
    "- <code style=\"color:white\">action_spec</code> defines the action space;\n",
    "\n",
    "- <code style=\"color:white\">reward_spec</code> defines the reward domain;\n",
    "\n",
    "- <code style=\"color:white\">done_spec</code> defines the done domain;\n",
    "\n",
    "- <code style=\"color:white\">observation_spec</code> which defines the domain of all other outputs from environment steps;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    agents: Composite(\n",
      "        action: Categorical(\n",
      "            shape=torch.Size([25]),\n",
      "            space=CategoricalBox(n=3),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([25])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([])) \n",
      "\n",
      "\n",
      "reward_spec: Composite(\n",
      "    agents: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([25, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([25, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([25, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([25])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([])) \n",
      "\n",
      "\n",
      "done_spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    truncated: Categorical(\n",
      "        shape=torch.Size([1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    agents: Composite(\n",
      "        done: Categorical(\n",
      "            shape=torch.Size([25, 1]),\n",
      "            space=CategoricalBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        terminated: Categorical(\n",
      "            shape=torch.Size([25, 1]),\n",
      "            space=CategoricalBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        truncated: Categorical(\n",
      "            shape=torch.Size([25, 1]),\n",
      "            space=CategoricalBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([25])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([])) \n",
      "\n",
      "\n",
      "observation_spec: Composite(\n",
      "    agents: Composite(\n",
      "        observation: BoundedContinuous(\n",
      "            shape=torch.Size([25, 3]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([25, 3]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([25, 3]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        mask: Categorical(\n",
      "            shape=torch.Size([25]),\n",
      "            space=CategoricalBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([25])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([])) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", env.full_action_spec, \"\\n\\n\")\n",
    "print(\"reward_spec:\", env.full_reward_spec, \"\\n\\n\")\n",
    "print(\"done_spec:\", env.full_done_spec, \"\\n\\n\")\n",
    "print(\"observation_spec:\", env.observation_spec, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Agent group mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.group is:  {'agents': ['97', '99', '69', '20', '13', '90', '43', '26', '45', '92', '4', '30', '28', '66', '71', '96', '21', '59', '47', '35', '95', '85', '34', '93', '44']} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"env.group is: \", env.group_map, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can append any TorchRL transform we need to our environment. These will modify its input/output in some desired way. In multi-agent contexts, it is paramount to provide explicitly the keys to modify.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instatiate a <code style=\"color:white\">RewardSum</code> transformer that will sum rewards over episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code style=\"color:white\">check_env_specs()</code> function runs a small rollout and compared it output against the environment specs. It will raise an error if the specs aren't properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instantiate an `MPL` that can be used in multi-agent contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = False \n",
    "\n",
    "actor_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs = env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs = env.action_spec.space.n,\n",
    "        n_agents = env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=3,\n",
    "        num_cells=64,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"logits\")],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[(\"agents\", \"logits\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=Categorical,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The critic reads the observations and returns the corresponding value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralised_critic = True\n",
    "shared_parameters = True\n",
    "\n",
    "module = MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=env.action_spec.space.n,\n",
    "            n_agents=env.n_agents,\n",
    "            centralised=centralised_critic,\n",
    "            share_params=shared_parameters,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=nn.Tanh,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_module = ValueOperator(\n",
    "            module=module,\n",
    "            in_keys=[(\"agents\", \"observation\")],\n",
    "            out_keys=[(\"agents\", \"action_value\")],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collectors perform the following operations:\n",
    "\n",
    "1. **Reset Environment**: Initialize the environment.\n",
    "2. **Compute Action**: Determine the next action using the policy and the latest observation.\n",
    "3. **Execute Step**: Step through the environment with the computed action.\n",
    "\n",
    "These operations repeat until the environment signals to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyTensorStorage(memory_size, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAC loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = DiscreteSACLoss(\n",
    "            actor_network=policy,\n",
    "            qvalue_network=value_module,\n",
    "            delay_qvalue=True,\n",
    "            num_actions=env.action_spec.space.n,\n",
    "            action_space=env.action_spec,\n",
    "        )\n",
    "\n",
    "loss_module.set_keys(\n",
    "    action_value=(\"agents\", \"action_value\"),\n",
    "    action=env.action_key,\n",
    "    reward=env.reward_key,\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "target_net_updater = SoftUpdate(loss_module, eps=1 - tau)\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 23:08:46,529 [torchrl][INFO] \n",
      "Iteration 0\n",
      "2025-01-30 23:08:49,354 [torchrl][INFO] \n",
      "Iteration 1\n",
      "2025-01-30 23:08:52,186 [torchrl][INFO] \n",
      "Iteration 2\n",
      "2025-01-30 23:08:55,096 [torchrl][INFO] \n",
      "Iteration 3\n",
      "2025-01-30 23:08:57,946 [torchrl][INFO] \n",
      "Iteration 4\n",
      "2025-01-30 23:09:00,753 [torchrl][INFO] \n",
      "Iteration 5\n",
      "2025-01-30 23:09:03,579 [torchrl][INFO] \n",
      "Iteration 6\n",
      "2025-01-30 23:09:06,397 [torchrl][INFO] \n",
      "Iteration 7\n",
      "2025-01-30 23:09:09,206 [torchrl][INFO] \n",
      "Iteration 8\n",
      "2025-01-30 23:09:12,052 [torchrl][INFO] \n",
      "Iteration 9\n",
      "2025-01-30 23:09:14,910 [torchrl][INFO] \n",
      "Iteration 10\n",
      "2025-01-30 23:09:17,817 [torchrl][INFO] \n",
      "Iteration 11\n",
      "2025-01-30 23:09:20,656 [torchrl][INFO] \n",
      "Iteration 12\n",
      "2025-01-30 23:09:23,486 [torchrl][INFO] \n",
      "Iteration 13\n",
      "2025-01-30 23:09:26,340 [torchrl][INFO] \n",
      "Iteration 14\n",
      "2025-01-30 23:09:29,174 [torchrl][INFO] \n",
      "Iteration 15\n",
      "2025-01-30 23:09:32,001 [torchrl][INFO] \n",
      "Iteration 16\n",
      "2025-01-30 23:09:34,955 [torchrl][INFO] \n",
      "Iteration 17\n",
      "2025-01-30 23:09:37,812 [torchrl][INFO] \n",
      "Iteration 18\n",
      "2025-01-30 23:09:40,712 [torchrl][INFO] \n",
      "Iteration 19\n",
      "2025-01-30 23:09:43,541 [torchrl][INFO] \n",
      "Iteration 20\n",
      "2025-01-30 23:09:46,375 [torchrl][INFO] \n",
      "Iteration 21\n",
      "2025-01-30 23:09:49,215 [torchrl][INFO] \n",
      "Iteration 22\n",
      "2025-01-30 23:09:52,048 [torchrl][INFO] \n",
      "Iteration 23\n",
      "2025-01-30 23:09:54,870 [torchrl][INFO] \n",
      "Iteration 24\n",
      "2025-01-30 23:09:57,749 [torchrl][INFO] \n",
      "Iteration 25\n",
      "2025-01-30 23:10:00,607 [torchrl][INFO] \n",
      "Iteration 26\n",
      "2025-01-30 23:10:03,491 [torchrl][INFO] \n",
      "Iteration 27\n",
      "2025-01-30 23:10:06,340 [torchrl][INFO] \n",
      "Iteration 28\n",
      "2025-01-30 23:10:09,191 [torchrl][INFO] \n",
      "Iteration 29\n",
      "2025-01-30 23:10:12,047 [torchrl][INFO] \n",
      "Iteration 30\n",
      "2025-01-30 23:10:14,887 [torchrl][INFO] \n",
      "Iteration 31\n",
      "2025-01-30 23:10:17,729 [torchrl][INFO] \n",
      "Iteration 32\n",
      "2025-01-30 23:10:20,604 [torchrl][INFO] \n",
      "Iteration 33\n",
      "2025-01-30 23:10:23,455 [torchrl][INFO] \n",
      "Iteration 34\n",
      "2025-01-30 23:10:26,350 [torchrl][INFO] \n",
      "Iteration 35\n",
      "2025-01-30 23:10:29,234 [torchrl][INFO] \n",
      "Iteration 36\n",
      "2025-01-30 23:10:32,748 [torchrl][INFO] \n",
      "Iteration 37\n",
      "2025-01-30 23:10:35,960 [torchrl][INFO] \n",
      "Iteration 38\n",
      "2025-01-30 23:10:38,954 [torchrl][INFO] \n",
      "Iteration 39\n",
      "2025-01-30 23:10:41,924 [torchrl][INFO] \n",
      "Iteration 40\n",
      "2025-01-30 23:10:44,883 [torchrl][INFO] \n",
      "Iteration 41\n",
      "2025-01-30 23:10:47,901 [torchrl][INFO] \n",
      "Iteration 42\n",
      "2025-01-30 23:10:50,853 [torchrl][INFO] \n",
      "Iteration 43\n",
      "2025-01-30 23:10:53,862 [torchrl][INFO] \n",
      "Iteration 44\n",
      "2025-01-30 23:10:57,106 [torchrl][INFO] \n",
      "Iteration 45\n",
      "2025-01-30 23:11:00,137 [torchrl][INFO] \n",
      "Iteration 46\n",
      "2025-01-30 23:11:03,126 [torchrl][INFO] \n",
      "Iteration 47\n",
      "2025-01-30 23:11:06,011 [torchrl][INFO] \n",
      "Iteration 48\n",
      "2025-01-30 23:11:08,896 [torchrl][INFO] \n",
      "Iteration 49\n",
      "2025-01-30 23:11:11,872 [torchrl][INFO] \n",
      "Iteration 50\n",
      "2025-01-30 23:11:14,888 [torchrl][INFO] \n",
      "Iteration 51\n",
      "2025-01-30 23:11:17,925 [torchrl][INFO] \n",
      "Iteration 52\n",
      "2025-01-30 23:11:20,827 [torchrl][INFO] \n",
      "Iteration 53\n",
      "2025-01-30 23:11:23,720 [torchrl][INFO] \n",
      "Iteration 54\n",
      "2025-01-30 23:11:26,640 [torchrl][INFO] \n",
      "Iteration 55\n",
      "2025-01-30 23:11:29,538 [torchrl][INFO] \n",
      "Iteration 56\n",
      "2025-01-30 23:11:32,472 [torchrl][INFO] \n",
      "Iteration 57\n",
      "2025-01-30 23:11:35,387 [torchrl][INFO] \n",
      "Iteration 58\n",
      "2025-01-30 23:11:38,251 [torchrl][INFO] \n",
      "Iteration 59\n",
      "2025-01-30 23:11:41,197 [torchrl][INFO] \n",
      "Iteration 60\n",
      "2025-01-30 23:11:44,372 [torchrl][INFO] \n",
      "Iteration 61\n",
      "2025-01-30 23:11:47,680 [torchrl][INFO] \n",
      "Iteration 62\n",
      "2025-01-30 23:11:50,598 [torchrl][INFO] \n",
      "Iteration 63\n",
      "2025-01-30 23:11:53,500 [torchrl][INFO] \n",
      "Iteration 64\n",
      "2025-01-30 23:11:56,450 [torchrl][INFO] \n",
      "Iteration 65\n",
      "2025-01-30 23:11:59,347 [torchrl][INFO] \n",
      "Iteration 66\n",
      "2025-01-30 23:12:02,388 [torchrl][INFO] \n",
      "Iteration 67\n",
      "2025-01-30 23:12:05,492 [torchrl][INFO] \n",
      "Iteration 68\n",
      "2025-01-30 23:12:08,428 [torchrl][INFO] \n",
      "Iteration 69\n",
      "2025-01-30 23:12:11,413 [torchrl][INFO] \n",
      "Iteration 70\n",
      "2025-01-30 23:12:14,346 [torchrl][INFO] \n",
      "Iteration 71\n",
      "2025-01-30 23:12:17,279 [torchrl][INFO] \n",
      "Iteration 72\n",
      "2025-01-30 23:12:20,201 [torchrl][INFO] \n",
      "Iteration 73\n",
      "2025-01-30 23:12:23,084 [torchrl][INFO] \n",
      "Iteration 74\n",
      "2025-01-30 23:12:26,133 [torchrl][INFO] \n",
      "Iteration 75\n",
      "2025-01-30 23:12:29,069 [torchrl][INFO] \n",
      "Iteration 76\n",
      "2025-01-30 23:12:32,004 [torchrl][INFO] \n",
      "Iteration 77\n",
      "2025-01-30 23:12:34,872 [torchrl][INFO] \n",
      "Iteration 78\n",
      "2025-01-30 23:12:37,826 [torchrl][INFO] \n",
      "Iteration 79\n",
      "2025-01-30 23:12:40,717 [torchrl][INFO] \n",
      "Iteration 80\n",
      "2025-01-30 23:12:43,668 [torchrl][INFO] \n",
      "Iteration 81\n",
      "2025-01-30 23:12:46,572 [torchrl][INFO] \n",
      "Iteration 82\n",
      "2025-01-30 23:12:49,442 [torchrl][INFO] \n",
      "Iteration 83\n",
      "2025-01-30 23:12:52,395 [torchrl][INFO] \n",
      "Iteration 84\n",
      "2025-01-30 23:12:55,283 [torchrl][INFO] \n",
      "Iteration 85\n",
      "2025-01-30 23:12:58,198 [torchrl][INFO] \n",
      "Iteration 86\n",
      "2025-01-30 23:13:01,064 [torchrl][INFO] \n",
      "Iteration 87\n",
      "2025-01-30 23:13:04,028 [torchrl][INFO] \n",
      "Iteration 88\n",
      "2025-01-30 23:13:06,971 [torchrl][INFO] \n",
      "Iteration 89\n",
      "2025-01-30 23:13:09,899 [torchrl][INFO] \n",
      "Iteration 90\n",
      "2025-01-30 23:13:12,778 [torchrl][INFO] \n",
      "Iteration 91\n",
      "2025-01-30 23:13:15,641 [torchrl][INFO] \n",
      "Iteration 92\n",
      "2025-01-30 23:13:18,538 [torchrl][INFO] \n",
      "Iteration 93\n",
      "2025-01-30 23:13:21,493 [torchrl][INFO] \n",
      "Iteration 94\n",
      "2025-01-30 23:13:24,371 [torchrl][INFO] \n",
      "Iteration 95\n",
      "2025-01-30 23:13:27,259 [torchrl][INFO] \n",
      "Iteration 96\n",
      "2025-01-30 23:13:30,234 [torchrl][INFO] \n",
      "Iteration 97\n",
      "2025-01-30 23:13:33,176 [torchrl][INFO] \n",
      "Iteration 98\n",
      "2025-01-30 23:13:36,147 [torchrl][INFO] \n",
      "Iteration 99\n"
     ]
    }
   ],
   "source": [
    "total_time = 0\n",
    "total_frames = 0\n",
    "sampling_start = time.time()\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    torchrl_logger.info(f\"\\nIteration {i}\")\n",
    "\n",
    "    sampling_time = time.time() - sampling_start\n",
    "\n",
    "    current_frames = tensordict_data.numel()\n",
    "    total_frames += current_frames\n",
    "    data_view = tensordict_data.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    training_tds = []\n",
    "    training_start = time.time()\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "            training_tds.append(loss_vals.detach())\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_actor\"]\n",
    "                + loss_vals[\"loss_alpha\"]\n",
    "                + loss_vals[\"loss_qvalue\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )\n",
    "            training_tds[-1].set(\"grad_norm\", total_norm.mean())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            target_net_updater.step()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "\n",
    "    iteration_time = sampling_time + training_time\n",
    "    total_time += iteration_time\n",
    "    training_tds = torch.stack(training_tds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Check `\\plots` directory to find the plots created from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RouteRL.services.plotter.Plotter at 0x13f7372c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RouteRL.services import plotter\n",
    "plotter(params[kc.PLOTTER])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
