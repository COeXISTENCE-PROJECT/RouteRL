{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating fleets of automated vehicles (AVs) making routing decisions: Medium traffic network, AV behaviors, QMIX algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, on the `Cologne` network, we simulate **100 human agents** for `950 days`. After 100 days **40 of the human agents** mutate into automated vehicles (AVs) and use the `QMIX` algorithm implemented from the `TorchRL` library to learn the optimal route. The AVs are `malicious` and their goal is to maximize human travel time. Since all AVs share the same reward signal, we model them using a  collaborative MARL algorithm. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The network used.\n",
    "> \n",
    "![Network used](plots_saved/cologne.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the **[paper](https://openreview.net/pdf?id=88zP8xh5D2)**, the reward function enforces a selected behavior on the agent. For an agent *k* with behavioral parameters **φₖ ∈ ℝ⁴**, the reward is defined as:\n",
    "\n",
    "$$\n",
    "r_k = \\varphi_{k1} \\cdot T_{\\text{own}, k} + \\varphi_{k2} \\cdot T_{\\text{group}, k} + \\varphi_{k3} \\cdot T_{\\text{other}, k} + \\varphi_{k4} \\cdot T_{\\text{all}, k}\n",
    "$$\n",
    "\n",
    "\n",
    "where **Tₖ** is a vector of travel time statistics provided to agent *k*, containing:\n",
    "\n",
    "- **Own Travel Time** ($T_{\\text{own}, k}$): The amount of time the agent has spent in traffic.\n",
    "- **Group Travel Time** ($T_{\\text{group}, k}$): The average travel time of agents in the same group (e.g., AVs for an AV agent).\n",
    "- **Other Group Travel Time** ($T_{\\text{other}, k}$): The average travel time of agents in other groups (e.g., humans for an AV agent).\n",
    "- **System-wide Travel Time** ($T_{\\text{all}, k}$): The average travel time of all agents in the traffic network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Behavioral Strategies & Objective Weightings\n",
    "\n",
    "| **Behavior**    | **ϕ₁** | **ϕ₂** | **ϕ₃** | **ϕ₄** | **Interpretation**                                    |\n",
    "|---------------|------|------|------|------|----------------------------------------------------|\n",
    "| **Altruistic**     | 0    | 0    | 0    | 1    | Minimize delay for everyone                       |\n",
    "| **Collaborative**  | 0.5  | 0.5  | 0    | 0    | Minimize delay for oneself and one’s own group    |\n",
    "| **Competitive**    | 2    | 0    | -1   | 0    | Minimize self-delay & maximize delay for others  |\n",
    "| **Malicious**      | 0    | 0    | -1   | 0    | Maximize delay for the other group               |\n",
    "| **Selfish**        | 1    | 0    | 0    | 0    | Minimize delay for oneself                        |\n",
    "| **Social**        | 0.5  | 0    | 0    | 0.5  | Minimize delay for oneself & everyone            |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QMIX algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[QMIX](https://arxiv.org/pdf/1803.11485)** is a deep MARL method that allows end-to-end learning of decentralized policies in a centralized setting amd makes efficient use of extra state information. \n",
    "\n",
    "\n",
    "> Tutorial based on [QMIX TorchRL Tutorial](https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/qmix_vdn.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level overview of QMIX algorithm\n",
    "\n",
    "Each agent has its own agent network that represents its individual value function Q<sub>a</sub>. \n",
    "\n",
    "The mixing network is a feed-forward neural network that has as input the agent network outputs and mixes them monotonically. It produces the values of Q<sub>tot</sub>.\n",
    "\n",
    "The weights of the mixing network are produced by separate hypernetworks. Each hypernetwork takes the state *s* as input and generated the weights of one layer of the mixing network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import EGreedyModule, QValueModule, SafeSequential\n",
    "from torchrl.modules.models.multiagent import MultiAgentMLP, QMixer\n",
    "from torchrl.objectives import SoftUpdate, ValueEstimators\n",
    "from torchrl.objectives.multiagent.qmixer import QMixerLoss\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "\n",
    "from routerl import TrafficEnvironment\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"device is: \", device)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 1000  # Number of team frames collected per training iteration\n",
    "n_iters = 40  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 1  # Number of optimization steps per training iteration\n",
    "minibatch_size = 2  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-2  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "memory_size = 1000  # Size of the replay buffer\n",
    "tau =  0.005\n",
    "gamma = 0.99  # discount factor\n",
    "\n",
    "mlp_depth=2\n",
    "mlp_num_cells=256\n",
    "\n",
    "eps_greedy_init=0.3\n",
    "eps_greedy_end=0\n",
    "\n",
    "mixing_embed_dim = 32\n",
    "\n",
    "human_learning_episodes = 100\n",
    "new_machines_after_mutation = 40\n",
    "\n",
    "# number of episodes the AV training will take\n",
    "training_episodes = (frames_per_batch / new_machines_after_mutation) * n_iters\n",
    "\n",
    "# Environment\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"new_machines_after_mutation\": new_machines_after_mutation,\n",
    "\n",
    "        \"human_parameters\" :\n",
    "        {\n",
    "            \"model\" : \"gawron\",\n",
    "\n",
    "            \"noise_weight_agent\" : 0,\n",
    "            \"noise_weight_path\" : 0.8,\n",
    "            \"noise_weight_day\" : 0.2,\n",
    "\n",
    "            \"beta\" : -1,\n",
    "            \"beta_k_i_variability\" : 0.1,\n",
    "            \"epsilon_i_variability\" : 0.1,\n",
    "            \"epsilon_k_i_variability\" : 0.1,\n",
    "            \"epsilon_k_i_t_variability\" : 0.1,\n",
    "\n",
    "            \"greedy\" : 0.1,\n",
    "            \"gamma_c\" : 0.0,\n",
    "            \"gamma_u\" : 0.0,\n",
    "            \"remember\" : 1,\n",
    "\n",
    "            \"alpha_zero\" : 0.8,\n",
    "            \"alphas\" : [0.2]  \n",
    "        },\n",
    "        \"machine_parameters\" :\n",
    "        {\n",
    "            \"behavior\" : \"selfish\",\n",
    "        }\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"ingolstadt\",\n",
    "        \"sumo_type\" : \"sumo\",\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes, int(training_episodes) + human_learning_episodes],\n",
    "        \"smooth_by\" : 50,\n",
    "        \"plot_choices\" : \"basic\",\n",
    "        \"phase_names\" : [\n",
    "            \"Human learning\", \n",
    "            \"Mutation - Machine learning\",\n",
    "            \"Testing phase\"\n",
    "        ],\n",
    "        \"records_folder\" : \"records\"\n",
    "    },\n",
    "    \"path_generation_parameters\":\n",
    "    {\n",
    "        \"origins\" : ['315358244', '10425609#0', '24608844', '315358250#0', '-306240162#1', '201201950#1', '201201953#4', '272042143', '266565295#5', '22716069#0', '24634507', '128361109#1', '315358242#0', '-447569998#1', '-54169231#2', '-10427692#1', '-32978638#0', '-18809673#6', '201950247#3', '-26677542#0', '26677213#0', '24599188#0.94', '176550249#3', '-24634413#2', '-160314345#5', '24634517#1', '201238726#0.117', '28319300#3', '-399835085#0', '-160314345#2', '168702040#1', '24634416', '-24634509#7', '-137454133#5', '-315358257#0.26', '-26677216#0', '286646456#1', '201201935#0', '-24634414#1', '23166741#2', '-24634510#5', '201238719#0', '24634513#0', '40888360#0', '24634510#1', '-224892361#8', '54169231#1', '315392062#0', '53396619#4', '-201950247#0', '-24634411', '176550249#4', '-24634505', '176550249#1', '28111977#8', '-201950263#6', '-24634413#0', '176550249#0', '129379925', '-28319300#2', '-22716549#6', '-201089423#1', '-25117391#2', '-201201945#0', '-136728347#4', '315358242#1', '22879845', '-25117391#1', '24634411', '-315358242#0', '-22724699#11', '26677214#0', '-26677213#5', '-315392062#4', '315358242#3', '23436553#4', '26676668', '-40888356#6', '-201238718#1', '24634517#6', '-24634506#0', '25117391#2', '-10427692#6', '-22690206#1', '-25145014#4', '-24634510#6', '286646456#0', '10427692#7', '-25187895#0', '-26677539', '201950247#7', '-24634517#16', '201950263#0', '201201950#3', '128361109#3', '23525483#7', '-18809672#6', '-24634417', '201201945#3', '-22716073', '25145014#0', '-24634514#1', '26677542#1', '-24634510#15', '24634414#5', '25190140#1', '-32124743', '23166741#5', '25117391#0', '-224892339#3', '-233675413#0', '28319300#0', '-201950263#10', '-24634517#3', '-201201945#6', '-40888351#2', '173203413#0', '32395327#0', '-54169231#0', '31860333#0', '24634417', '218647954', '-24634513#5', '201238724', '-170018165#3', '-201950259#6', '26677214#5', '24608845', '24634509#1', '-201238718#0', '18809672#7', '25149001#4', '201089423#0', '160314345#2', '-24634508', '24634517#13', '201950259#0', '160314345#3', '-25117417#1', '-224892361#6', '26677540#0', '224251774#1', '24634511', '26677540#1', '-40888351#6', '24634413#0', '-201963533#5', '-36962701#1', '23525483#1', '-201950259#3', '-41203916#3', '201201945#1', '-201950247#5', '-40888443', '24634516#1', '-24634510#0', '18813598#8', '170018165#2.175', '24693977#1', '-26677417#0', '-22690206#2', '24634510#0', '-201963533#1', '201950247#8', '24608846#0', '26677216#1', '201201945#2', '315358253#2', '-26677541', '18813598#7', '170425366#0', '32124637#1', '-201238724', '22690205#3', '22724699#3', '201201953#2', '-173203413#7', '-224892361#1', '24634506#0', '-137454133#0', '-22724699#2', '32021112#0', '201238726#0', '-24634511', '-18813598#7', '-28111977#9', '26677214#3', '25145012#5', '-36962701#0', '168702039#1', '10427692#6', '-37681424#1', '224892361#3', '-26677216#4', '315358246', '-54169231#1', '-138300620#6', '40888354#0', '653473569#3', '-315358253#1', '201201950#4', '-24634507', '-24634510#18', '26677541', '26677542#0', '26677216#0', '24634506#2', '-201963533#1.145', '-137454133#1', '-26677542#1', '-32999434#1', '128906555', '-315358251#0', '-315358255#4', '-22690205#1', '-25145013#0'],\n",
    "        \"destinations\" : ['-25149001#4', '-128361102#1', '32978638#0', '386687235', '399835085#1', '-266565295#5', '24634414#1', '-24634415', '24634517#13', '201238729#3', '306240162#0', '-26677539', '447569997#1', '40888354#0', '315358242#0', '-24634506#2', '30482615#1', '-10427692#3', '24634510#6', '-24634505', '272042145', '-201238724', '-83304175#2', '26677541', '201238718#0', '-25145014#0', '24634509#1', '-315358242#1', '-315358242#0', '402600768#1', '28319300#3', '-201963533#4', '24608845', '-25190140#1', '137454133#2', '201201950#1', '40888354#1', '40888356#0', '138300620#0', '-201238718#0', '-22724699#1', '-32395288#1', '-218647954', '22724699#7', '26677214#3', '24634414#2', '-160314346#0', '201201953#10', '28111977#9', '-25145011#0', '-170018165#0', '23166741#5', '4942376', '24634506#2', '31860333#1', '176550249#4', '-315358258', '26642363', '224892361#3', '-160314345#0', '26677214#4', '-26677416#1', '28319300#0', '-25187893', '201238726#0', '24634411', '-32395361#1', '201950250#1', '-25145012#3', '-26677213#5', '22716549#0', '-315358252#1', '-24634413#2', '-393420106#1', '-24634521#1', '-201963533#1.145', '24634513#0', '-26677216#0', '-24693977#1', '-379510292', '-24634507', '-201963522#3', '-201950247#3', '-24634510#5', '-24634518#0', '176550249#3', '201950247#0', '24634413#1', '28111977#0', '24634511', '40888359', '170018165#2', '51857516#1', '32999110#0', '201201953#12', '-201201945#0', '-129379918#1', '-170018165#3', '24634510#1', '-26677542#1', '201950247#6', '-201238718#1', '53396619#3', '-201201945#0.78', '24634507', '-201950247#5', '-23436553#2', '-315358246', '-24634413#0', '-315392062#4', '653473569#1', '24634510#7', '201950263#0', '201950263#7', '25117417#1', '-22724699#2', '-22724699#11', '201963522#6', '-24634506#0', '-136436468#0', '-10427692#7', '-24634510#18', '-40888350', '201950259#0', '-201950259#6', '-24634513#5', '315392062#0', '201950250#0', '-22690205#3', '-26677540#1', '-138300620#6', '-201950263#6', '26677542#1', '-25187895#1', '-22690206#1', '160314346#0', '-53396619#2', '40888354#2', '-26677417#0', '-32999434#1', '-24634508', '-24634514#1', '24634517#1', '-201950247#7', '129379967#0', '315358257#0', '-315358248#1', '23525483#1', '-23525483#5', '286646456#1', '-22716549#6', '-224892361#6', '22724699#3', '136436468#1', '-129379922', '-24634414#5', '160314345#1', '204588664#0', '22724699#2', '315358245.27', '286646456#0', '160314345#2', '-173203413#7', '-315358257#2', '-286646456#0', '-26676668', '315358255#0', '23436553#2'],\n",
    "        \"number_of_paths\" : 4,\n",
    "        \"beta\" : -5,\n",
    "        \"num_samples\" : 10,\n",
    "        \"visualize_paths\" : True\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(seed=42, create_agents=False, create_paths=False, **env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Available paths create using the [Janux](https://github.com/COeXISTENCE-PROJECT/JanuX) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  |\n",
    "|---------|---------|\n",
    "|  ![](plots_saved/0_0.png) |  ![](plots_saved/0_1.png) |\n",
    "| ![](plots_saved/1_0.png) | ![](plots_saved/1_1.png) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  1035 \n",
      "\n",
      "Number of human agents is:  1035 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for human in env.human_agents:\n",
    "\n",
    "    inverse = 1 / np.array(human.initial_knowledge)\n",
    "    invserse_normalized = inverse / inverse.sum()\n",
    "\n",
    "    indices = np.arange(len(human.initial_knowledge))\n",
    "    human.default_action = np.random.choice(indices, size=1, p=invserse_normalized)[0]\n",
    "    \n",
    "    print(human.default_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mutation**: a portion of human agents are converted into machine agents (autonomous vehicles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  1035 \n",
      "\n",
      "Number of human agents is:  995 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  40 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `TorchRL` enables us to make different groups with different agents. Here, all the AV agents are included in one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_list = []\n",
    "for machines in env.machine_agents:\n",
    "    machine_list.append(str(machines.id))\n",
    "      \n",
    "group = {'agents': machine_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo environment wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In order to employ the `TorchRL` library in our environment we need to use their `PettingZooWrapper` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True, # Whether to use the mask in the outputs. It is important for AEC environments to mask out non-acting agents.\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False, # Whether the environment’s done keys are set by aggregating the agent keys using any() (when True) or all() (when False).\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Agent group mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.group is:  {'agents': ['913', '373', '284', '1021', '542', '511', '489', '403', '1020', '364', '959', '1028', '824', '349', '872', '699', '292', '290', '358', '491', '507', '790', '892', '287', '850', '471', '1015', '946', '1001', '835', '704', '496', '736', '886', '559', '265', '432', '1006', '712', '626']} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"env.group is: \", env.group_map, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code style=\"color:white\">check_env_specs()</code> function runs a small rollout and compared it output against the environment specs. It will raise an error if the specs aren't properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 14:31:01,134 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instantiate an `MPL` that can be used in multi-agent contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs=env.action_spec.space.n,\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=True,\n",
    "        device=device,\n",
    "        depth=mlp_depth,\n",
    "        num_cells=mlp_num_cells,\n",
    "        activation_class=nn.Tanh,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = TensorDictModule(\n",
    "        net, in_keys=[(\"agents\", \"observation\")], out_keys=[(\"agents\", \"action_value\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_module = QValueModule(\n",
    "    action_value_key=(\"agents\", \"action_value\"),\n",
    "    out_keys=[\n",
    "        env.action_key,\n",
    "        (\"agents\", \"action_value\"),\n",
    "        (\"agents\", \"chosen_action_value\"),\n",
    "    ],\n",
    "    spec=env.action_spec,\n",
    "    action_space=None,\n",
    ")\n",
    "\n",
    "qnet = SafeSequential(module, value_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_explore = TensorDictSequential(\n",
    "    qnet,\n",
    "    EGreedyModule(\n",
    "        eps_init=eps_greedy_init,\n",
    "        eps_end=eps_greedy_end,\n",
    "        annealing_num_steps=int(total_frames * (1 / 2)), # Number of steps it will take for epsilon to reach the eps_end value\n",
    "        action_key=env.action_key, # The key where the action can be found in the input tensordict.\n",
    "        spec=env.action_spec,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixer\n",
    "\n",
    "> `QMixer` mixes the local Q values of the agents into a global Q value through a monotonic hyper-network whose parameters are obtained from a global state, according to [Qmix paper](https://arxiv.org/pdf/1803.11485)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixer = TensorDictModule(\n",
    "    module=QMixer(\n",
    "        state_shape=env.observation_spec[\n",
    "            \"agents\", \"observation\"\n",
    "        ].shape,\n",
    "        mixing_embed_dim=mixing_embed_dim,\n",
    "        n_agents=env.n_agents,\n",
    "        device=device,\n",
    "    ),\n",
    "    in_keys=[(\"agents\", \"chosen_action_value\"), (\"agents\", \"observation\")],\n",
    "    out_keys=[\"chosen_action_value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "        env,\n",
    "        qnet_explore,\n",
    "        device=device,\n",
    "        storing_device=device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyTensorStorage(memory_size, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qmix loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = QMixerLoss(qnet, mixer, delay_value=True)\n",
    "\n",
    "loss_module.set_keys(\n",
    "    action_value=(\"agents\", \"action_value\"),\n",
    "    local_value=(\"agents\", \"chosen_action_value\"),\n",
    "    global_value=\"chosen_action_value\",\n",
    "    action=env.action_key,\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma) # The value estimator used for the loss computation\n",
    "target_net_updater = SoftUpdate(loss_module, eps=1 - tau) # Technique used to update the target network\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/40 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m## Generate the rollouts\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdel\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_key\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:953\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_frames:\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 953\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tensordict_out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total_frames:\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\_utils.py:470\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[1;32m--> 470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:1071\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1070\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[1;32m-> 1071\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\common.py:2576\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_and_maybe_reset\u001b[39m(\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase\n\u001b[0;32m   2536\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001b[39;00m\n\u001b[0;32m   2538\u001b[0m \n\u001b[0;32m   2539\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;124;03m            is_shared=False)\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2576\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m     tensordict_ \u001b[38;5;241m=\u001b[39m step_mdp(\n\u001b[0;32m   2580\u001b[0m         tensordict,\n\u001b[0;32m   2581\u001b[0m         keep_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2586\u001b[0m         done_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_keys,\n\u001b[0;32m   2587\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\common.py:1408\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[0;32m   1407\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1408\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1409\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\transforms\\transforms.py:738\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    736\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    737\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[1;32m--> 738\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    744\u001b[0m         next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    745\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:616\u001b[0m, in \u001b[0;36mPettingZooWrapper._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     (\n\u001b[0;32m    603\u001b[0m         observation_dict,\n\u001b[0;32m    604\u001b[0m         rewards_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         info_dict,\n\u001b[0;32m    608\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_parallel(tensordict)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m     (\n\u001b[0;32m    611\u001b[0m         observation_dict,\n\u001b[0;32m    612\u001b[0m         rewards_dict,\n\u001b[0;32m    613\u001b[0m         terminations_dict,\n\u001b[0;32m    614\u001b[0m         truncations_dict,\n\u001b[0;32m    615\u001b[0m         info_dict,\n\u001b[1;32m--> 616\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_aec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# We start with zeroed data and fill in the data for alive agents\u001b[39;00m\n\u001b[0;32m    620\u001b[0m tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_step_output_zero\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:766\u001b[0m, in \u001b[0;36mPettingZooWrapper._step_aec\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    763\u001b[0m         action \u001b[38;5;241m=\u001b[39m group_action_np[agent_index]\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m terminations_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mterminations\n\u001b[0;32m    768\u001b[0m truncations_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mtruncations\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\environment.py:474\u001b[0m, in \u001b[0;36mTrafficEnvironment.step\u001b[1;34m(self, machine_action)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# The cumulative reward of the last agent must be 0\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cumulative_rewards[agent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmachine_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect rewards if it is the last agent to act\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_selector\u001b[38;5;241m.\u001b[39mis_last():\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# Increase day number\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\environment.py:773\u001b[0m, in \u001b[0;36mTrafficEnvironment.simulation_loop\u001b[1;34m(self, machine_action, machine_id)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# If all machines that have start time as the simulator timestep acted\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine_same_start_time:\n\u001b[1;32m--> 773\u001b[0m     travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_help_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_dict \u001b[38;5;129;01min\u001b[39;00m travel_times:\n\u001b[0;32m    776\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtravel_times_list\u001b[38;5;241m.\u001b[39mappend(agent_dict)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\environment.py:634\u001b[0m, in \u001b[0;36mTrafficEnvironment._help_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39madd_vehicle(action_dict)\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_actions[agent\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m action_dict\n\u001b[1;32m--> 634\u001b[0m timestep, stopped_vehicles_info, arrivals, teleported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_detectors_info \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_detectors_info(stopped_vehicles_info)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\simulator.py:384\u001b[0m, in \u001b[0;36mSumoSimulator.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m teleported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m veh_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_vehicles\u001b[38;5;241m.\u001b[39mcopy():\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetSpeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mveh_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_vehicles[veh_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_vehicles[veh_id] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstuck_time:\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_vehicle.py:307\u001b[0m, in \u001b[0;36mVehicleDomain.getSpeed\u001b[1;34m(self, vehID)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetSpeed\u001b[39m(\u001b[38;5;28mself\u001b[39m, vehID):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"getSpeed(string) -> double\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    Returns the (longitudinal) speed in m/s of the named vehicle within the last step.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAR_SPEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehID\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:147\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:152\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m r\u001b[38;5;241m.\u001b[39mreadLength()\n\u001b[0;32m    154\u001b[0m response, retVarID \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39msend(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\n\u001b[1;32m--> 131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceiving\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mgetDebugString())\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[0m, in \u001b[0;36mConnection._recvExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 109\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, tensordict_data in tqdm(enumerate(collector), total=n_iters, desc=\"Training\"):\n",
    "\n",
    "    ## Generate the rollouts\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"reward\"), tensordict_data.get((\"next\", env.reward_key)).mean(-2)\n",
    "    )\n",
    "    del tensordict_data[\"next\", env.reward_key]\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"episode_reward\"),\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\")).mean(-2),\n",
    "    )\n",
    "    del tensordict_data[\"next\", \"agents\", \"episode_reward\"]\n",
    "\n",
    "\n",
    "    current_frames = tensordict_data.numel()\n",
    "    total_frames += current_frames\n",
    "    data_view = tensordict_data.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "    \n",
    "\n",
    "    training_tds = []\n",
    "\n",
    "    ## Update the policies of the learning agents\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "            training_tds.append(loss_vals.detach())\n",
    "\n",
    "            loss_value = loss_vals[\"loss\"]\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )\n",
    "            training_tds[-1].set(\"grad_norm\", total_norm.mean())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            target_net_updater.step()\n",
    "\n",
    "    qnet_explore[1].step(frames=current_frames)  # Update exploration annealing\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    training_tds = torch.stack(training_tds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_explore.eval() # set the policy into evaluation mode\n",
    "\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    env.rollout(len(env.machine_agents), policy=qnet_explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Check `\\plots` directory to find the plots created from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The plots reveal that the introduction of AVs into urban traffic influences human agents' decision-making. This insight highlights the need for research aimed at mitigating potential negative effects of AV introduction, such as increased human travel times, congestion, and subsequent rises in $CO_2$ emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  |\n",
    "|---------|---------|\n",
    "| **Action shifts of human and AV agents** ![](plots_saved/qmix_actions_shifts.png) | **Action shifts of all vehicles in the network** ![](plots_saved/qmix_actions.png) |\n",
    "| ![](plots_saved/qmix_rewards.png) | ![](plots_saved/qmix_travel_times.png) |\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"plots_saved/qmix_tt_dist.png\" width=\"700\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interrupt the connection with `SUMO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
