{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating fleets of automated vehicles (AVs) making routing decisions: Medium traffic network, AV behaviors, IPPO/MAPPO algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, on the `Cologne` network, we simulate **100 human agents** for `950 days`. After 100 days **40 of the human agents** mutate into automated vehicles (AVs) and use the [`MAPPO`]((https://arxiv.org/pdf/2103.01955)) (Multi-agent Proximal Policy Optimization) algorithm implemented from the `TorchRL` library to learn the optimal route. The AVs are `malicious` and their goal is to maximize human travel time. Since all AVs share the same reward signal, we model them using an algorithm that can be suitable for collaborative MARL tasks. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This tutorial is based on [Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial](https://pytorch.org/rl/stable/tutorials/multiagent_ppo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "\n",
    "# Now you can import the module\n",
    "from routerl import TrafficEnvironment\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"device is: \", device)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 40  # Number of team frames collected per training iteration\n",
    "n_iters = 10  # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 1  # Number of optimization steps per training iteration\n",
    "minibatch_size = 2  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4 # Learning rate\n",
    "max_grad_norm = 3.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "\n",
    "policy_network_depth=3\n",
    "policy_network_num_cells = 64\n",
    "\n",
    "critic_network_depth=3\n",
    "critic_network_num_cells = 64\n",
    "\n",
    "# Human learning phase\n",
    "human_learning_episodes = 200\n",
    "new_machines_after_mutation = 40\n",
    "\n",
    "# number of episodes the AV training will take\n",
    "training_episodes = (frames_per_batch / new_machines_after_mutation) * n_iters\n",
    "\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"new_machines_after_mutation\": new_machines_after_mutation,\n",
    "\n",
    "        \"human_parameters\" :\n",
    "        {\n",
    "            \"model\" : \"gawron\",\n",
    "\n",
    "            \"noise_weight_agent\" : 0,\n",
    "            \"noise_weight_path\" : 0.8,\n",
    "            \"noise_weight_day\" : 0.2,\n",
    "\n",
    "            \"beta\" : -1,\n",
    "            \"beta_k_i_variability\" : 0.1,\n",
    "            \"epsilon_i_variability\" : 0.1,\n",
    "            \"epsilon_k_i_variability\" : 0.1,\n",
    "            \"epsilon_k_i_t_variability\" : 0.1,\n",
    "\n",
    "            \"greedy\" : 0.1,\n",
    "            \"gamma_c\" : 0.0,\n",
    "            \"gamma_u\" : 0.0,\n",
    "            \"remember\" : 1,\n",
    "\n",
    "            \"alpha_zero\" : 0.8,\n",
    "            \"alphas\" : [0.2]  \n",
    "        },\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"ingolstadt\",\n",
    "        \"sumo_type\" : \"sumo\",\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes, int(training_episodes) + human_learning_episodes],\n",
    "        \"smooth_by\" : 50,\n",
    "        \"phase_names\" : [\n",
    "            \"Human learning\", \n",
    "            \"Mutation - Machine learning\",\n",
    "            \"Testing phase\"\n",
    "        ],\n",
    "        \"records_folder\" : \"records\"\n",
    "    },\n",
    "    \"path_generation_parameters\":\n",
    "    {\n",
    "        \"origins\" : ['315358244', '10425609#0', '24608844', '315358250#0', '-306240162#1', '201201950#1', '201201953#4', '272042143', '266565295#5', '22716069#0', '24634507', '128361109#1', '315358242#0', '-447569998#1', '-54169231#2', '-10427692#1', '-32978638#0', '-18809673#6', '201950247#3', '-26677542#0', '26677213#0', '24599188#0.94', '176550249#3', '-24634413#2', '-160314345#5', '24634517#1', '201238726#0.117', '28319300#3', '-399835085#0', '-160314345#2', '168702040#1', '24634416', '-24634509#7', '-137454133#5', '-315358257#0.26', '-26677216#0', '286646456#1', '201201935#0', '-24634414#1', '23166741#2', '-24634510#5', '201238719#0', '24634513#0', '40888360#0', '24634510#1', '-224892361#8', '54169231#1', '315392062#0', '53396619#4', '-201950247#0', '-24634411', '176550249#4', '-24634505', '176550249#1', '28111977#8', '-201950263#6', '-24634413#0', '176550249#0', '129379925', '-28319300#2', '-22716549#6', '-201089423#1', '-25117391#2', '-201201945#0', '-136728347#4', '315358242#1', '22879845', '-25117391#1', '24634411', '-315358242#0', '-22724699#11', '26677214#0', '-26677213#5', '-315392062#4', '315358242#3', '23436553#4', '26676668', '-40888356#6', '-201238718#1', '24634517#6', '-24634506#0', '25117391#2', '-10427692#6', '-22690206#1', '-25145014#4', '-24634510#6', '286646456#0', '10427692#7', '-25187895#0', '-26677539', '201950247#7', '-24634517#16', '201950263#0', '201201950#3', '128361109#3', '23525483#7', '-18809672#6', '-24634417', '201201945#3', '-22716073', '25145014#0', '-24634514#1', '26677542#1', '-24634510#15', '24634414#5', '25190140#1', '-32124743', '23166741#5', '25117391#0', '-224892339#3', '-233675413#0', '28319300#0', '-201950263#10', '-24634517#3', '-201201945#6', '-40888351#2', '173203413#0', '32395327#0', '-54169231#0', '31860333#0', '24634417', '218647954', '-24634513#5', '201238724', '-170018165#3', '-201950259#6', '26677214#5', '24608845', '24634509#1', '-201238718#0', '18809672#7', '25149001#4', '201089423#0', '160314345#2', '-24634508', '24634517#13', '201950259#0', '160314345#3', '-25117417#1', '-224892361#6', '26677540#0', '224251774#1', '24634511', '26677540#1', '-40888351#6', '24634413#0', '-201963533#5', '-36962701#1', '23525483#1', '-201950259#3', '-41203916#3', '201201945#1', '-201950247#5', '-40888443', '24634516#1', '-24634510#0', '18813598#8', '170018165#2.175', '24693977#1', '-26677417#0', '-22690206#2', '24634510#0', '-201963533#1', '201950247#8', '24608846#0', '26677216#1', '201201945#2', '315358253#2', '-26677541', '18813598#7', '170425366#0', '32124637#1', '-201238724', '22690205#3', '22724699#3', '201201953#2', '-173203413#7', '-224892361#1', '24634506#0', '-137454133#0', '-22724699#2', '32021112#0', '201238726#0', '-24634511', '-18813598#7', '-28111977#9', '26677214#3', '25145012#5', '-36962701#0', '168702039#1', '10427692#6', '-37681424#1', '224892361#3', '-26677216#4', '315358246', '-54169231#1', '-138300620#6', '40888354#0', '653473569#3', '-315358253#1', '201201950#4', '-24634507', '-24634510#18', '26677541', '26677542#0', '26677216#0', '24634506#2', '-201963533#1.145', '-137454133#1', '-26677542#1', '-32999434#1', '128906555', '-315358251#0', '-315358255#4', '-22690205#1', '-25145013#0'],\n",
    "        \"destinations\" : ['-25149001#4', '-128361102#1', '32978638#0', '386687235', '399835085#1', '-266565295#5', '24634414#1', '-24634415', '24634517#13', '201238729#3', '306240162#0', '-26677539', '447569997#1', '40888354#0', '315358242#0', '-24634506#2', '30482615#1', '-10427692#3', '24634510#6', '-24634505', '272042145', '-201238724', '-83304175#2', '26677541', '201238718#0', '-25145014#0', '24634509#1', '-315358242#1', '-315358242#0', '402600768#1', '28319300#3', '-201963533#4', '24608845', '-25190140#1', '137454133#2', '201201950#1', '40888354#1', '40888356#0', '138300620#0', '-201238718#0', '-22724699#1', '-32395288#1', '-218647954', '22724699#7', '26677214#3', '24634414#2', '-160314346#0', '201201953#10', '28111977#9', '-25145011#0', '-170018165#0', '23166741#5', '4942376', '24634506#2', '31860333#1', '176550249#4', '-315358258', '26642363', '224892361#3', '-160314345#0', '26677214#4', '-26677416#1', '28319300#0', '-25187893', '201238726#0', '24634411', '-32395361#1', '201950250#1', '-25145012#3', '-26677213#5', '22716549#0', '-315358252#1', '-24634413#2', '-393420106#1', '-24634521#1', '-201963533#1.145', '24634513#0', '-26677216#0', '-24693977#1', '-379510292', '-24634507', '-201963522#3', '-201950247#3', '-24634510#5', '-24634518#0', '176550249#3', '201950247#0', '24634413#1', '28111977#0', '24634511', '40888359', '170018165#2', '51857516#1', '32999110#0', '201201953#12', '-201201945#0', '-129379918#1', '-170018165#3', '24634510#1', '-26677542#1', '201950247#6', '-201238718#1', '53396619#3', '-201201945#0.78', '24634507', '-201950247#5', '-23436553#2', '-315358246', '-24634413#0', '-315392062#4', '653473569#1', '24634510#7', '201950263#0', '201950263#7', '25117417#1', '-22724699#2', '-22724699#11', '201963522#6', '-24634506#0', '-136436468#0', '-10427692#7', '-24634510#18', '-40888350', '201950259#0', '-201950259#6', '-24634513#5', '315392062#0', '201950250#0', '-22690205#3', '-26677540#1', '-138300620#6', '-201950263#6', '26677542#1', '-25187895#1', '-22690206#1', '160314346#0', '-53396619#2', '40888354#2', '-26677417#0', '-32999434#1', '-24634508', '-24634514#1', '24634517#1', '-201950247#7', '129379967#0', '315358257#0', '-315358248#1', '23525483#1', '-23525483#5', '286646456#1', '-22716549#6', '-224892361#6', '22724699#3', '136436468#1', '-129379922', '-24634414#5', '160314345#1', '204588664#0', '22724699#2', '315358245.27', '286646456#0', '160314345#2', '-173203413#7', '-315358257#2', '-286646456#0', '-26676668', '315358255#0', '23436553#2'],\n",
    "        \"number_of_paths\" : 4,\n",
    "        \"beta\" : -.5,\n",
    "        \"visualize_paths\" : True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(seed=42, create_agents=False, create_paths=False, **env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  2017 \n",
      "\n",
      "Number of human agents is:  2017 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(human_learning_episodes):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\environment.py:495\u001b[0m, in \u001b[0;36mTrafficEnvironment.step\u001b[1;34m(self, machine_action)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_rewards()\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# If there are only humans in the system\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmachine_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmachine_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_rewards()\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\environment.py:738\u001b[0m, in \u001b[0;36mTrafficEnvironment.simulation_loop\u001b[1;34m(self, machine_action, machine_id)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;66;03m# If all machines that have start time as the simulator timestep acted\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine_same_start_time:\n\u001b[1;32m--> 738\u001b[0m     travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_help_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_dict \u001b[38;5;129;01min\u001b[39;00m travel_times:\n\u001b[0;32m    741\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtravel_times_list\u001b[38;5;241m.\u001b[39mappend(agent_dict)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\environment.py:619\u001b[0m, in \u001b[0;36mTrafficEnvironment._help_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39madd_vehicle(action_dict)\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_actions[agent\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m action_dict\n\u001b[1;32m--> 619\u001b[0m timestep, arrivals, teleported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m travel_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m veh_id \u001b[38;5;129;01min\u001b[39;00m arrivals:\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\simulator.py:324\u001b[0m, in \u001b[0;36mSumoSimulator.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m teleported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m veh_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_vehicles\u001b[38;5;241m.\u001b[39mcopy():\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetSpeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mveh_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_vehicles[veh_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_vehicles[veh_id] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstuck_time:\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_vehicle.py:307\u001b[0m, in \u001b[0;36mVehicleDomain.getSpeed\u001b[1;34m(self, vehID)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetSpeed\u001b[39m(\u001b[38;5;28mself\u001b[39m, vehID):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"getSpeed(string) -> double\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    Returns the (longitudinal) speed in m/s of the named vehicle within the last step.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAR_SPEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehID\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:147\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:152\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m r\u001b[38;5;241m.\u001b[39mreadLength()\n\u001b[0;32m    154\u001b[0m response, retVarID \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:130\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recvExact()\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(human_learning_episodes):\n",
    "    env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mutation**: a portion of human agents are converted into machine agents (autonomous vehicles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  2017 \n",
      "\n",
      "Number of human agents is:  1977 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  40 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `TorchRL` enables us to make different groups with different agents. Here, all the AV agents are included in one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = {'agents': [str(machine.id) for machine in env.machine_agents]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True, # Whether to use the mask in the outputs. It is important for AEC environments to mask out non-acting agents.\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False, # Whether the environment’s done keys are set by aggregating the agent keys using any() (when True) or all() (when False).\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code style=\"color:white\">check_env_specs()</code> function runs a small rollout and compared it output against the environment specs. It will raise an error if the specs aren't properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 00:48:14,674 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy/Actor network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = False \n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs = env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs = env.action_spec.space.n,\n",
    "        n_agents = env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=policy_network_depth,\n",
    "        num_cells=policy_network_num_cells,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"logits\")],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[(\"agents\", \"logits\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=Categorical,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The critic reads the observations and returns the corresponding value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1, \n",
    "    n_agents=env.n_agents,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=critic_network_depth,\n",
    "    num_cells=critic_network_num_cells,\n",
    "    activation_class=torch.nn.ReLU,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"state_value\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  \n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,\n",
    ")\n",
    "loss_module.set_keys( \n",
    "    reward=env.reward_key,  \n",
    "    action=env.action_key, \n",
    "    sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "    value=(\"agents\", \"state_value\"),\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ") \n",
    "\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = -5.96999979019165: 100%|██████████| 10/10 [30:16<00:00, 181.67s/it]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "loss_values = []\n",
    "loss_entropy = []\n",
    "loss_objective = []\n",
    "loss_critic = []\n",
    "\n",
    "for tensordict_data in collector: ##loops over frame_per_batch\n",
    "\n",
    "    ## Generate the rollouts\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),  # Adjust index to start from 0\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),  # Adjust index to start from 0\n",
    "    )\n",
    "\n",
    "    # Compute GAE for all agents\n",
    "    with torch.no_grad():\n",
    "            GAE(\n",
    "                tensordict_data,\n",
    "                params=loss_module.critic_network_params,\n",
    "                target_params=loss_module.target_critic_network_params,\n",
    "            )\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  \n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    ## Update the policies of the learning agents\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            ) \n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss_values.append(loss_value.item())\n",
    "\n",
    "            loss_entropy.append(loss_vals[\"loss_entropy\"].item())\n",
    "\n",
    "            loss_objective.append(loss_vals[\"loss_objective\"].item())\n",
    "\n",
    "            loss_critic.append(loss_vals[\"loss_critic\"].item())\n",
    "\n",
    "\n",
    "   \n",
    "    collector.update_policy_weights_()\n",
    "   \n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))  # Get done status for the group\n",
    "\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "\n",
    "\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval() # set the policy into evaluation mode\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.rollout(len(env.machine_agents), policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Check `\\plots` directory to find the plots created from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plotter_params['phases'] =[0, human_learning_episodes, int(training_episodes) + human_learning_episodes],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\environment\\environment.py:799\u001b[0m, in \u001b[0;36mTrafficEnvironment.plot_results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method that plot the results of the simulation.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m \n\u001b[0;32m    795\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124;03m        None\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[43mplotter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplotter_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\services\\plotter.py:717\u001b[0m, in \u001b[0;36mplotter\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m    714\u001b[0m     params \u001b[38;5;241m=\u001b[39m params[kc\u001b[38;5;241m.\u001b[39mPLOTTER]\n\u001b[0;32m    716\u001b[0m plotter \u001b[38;5;241m=\u001b[39m Plotter(params)\n\u001b[1;32m--> 717\u001b[0m \u001b[43mplotter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plotter\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\services\\plotter.py:91\u001b[0m, in \u001b[0;36mPlotter.plot\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot the results of the training\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_episodes()\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_mean_rewards\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualize_mean_travel_times()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualize_tt_distributions()\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\RouteRL_exps_benchmark\\RouteRL\\routerl\\services\\plotter.py:141\u001b[0m, in \u001b[0;36mPlotter.visualize_mean_rewards\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phase_idx, phase \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphases):\n\u001b[0;32m    140\u001b[0m     color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase_colors[phase_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase_colors)]\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxvline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphase_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick_label_size)\n\u001b[0;32m    148\u001b[0m plt\u001b[38;5;241m.\u001b[39myticks(fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick_label_size)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\matplotlib\\pyplot.py:2731\u001b[0m, in \u001b[0;36maxvline\u001b[1;34m(x, ymin, ymax, **kwargs)\u001b[0m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39maxvline)\n\u001b[0;32m   2730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maxvline\u001b[39m(x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, ymin: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, ymax: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Line2D:\n\u001b[1;32m-> 2731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxvline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mymin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mymax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\anaconda3\\envs\\torchrl\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:852\u001b[0m, in \u001b[0;36mAxes.axvline\u001b[1;34m(self, x, ymin, ymax, **kwargs)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;66;03m# Strip away the units for comparison with non-unitized bounds.\u001b[39;00m\n\u001b[0;32m    851\u001b[0m xx, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_unit_info([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)], kwargs)\n\u001b[1;32m--> 852\u001b[0m scalex \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xaxis_transform(which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    855\u001b[0m l \u001b[38;5;241m=\u001b[39m mlines\u001b[38;5;241m.\u001b[39mLine2D([x, x], [ymin, ymax], transform\u001b[38;5;241m=\u001b[39mtrans, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJNCAYAAADgesaeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7nklEQVR4nO3de5hdZX03/N/ec54kMzmSkMyQcBAUFCSFQFTEPAoe0MAFxdb3FY+NVVLAFyWPiV6PuXxaQ0EtNUUsUH0LaKSvBaUPahNAoxWMx0JakOMEApmcycxkMue93j9ChswpDLBn7Z09n891rSuz7nXvtX8Jd/ZMvtz3vTJJkiQBAAAAACnKFroAAAAAAMYfoRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJC68kIX8HLlcrnYsmVLTJo0KTKZTKHLAQAAAOAgSZJEW1tbzJ49O7LZkedDHXah1JYtW6KxsbHQZQAAAABwCJs3b46GhoYRrx92odSkSZMiYv9vrK6ursDVAAAAAHCw1tbWaGxs7M9wRnLYhVIHluzV1dUJpQAAAACK1Ettu2SjcwAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSJ5QCAAAAIHVCKQAAAABSV17oAgAAoJQlSVLY94+IXJIMe/TlciNeyyVJ9B3i2mhen81khhxl2ezA81H0Ga7fSH0ymUxB/7wBGD2hVIF87eabY2VTU8Hev7A/GhVeUuAfVgr951/Q9/dnP675u1dg/vwLZ7z/2QsJSNlYBV4vt8+oQ7ixvPcYh4D5rLssm42KF64JF2F8EEoVSFdPT7RVVha6DAAAKDkHZmr1FroQXrHybLb/qDjo6/62srKhbcP0G7ZvJjOgLSsAo8ice+yxce6xxxa6jFQIpQAAACgqvblc9OZyhS4DCmJSZaVQCgAAYDjZXC4yEZFJksi+cGSSJLIvtGWSJDKxf+lokslEkslELiJyB75+4dckInJZz14CGK+EUgXy5+96V7zh0UcLXUZERBTDZNViWTNeDFUUQw0RxVGHcVF8iuG/SeEr2K8Y6iiGGiKMi4MVQx3+e7yoGOooVA1JLhe5np7I9fRE0tMT0dMT0du7/9eDz7u7I3OgT1dXRG9vZHp7I9fdHUlHR+Ta2yPZty9y7e3R19YWuX37IunszG+t8WJ4lTsosMrFQYHWgfYYGGwdaBvptQNCsBdeO2wwNkLfwfcY9r6D2gffd7ha+zKZiGw2oqIikoqKiPLy/l8PPnLl5RFlZZGUle3vk81GcuDXsrKIbDZy2ez+84OOg+saadP64TapH25j+9H0O7hPofewAw4vQqkCOfroo+Poo48udBkAAPCyJH190dfdHbmursh1dUVfZ+f+Xw+cd3VFrru7/3zItRde09fRMeDoPfB1Z2f07ds34BqvTLayMspqal48amujrLo6ympqovxAW3X1/vaD+1VXR/kwbQf6HXhttqpqSBCevBBMvdzQq++Fr3sPOnoGnffmctHT1ze0bZh+o+3b09cnSKPovHHWrEKXkBqhFAAAMGqZsrIor6mJqKlJ5f2SJIlcV1f07ts3MLA68HVn5/5rg9oO7jfgtcMEYrnu7lR+L2nLdXdHrrs7elpaxuYNMpmhwdaBQOuF8Gu44+BALFNeHuVlZVFRVhaZlziy5eWRyWYjU1ERmfLy/Uc2G0kuF5HLDfg1yeUiMpn9/V844mV+nclmC/70VMapcTTuhFIAAEDRymQy+wOO6uoxe49cb2/kOjtfnK01wjHk+kHh15CZXoNCsyjFTbuTZP/vcd++iN27C10NlIzjr7giTrj88kKXkQqhFAAAMK5ly8sjO3FilE+cOCb3T5Jk/5LGwbO1Dp7pNajt4H79gdcwM70OXMvlea8vgDQIpQAAAMZQJpOJsqqqKKuqiqivH5P3SHK5Q+/VNdyeXYOXQL5EMJb09IxJ7cD4JZQCAAA4zGWy2SivrY3y2toxe49cT8/wwdcIe3WNGIwdFJAlfX0Dj1wukt7eyPX1Rbxwnuvt7b9ekssgYRwTSgEAAPCSshUVka2oiIpJkwpWQ5LLvRhg9fZGksu9uEF5Wdn+pwFms/1PBUz6+vY/ETCX2x9yvfD1gYArSZJDfw0FUDllSqFLSI1QCgAAgMNC/1PxKipG17+sbIwrAl6NbKELAAAAAGD8EUoBAAAAkDqhFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkLq8hlJLliyJTCYTmUwmFi9ePGK/m266KT7zmc/Exz72sfjP//zPfJYAAAAAwGGgPF832rp1a9TW1sa6desiIuKEE04Ytt+9994bd999d/zgBz+Itra2WLhwYWzYsCEmTJiQr1IAAAAAKHJ5C6VWr14dCxYsiLPPPjsqKipG7HfttdfG+9///oiImDRpUsydOzfWrFkTf/EXfzFs/66urujq6uo/b21tzVfJAAAAABRIXpbv9fT0xJ133hmXXHJJNDQ0xNq1a4ft19fXF+vXr4+5c+f2tx1//PGxfv36Ee+9atWqqK+v7z8aGxvzUTIAAAAABZSXUKqioiIefvjhaG5ujgsvvDDOO++8ePDBB4f02717d3R2dsbUqVP72yZOnBhbtmwZ8d7Lly+PlpaW/mPz5s35KBkAAACAAhr18r3ly5fHxo0bh712/vnnx5IlS2LmzJlxww03RHt7e1x//fVx4403DuiXyWQiIqK6urq/rbu7+5DL/aqqqqKqqmq0ZQIAAABwGBh1KLVq1apR33Tp0qWxcuXKIe3Tpk2LqqqqaGlp6W9ra2uL2bNnj/reAAAAABz+8rJ8b8hNs9mYP3/+kPZMJhOLFi2Kxx9/vL/tiSeeiEWLFo1FGQAAAAAUqbyEUk1NTXHHHXdExP5Nz2+55Za46qqrIiIiSZJYsWJFNDc3R8T+WVQ/+clPImL/k/See+65uPjii/NRBgAAAACHiUySJMmrvcmGDRviggsuiDlz5sTChQtj2bJl/U/J6+joiBNPPDHWrFkTZ555ZkREXH311bF3797YvXt3fPKTn4yTTz551O/V2toa9fX10dLSEnV1da+2dAAAAADyaLTZTV5CqTQJpQAAAACK12izmzHZUwoAAAAADkUoBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDq8hpKLVmyJDKZTGQymVi8ePGwfbZt2xbvec97YtKkSXHWWWfFo48+ms8SAAAAADgM5C2U2rp1a9TW1sa6deti3bp1cf311w/b7+qrr44lS5bEPffcE729vXHRRRflqwQAAAAADhPl+brR6tWrY8GCBXH22WdHRUXFsH2SJInzzz8/3va2t0VExLe+9a048cQTY8eOHTFjxoxhX9PV1RVdXV39562trfkqGQAAAIACyctMqZ6enrjzzjvjkksuiYaGhli7du2w/TKZTH8gFRExZ86cmDhxYkyePHnEe69atSrq6+v7j8bGxnyUDAAAAEAB5SWUqqioiIcffjiam5vjwgsvjPPOOy8efPDBl3zdhg0b4mMf+9iIM6siIpYvXx4tLS39x+bNm/NRMgAAAAAFlEmSJBlNx+XLl8fGjRuHvXb++efHkiVL+s8/9KEPRXV1ddx4442HvOeHP/zh+NrXvhbTpk0bdcGtra1RX18fLS0tUVdXN+rXAQAAADD2RpvdjDqUejk2bNgQK1eujB//+Mcj9vne974XDQ0N8Za3vOVl3VsoBQAAAFC8Rpvd5O3pewNums3G/PnzR7y+YcOGKCsre9mBFAAAAAClIS+hVFNTU9xxxx0RsX/T81tuuSWuuuqqiNj/xL0VK1ZEc3NzRERs3Lgx7rrrrjj99NNj06ZNsWHDhrjlllvyUQYAAAAAh4m8LN/bsGFDXHDBBTFnzpxYuHBhLFu2rP8peR0dHXHiiSfGmjVrYsaMGbFw4cLYsWPHgNf/6le/ijPOOGNU72X5HgAAAEDxKuieUmNJKAUAAABQvAq6pxQAAAAAHIpQCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDU5TWUWrJkSWQymchkMrF48eKX7H/11VfHRz7ykXyWAAAAAMBhoDxfN9q6dWvU1tbGunXrIiLihBNOOGT/hx56KG688cZ461vfmq8SAAAAADhM5C2UWr16dSxYsCDOPvvsqKioOGTf7u7uuOmmm+KDH/xgPPPMM4fs29XVFV1dXf3nra2teakXAAAAgMLJy/K9np6euPPOO+OSSy6JhoaGWLt27SH7f+UrX4nPfOYzkc2+9NuvWrUq6uvr+4/GxsZ8lAwAAABAAeUllKqoqIiHH344mpub48ILL4zzzjsvHnzwwWH73n///dHQ0BDz5s0b1b2XL18eLS0t/cfmzZvzUTIAAAAABTTq5XvLly+PjRs3Dnvt/PPPjyVLlsTMmTPjhhtuiPb29rj++uvjxhtvHNCvvb09fvCDH8Q111wz6gKrqqqiqqpq1P0BAAAAKH6ZJEmSfN90w4YNsXLlyvjxj388oP3WW2+NpUuXRmVlZURE7Nu3L3K5XJx44onx+9//flT3bm1tjfr6+mhpaYm6urp8lw4AAADAqzDa7CZvG50fLJvNxvz584e0X3TRRbFo0aL+86997Wvx7LPPxte//vWxKAMAAACAIpWXPaWamprijjvuiIj9m57fcsstcdVVV0VERJIksWLFimhubo7a2tpoaGjoP+rq6qK2tjZmzZqVjzIAAAAAOEzkJZTavn17LF26NE477bS48sorY9myZTF58uSIiOjs7Iw1a9bE008/nY+3AgAAAKAEjMmeUmPJnlIAAAAAxWu02U1eZkoBAAAAwMshlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpAAAAAFInlAIAAAAgdXkNpZYsWRKZTCYymUwsXrz4kH137doV11xzTXz3u9+Nhx56KJ9lAAAAAFDkyvN1o61bt0ZtbW2sW7cuIiJOOOGEEfs2NTXFZZddFv/8z/8c06ZNy1cJAAAAABwm8hZKrV69OhYsWBBnn312VFRUjNivq6srLrjggvj+978/qkCqq6srurq6+s9bW1vzUi8AAAAAhZOX5Xs9PT1x5513xiWXXBINDQ2xdu3aEfv+4z/+Y1RXV8ftt98e55xzTlx77bWRJMmI/VetWhX19fX9R2NjYz5KBgAAAKCAMsmhEqGXadu2bbFy5cq4+eab47e//W2ccsopQ/osXLgwzjrrrPjbv/3beOqpp+LUU0+Na665Jj75yU8Oe8/hZko1NjZGS0tL1NXV5at0AAAAxpmevbujr6u90GXAAOU1dVFeW1/oMl6V1tbWqK+vf8nsZtSh1PLly2Pjxo3DXjv//PNjyZIl/ecf+tCHorq6Om688cYhfevq6uK73/1uvPe9742IiI9//OPxyCOPxP333z+aMkb9GwMAAICevbsi19MVVVNm97flejrj6buvjecfvreAlcHwjjzrI3HkWR8pdBmvymizm1HvKbVq1apRv/nSpUtj5cqVw17r7e2Nvr6+/vOTTz45/uM//mPU9wYAAIBD6djRFHse/UXsefTn0bHtiYiImDR3fjSce3mU106OJ/+/5bFvyyMFrhLI20bnB8tmszF//vxhr5188snx+OOPv1hAeXmcdNJJY1EGAAAA40hXy9bY/JO/i9YnNwy51vb07+ORf/p4VNROjp69uwpQHTBYXjY6b2pqijvuuCMi9m96fsstt8RVV10VERFJksSKFSuiubk5IiKuvPLK+Nd//df+1z7wwANxxRVX5KMMAAAAxoEk1xvP//HnseN3P4zWp34dXXuaY8fvfhiP3PTRYQOpfrk+gRQUkbxsdL5hw4a44IILYs6cObFw4cJYtmxZ/1PyOjo64sQTT4w1a9bEmWeeGRER1157bWzdujVmzJgRU6dOjU984hOjfi97SgEAAIxfnbueiU3/tiovy+8q62bGse9fFdXTj8pDZZAnmWxkMnmZQ1Qwed/ovFgIpQAAAMafJMnFjt/eGc/99B8j6e1+yf7ltVNiyomLov41b462p/8Q2zfcHklfT//1mpnHxbHvvzoqJ00fy7JhXMr7RucAAACQtiRJovXJDdH8H/886tlR0055T8x5+6eivHpSRETUHf0nMe0N74zmX3w72p97OOqPOzNmv+0TUVZVO5alAy9BKAUAAEDR6evcGy1P/Tq2b/iX2Nf8x2H7lNdOiaSvO/q62iMiorJ+Vhz1ns9G3dGnDelbPa0xjr7gf41pzcDLI5QCAACgYPY+81Ds+P0PI9fbFWWVtZGtrI3OnU2x99n/isj1jfi6GaddFHMWLYlMeVX07tsTvfv2RPW0xshk/TMXDhf+tgIAAFAQe595KB777v9zyPBpsKops6PxXZ+JuqP/pL+tYsKUqJgwZSxKBMaQUAoAAIDUdbftiKfu/OKoA6mqKbNj1psviaknnROZMv+UhVLgbzIAAACpyvV2R9MdX4ze9ucP2S9TVhETj3pjTH39OTH1pP9haR6UGH+jAQAAGBNJkkSupyN697VEb0dL9O5rib6O1tjz2C+j/bmHB/Sd0PD6mNDw+sh1tke2sjomzj01Js09NcoqawpUPTDWhFIAAACMWq6vJ3rbn4/e9uejp/356N33fPS074nefc9Hb/ueF9r2bzze29ESSW/3S96zcvLsOPbiL0d5TV0KvwOgWAilAAAAiFxvd/Ts3RU9bTv3H3t3RvfeXdG7d9f+9r27o6d9d/R1tOb1fTPlVXHMRV8SSME4JJQCAAAocUlfb3S37Yju1u3R3botelq295/3tG6Pnrad0dvRUpDa5r7ns1E787iCvDdQWEIpAACAw1yS5KKndUd07WmO7j3N+39taY7ulm3R1bI1etp2RiS5gtaYyZZHWU1dlNfWR3lNXVRMmBpTTnpHTD7+zQWtCygcoRQAAMBh4EDw1Ll7c3Ttfja6dj8bnc8/G13Pb4nuPc2R9PWkW1AmGxUTpkT5hClRXjvlha8nR3nti0dFbX3/19nK2shkMunWCBQ1oRQAAEARyfX17A+cdj79wrEpOnc9E527n42kt2vM3z9bUR0Vk6ZHxcRpA47yiVOjYsK0qJg4NSomTImymrrIZLJjXg9QuoRSAAAABZAkSfS07YiObU/Evu1PRuf2p6JjR1N07t4ckesbk/fMVlRHZd3MqKg7IirrZkRl3RFRMWlGVNbNiIpJM6Ji0vQoq5poRhOQCqEUAADAGEuSJLr3bIn25kejY+vjsW/rY7Fv2+N5f5JdtqI6KicfGVWTj4zK+iOjcvKsqKqfFZX1M6OybuYLs5sETkBxEEoBAADkWW9Ha7Q/9/D+Y8sjsW/ro3kLoLKVNVE1tTGqpzZE1ZSGqJo6J6omz46qKXOifMIUoRNw2BBKAQAAvApJkkR3y9bYu/mhF46N0bXrmVd934pJ06N6+ryonjY3qqcfFdVTj4rq6UdF+YSpgiegJAilAAAAXqbu1u3R9vQfom3T72Pv0/8Z3a3bXvG9slUTombGMVFzxLFRc8QxUTPj6KieMS/KqyflsWKA4iOUAgAAeAm53q7Y+8yD0frUb6L1qd9E585Nr+g+5bVTovbI46N21vFRM/M1UTvrNVFZP8vMJ2BcEkoBAAAMo2fvrmh54oFoeeKBaGv6XeR6Ol/W6zPllVE764SYMOfEmDD7dTFh9uuiou4IARTAC4RSAAAAL+hu3R57/rg+nv/jz6P92f+KiGTUry2vqY8JjW+IiY0nx8TGN0TNzOMiW1YxdsUCHOaEUgAAwLjW074n9vxxfez+73ui/dmNo35dWfXEmHjUqTFp7qkxad6pUT19nllQAC+DUAoAABh3kr7eaHnyV7HroR9HyxO/isj1vfSLMtmYMOfEqDtmQdQdfVrUHnlCZLJlY18sQIkSSgEAAONG157m2PmHu2LXgz+J3n3Pv2T/bGVN1B17ZtS/ZmHUH7Mgymsnj32RAOOEUAoAAChpSZJEW9NvY8fv7oyWxx+Il9onqqx6YtQf/5aYcsJbY9LRfxLZ8qp0CgUYZ4RSAABASUr6emP3I/fF9l/dHh3bnzxk30x5VUx+zZtiyknviLpjTo9seWVKVQKMX0IpAACgpOR6u2PXgz+KrQ98N3patx+y74Q5J8W0U94dU163KMqqJqRUIQARQikAAKBE5Pp6YvdDP4nmX956yDAqWzUhpr3hnTH91MVRM2NeegUCMIBQCgAAOKwlSRJ7Hv1FbPnpN6Pr+S0j9qua2hhHLPjTmPr6c6KssjbFCgEYjlAKAAA4bO3b+lg8e8/1sfeZB0fsM6Hh9THzzA9E/WsWRiaTTbE6AA5FKAUAABx2+rraY8vPboodv/thjPQ0vYmNp8SRb/1ITJp7arrFATAqQikAAOCwsufRX8Tmf//76Nm7c9jrE+acGEe+9eMxad78yGQyKVcHwGgJpQAAgMNCb0drPPPjr8WeP/5s2OuV9TNjzqJPxuTXvU0YBXAYEEoBAABFr23TH2LTv305etp2DLmWKa+KI9/yoThiwZ9GtryqANUB8EoIpQAAgKKV5Hpjy/pvxbYH1sRwe0fVHXN6NL7ryqiafGT6xQHwqgilAACAotS7ryWafvClaNv0uyHXymrqovGcy2LKSe+wVA/gMCWUAgAAis6+7U/GU9//QnTvaR5ybdK8+TH3fcujctKMAlQGQL4IpQAAgKKy57FfxqYf/u/I9XQOaM9ky2P2oiVxxIKLI5PJFqg6APJFKAUAABSNXQ/9OJ6++9qIJDegvWLitDjmoi/FhDknFagyAPJNKAUAABSFbRtuj+fuvWFI+4Q5J8UxF30pKiZOK0BVAIwVoRQAAFBQSZJE8/p/iq333zbk2tQ3vDOOevdnIlteWYDKABhLQikAAKCgtt5/27CB1MwzPxCzF33C0/UASpRQCgAAKJjtv7kjmtf/05D2Of/jkzHzzD8vQEUApEUoBQAAFMSuh34Sz677+pD2o97z2Zj+xvcWoCIA0uQ5qgAAQOpan/p1PH33NUPaG865XCAFME4IpQAAgFR17n42mn7wpYgkN6D9yLM/HkecfmGBqgIgbUIpAAAgNX1d++Kp738h+jr3Dmg/4sw/j1lv+mCBqgKgEIRSAABAKpIkF5v+7cvRuXPTgPbJrz075iz6S0/ZAxhnhFIAAMCrliS5SHJ9h+yz7f7vRstj/zGgrXrGMTH3vf9TIAUwDnn6HgAA8Kq0b3kkNt315eht3x0z3/TBmLXwA0P67Gt+NLb84tsD2spq6uLYP/3rKKusTatUAIqImVIAAMCr8uw910fX7s3R19UeW376j7Fv62MDrud6u2LTv62KOHgmVSYbR1/wv6JqyuyUqwWgWAilAACAVyzX2xXtzz08oG3Xxn8fcL7l598eso/UrDd9MOqOPm2sywOgiAmlAACAV6xz1zMRSW5A2/MP39e/v9TezQ/F9l/dPuB6zczXxKy3XJJajQAUJ6EUAADwinXu2DSkrbf9+Wh7+g+R6+mKp//P1RGR9F/LlFXEvPctj2xZRXpFAlCUbHQOAAC8Yh07moZt3/3f98S+LX+Mrue3DGg/8q0fi5ojjkmjNACKnFAKAAB4xQbvFXXAnj/+PA6eIRURMWHOSTHzjPePfVEAHBaEUgAAwCs20kypXPe+QS2ZaHznFZHJlo19UQAcFuwpBQAAvCJ93R3Rvad5VH2nnfLuqJ11/BhXBMDhRCgFAAC8Ip07nx5Vv2xFdcw+++NjXA0AhxuhFAAA8Ip07hy4dK9i4vTIDPNUvVlv+mBUTJyWVlkAHCaEUgAAwCsyeD+p2tmvjbpjzxjQVlk3M45YcHGaZQFwmBBKAQAAr0jnjk0DzmtmHB0zz/xAROaFf2ZkstHwzisiW1GVfnEAFD1P3wMAAF6RwTOlqqfPi4kNJ8UJH1odrU/9NibNPTUmHnVygaoDoNgJpQAAgJetr3Nv9LTtGNBWM+PoiIiYMOekmDDnpEKUBcBhxPI9AADgZevYuWlgQ7YsqqY1FqQWAA5PQikAAGBEfd0d0de9b0j74P2kqqc2RnaYJ+8BwEgs3wMAAIbV8uSG2HTX30RfR2sccfqfxpx3LI1MJhMRw+8nBQAvh5lSAADAsJ6794bo62iNiIjtv/l+7HnkZ/3XOncODKUO7CcFAKMllAIAAIbo69wbnYP2jXr2vhsi19MZEcPMlJoxL6XKACgVQikAAGCIIRuZR0RP6/bY+sCa6Ny1OXrbnx9wzUwpAF4ue0oBAABDDN7I/IBtv1oTO/9w14C2TFlFVE2ZnUJVAJQSoRQAADDE4OV5ByS93dHb2z2gbdK8+ZHJ+qcFAC9PXr9zLFmyJG6++eaIiHjf+94Xd91115A+vb29sWLFipg+fXq0t7fHlClT4tOf/nQ+ywAAAF6lzhFCqcGqZxwdR73nqjGuBoBSlLdQauvWrVFbWxvr1q2LiIgTTjhh2H7f/OY3o76+PpYtWxYREW9/+9tj4cKFccYZZ+SrFAAA4FUaaabUwSbOPTWOveh/R1n1xBQqAqDU5C2UWr16dSxYsCDOPvvsqKioGLHfI488EpMmTeo/r66ujpaWlhH7d3V1RVdXV/95a2trfgoGAACG1dP+fPTuG7iR+cyFH4htD6zpP59y0jti7nv/Z2TLRv7ZHwAOJS9P3+vp6Yk777wzLrnkkmhoaIi1a9eO2PfCCy+M1atXxy9/+ctoamqK6dOnxznnnDNi/1WrVkV9fX3/0djYmI+SAQCAEQze5DxTVhGzz/6LmLt4RUx9/Tkx930rYt7iFQIpAF6VTJIkSb5utm3btli5cmXcfPPN8dvf/jZOOeWUYfv9wz/8Q3zuc5+LxYsXx2233RbZ7MjZ2HAzpRobG6OlpSXq6uryVToAAPCC7b+5I55d9/X+85qZr4nXffymAlYEwOGktbU16uvrXzK7GfXyveXLl8fGjRuHvXb++efHkiVLYubMmXHDDTdEe3t7XH/99XHjjTcO27+2tjZuv/32+OhHPxqXXnppfPOb3xzxfauqqqKqqmq0ZQIAAK/S4E3Oa2YcXaBKAChlow6lVq1aNeqbLl26NFauXDnstVtvvTU6OjrivPPOi/vuuy/e/OY3x6JFi+LP/uzPRn1/AAAgf5IkiUhykcmWRUREx86BoVS1UAqAMZC3jc4Pls1mY/78+cNeu/322+Oyyy6LiIjXv/71ceWVV8YvfvELoRQAAKSku3V7NP/i/429z/xn9Hbujb6u9shkslF/3MKYt/jzZkoBkIq8bHTe1NQUd9xxR0Ts3/T8lltuiauuuioi9v9flxUrVkRzc3NERLzxjW+MP/zhD/2vLSsriwULFuSjDAAAYBQ2r/372PXgj6Lr+S3R19EakeuLpK8n9jz689j0f66Ovq72Af2FUgCMhbyEUtu3b4+lS5fGaaedFldeeWUsW7YsJk+eHBERnZ2dsWbNmnj66acjIuLzn/98bN26Na677rq44YYborKyMi655JJ8lAEAALyEXE9XtDz+wIjX9zzy0wHn2craqKg7YqzLAmAcysvyvTPOOKN/JtRgNTU10dTUNOD8uuuuy8fbAgAAL1PHzqaIJDfq/jUz5kUmkxnDigAYr/IyUwoAADg8dGx7YsB55eQjY+ob3jlif5ucAzBWhFIAADCODA6lao98bcxZ9InIlFcN279mulAKgLEhlAIAgHFk37YnB5zXHnFsVEycFkecfuGw/c2UAmCsCKUAAGCcSJJcdGwfGErVzDwuIiJmnvl/RVnVhCGv8eQ9AMaKUAoAAMaJ7j3NkeveN6DtQChVXjMpZp75gQHXymunRPmEKanVB8D4IpQCAIBxYvDSvfKa+qiYOK3/fMbpF/WHVBERR5x+kSfvATBmygtdAAAAkI7Bm5zXzDxuQOhUVlkTr/m//y5an/x1lNfWx6R5f5J2iQCMI0IpAAAYJ4buJ3XskD7l1ZNi6klvT6skAMYxy/cAAGCcGG6mFAAUilAKAADGgd6O1uhu3TagrfYIoRQAhSOUAgCAcWDw0r1MWUVUTzuqQNUAgFAKAADGhcFL96pnHB2ZMlvMAlA4QikAABgH9g0KpWrtJwVAgQmlAABgHBjy5L0jhj55DwDSJJQCAIASl+vric4dmwa0efIeAIUmlAIAgBLXufPpSHK9A9pqzZQCoMCEUgAAUOJ69u4acF4xcVqUVU8sUDUAsJ9QCgAASlzS2z3gPFtZU6BKAOBFQikAAChxuUGhVKasokCVAMCLhFIAAFDikr5BM6XKKwtUCQC8SCgFAAAlzkwpAIqRUAoAAErckD2lzJQCoAgIpQAAoMTl+noGnGeEUgAUAaEUAACUuCEzpcqEUgAUnlAKAABKnJlSABQjoRQAAJS4oTOlbHQOQOEJpQAAoMQNefqemVIAFAGhFAAAlLhk0PI9T98DoBgIpQAAoMQNmSll+R4ARUAoBQAAJW7InlJmSgFQBIRSAABQ4nJ9ZkoBUHyEUgAAUOLMlAKgGAmlAACgxOV6B2507ul7ABQDoRQAAJS4ZNDyvWyZUAqAwhNKAQBAiRvy9D0zpQAoAkIpAAAocUnfwOV79pQCoBgIpQAAoMQNmSnl6XsAFAGhFAAAlLik10wpAIqPUAoAAEpcrs9MKQCKj1AKAABKXDJo+Z6ZUgAUA6EUAACUsCRJhmx07ul7ABQDoRQAAJSwZNDSvQjL9wAoDkIpAAAoYYOfvBdh+R4AxUEoBQAAJWzwk/ciIjJlQikACk8oBQAAJWzwk/cizJQCoDgIpQAAoIQNN1NKKAVAMRBKAQBACRs6UyoTkS0rSC0AcDChFAAAlLBk0EbnmfLKyGQyBaoGAF4klAIAgBI2ePletryiQJUAwEBCKQAAKGGDl+958h4AxUIoBQAAJWzw8j2bnANQLIRSAABQwnKD95Qqs3wPgOIglAIAgBI2ePmemVIAFAuhFAAAlLDBG51nhFIAFAmhFAAAlLAhe0rZ6ByAIiGUAgCAEjbk6XtmSgFQJIRSAABQwgYv38uW2+gcgOIglAIAgBI29Ol7ZkoBUByEUgAAUMKSvsEzpYRSABQHoRQAAJSwITOlLN8DoEgIpQAAoIR5+h4AxUooBQAAJWzo0/fMlAKgOAilAACghJkpBUCxEkoBAEAJGzpTSigFQHEQSgEAQAlLej19D4DiJJQCAIASNuTpe5bvAVAkhFIAAFDCkr7BM6VsdA5AcRBKAQBACRsyU8ryPQCKhFAKAABKmKfvAVCshFIAAFDCcoOW72Us3wOgSAilAACghA2ZKWX5HgBFQigFAAAlLNfn6XsAFKfyfN/w7rvvjieffDKOOeaYWLRoUUyYMGFIn5tuuin++Mc/xvPPPx+XX355vPGNb8x3GQAAQAw3U8ryPQCKQ15DqZUrV8bMmTPj8ssvH7HPvffeG3fffXf84Ac/iLa2tli4cGFs2LBh2PAKAAB4dTx9D4Bilbfle7fddlts2rQpPvWpTx2y37XXXhuLFy+OiIhJkybF3LlzY82aNSP27+rqitbW1gEHAADw0pJcb0SSG9Bm+R4AxSIvoVR3d3dcddVVcdRRR8XHPvaxuOCCC+Kxxx4b0q+vry/Wr18fc+fO7W87/vjjY/369SPee9WqVVFfX99/NDY25qNkAAAoebneniFtNjoHoFjkJZT66U9/Gj09PfHxj388vvWtb8WcOXPi3HPPja6urgH9du/eHZ2dnTF16tT+tokTJ8aWLVtGvPfy5cujpaWl/9i8eXM+SgYAgJI3eD+pCKEUAMVj1HtKLV++PDZu3DjstbPOOivmzZvXPwPqc5/7XHzjG9+I9evXx7nnntvfL5PJREREdXV1f1t3d3dUVIy82WJVVVVUVVWNtkwAAOAFub6hM6UyZTY6B6A4jDqUWrVq1YjXrrnmmujr6+s/b2xsjMmTJ8euXbsG9Js2bVpUVVVFS0tLf1tbW1vMnj375dQMAACMgplSABSzvCzfO/nkk+OJJ56IJEn628rLy+Okk04a0C+TycSiRYvi8ccf72974oknYtGiRfkoAwAAOIiZUgAUs7yEUuecc07Mmzcv/v3f/z0iIh5//PE4+eST4+STT44kSWLFihXR3NwcERFLly6Nn/zkJxER0draGs8991xcfPHF+SgDAAA4yJCZUtmyyGTLClMMAAwy6uV7h1JWVhY//OEPY8WKFfHYY4/FU089FbfddltERHR2dsaaNWti8eLFceSRR8Z73/ve+K//+q/4whe+ELt37441a9YM2GMKAADIj9ygUMrSPQCKSV5CqYiIY445Jr73ve8Naa+pqYmmpqYBbZ/73Ofy9bYAAMAIkr6BoZSlewAUk7ws3wMAAIqPmVIAFDOhFAAAlKhk0EbnQikAiolQCgAAStTgmVKZMqEUAMVDKAUAACVq8NP3MmZKAVBEhFIAAFCihizfs9E5AEVEKAUAACVqyPI9M6UAKCJCKQAAKFGevgdAMRNKAQBAiRq8fC9j+R4ARUQoBQAAJcpMKQCKmVAKAABKVNI3aE8pM6UAKCJCKQAAKFGJmVIAFDGhFAAAlChP3wOgmAmlAACgRA3e6DxbJpQCoHgIpQAAoESZKQVAMRNKAQBAiRry9D0bnQNQRIRSAABQogYv3zNTCoBiIpQCAIAS5el7ABQzoRQAAJSo3OCZUpbvAVBEhFIAAFCizJQCoJgJpQAAoER5+h4AxUwoBQAAJSrp8/Q9AIqXUAoAAEqUmVIAFDOhFAAAlKhk0Ebn2TKhFADFQygFAAAlykwpAIqZUAoAAEpQkiSevgdAURNKAQBACUpyvUPaMjY6B6CICKUAAKAEDZ4lFWGmFADFRSgFAAAlaPB+UhFmSgFQXIRSAABQggY/eS/CTCkAiotQCgAAStCwM6WEUgAUEaEUAACUoOH2lLJ8D4BiIpQCAIASlOsbGEplyioik8kUqBoAGEooBQAAJWjwTClL9wAoNkIpAAAoQbnegRudZ8uEUgAUF6EUAACUoGTQ8j1P3gOg2AilAACgBA1++l6m3CbnABQXoRQAAJSgpG/g8r2M5XsAFBmhFAAAlKDBM6Us3wOg2AilAACgBCW9g2dKWb4HQHERSgEAQAnK2egcgCInlAIAgBKU2OgcgCInlAIAgBI0ZE8pG50DUGSEUgAAUIKSvsEzpYRSABQXoRQAAJSgwRud21MKgGIjlAIAgBI0ePlexvI9AIqMUAoAAErQ0Kfv2egcgOIilAIAgBI0ePmePaUAKDblhS4AAADIv8mvfWtUTW2IpK87kt6emNjw+kKXBAADCKUAAKAETXnt2THltWcXugwAGJHlewAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOqEUgAAAACkTigFAAAAQOrKC13Ay5UkSUREtLa2FrgSAAAAAAY7kNkcyHBGctiFUm1tbRER0djYWOBKAAAAABhJW1tb1NfXj3g9k7xUbFVkcrlcbNmyJSZNmhSZTKbQ5bxsra2t0djYGJs3b466urpClwP9jE2KlbFJsTI2KVbGJsXK2KRYGZv5lyRJtLW1xezZsyObHXnnqMNuplQ2m42GhoZCl/Gq1dXVGewUJWOTYmVsUqyMTYqVsUmxMjYpVsZmfh1qhtQBNjoHAAAAIHVCKQAAAABSJ5RKWVVVVXzxi1+MqqqqQpcCAxibFCtjk2JlbFKsjE2KlbFJsTI2C+ew2+gcAAAAgMOfmVIAAAAApE4oBQAAAEDqhFIAAAAApE4oBQAAAEDqhFIAAAAApE4olaL29va49NJLY/ny5XH55ZdHV1dXoUtinPubv/mbyGQykclk4pRTTokI45TCueeee+KMM86ITZs29bcdajxu27YtPvGJT8SyZcvi85//fHiYLGNhuHEZMfznZ4TPUNLxox/9KI477riYOnVqXHbZZdHb2xsRh/5cfPTRR2PJkiXxmc98Jr761a8WqnRK3EhjMyJiyZIl/Z+bixcv7m/3/Zw03H///fG6170uJk+eHFdccUV/u581C08olaJPfepTcc4558SqVavitNNOi+XLlxe6JMaxrq6ueOaZZ2LdunWxbt26+P73vx8RximFsWPHjti7d2/8+te/HtB+qPF48cUXx6c+9am45pproqqqKlavXp122ZS4kcblSJ+fET5DGXs7d+6M73znO7FmzZpYvXp1fPvb347rrrsuIkb+XOzu7o4LL7wwvvSlL8VXv/rV+O///u+46667Cvi7oBQdamxu3bo1amtr+z83r7/++v7X+X7OWNu7d2/89Kc/jV/+8pfxne98J77xjW/EPffcExF+1iwKCal47rnnkurq6qSjoyNJkiTZvn17UlNTk7S2tha4MsarG2+8Mbn66quT9vb2/jbjlELq6+tLIiJpampKkuTQ4/GBBx5IGhsb+1/761//OmloaEhyuVwhSqeEDR6XSTL852eS+AwlHQ888ECyb9++/vNly5Yl73nPew75ubhmzZrkrLPO6r/2L//yL8mb3/zmVOum9I00NpMkSVasWJHcdtttSXd395DX+H7OWOvo6Bgwpk4//fTkvvvu87NmkTBTKiU/+9nPYvr06VFdXR0RETNmzIiqqqoh//cV0rJmzZr4/Oc/H7NmzYpbb701IoxTCiubHfgt6VDj8b777ou5c+f29z3++OPj2WefjaeeeirVmil9g8dlxPCfnxE+Q0nHmWeeGTU1Nf3nc+bMiYaGhkN+Lg53bcOGDZaXklcjjc2enp64884745JLLomGhoZYu3Ztfx/fz0lDdXV1ZDKZiNi/XO8Nb3hDvO1tb/OzZpEQSqXkueeei6lTpw5omzhxYmzZsqVAFTHe3XfffbFr16648sor48Mf/nDcfffdxilF5VDjcfC1iRMnRkQYq6RiuM/PCN/rKYzf/OY38Zd/+ZeH/Fwc7lpvb29s37499XoZPw6MzYqKinj44Yejubk5LrzwwjjvvPPiwQcfjIihn5u+nzOW7r///nj3u98de/fujY6ODj9rFgmhVEoymUx/AntAd3d3VFRUFKgiiKivr4+VK1fGF77whfj7v/9745SicqjxOPhad3d3RISxSmoGf35G+F5P+pqammLKlCkxf/78Q34u+swkbQePzQNmzpwZN9xwQ3zgAx/o31PK2CRNxxxzTHz0ox+Ne++9Nz772c/6WbNICKVSMnv27GhpaRnQtnfv3pg9e3aBKoIXLV26NDZv3mycUlQONR4HX2tra+t/DaTpwOdnhO/1pCuXy8UNN9wQ11xzTUQMHX8Hfy4Od62ysjKmTZuWbtGMC4PH5mCH+tz0/ZyxNGvWrPjoRz8aX/nKV2L9+vV+1iwSQqmULFq0KJ599tn+hPXAtL8FCxYUsiyIiP17psyfP984pagcajy+/e1vj8cff7y/7xNPPBHHHHNMHHXUUQWplfHrwOdnhO/1pOu6666LT3/60/3/J/9Qn4vDXXvLW97i//gzJgaPzcEO/tz0/ZxCOO2002LOnDl+1iwSQqmUHHnkkfGud70r1q9fHxERa9eujUsvvXTED2sYSzt37ozbbrst+vr6IkmS+Lu/+7v467/+a+OUgkqSZMCvhxqPZ5xxRkyZMqX/h4W1a9fGlVdeWZjCKWmDx+VIn58RvteTnq997WtxwgknRHd3dzz11FPxrW99K6ZNmzbi5+L5558fmzdvjtbW1iHXIJ+GG5uPP/543HHHHRER0dPTE7fccktcddVVERG+n5OKzs7O+N3vftd//qMf/SiuuOIKP2sWiUxy4KcsxtzOnTvjc5/7XMybNy92794dV199dVRWVha6LMahpqameMc73hGVlZVx1llnxRVXXBEnnXRSRBinFMbevXvj1ltvjUsvvTS++MUvxl/91V/F9OnTDzken3zyyfjyl78cRx11VCRJEl/84hf7n6wC+TDcuGxraxvx8zPCZyhj7+tf/3pcccUVA9pe97rXxcMPP3zIz8Xf/OY3cfPNN8eMGTNi5syZcdlllxWifErYSGPz29/+dlxwwQUxZ86cWLhwYSxbtiwaGxv7+/h+zlh78MEH49xzz43jjjsu3vSmN8WCBQvi4osvjohDf982NtMhlAIAAAAgdZbvAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJC6/x/fx/LYsx1j2gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The plots reveal that the introduction of AVs into urban traffic influences human agents' decision-making. This insight highlights the need for research aimed at mitigating potential negative effects of AV introduction, such as increased human travel times, congestion, and subsequent rises in $CO_2$ emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  |\n",
    "|---------|---------|\n",
    "| **Action shifts of human and AV agents** ![](plots_saved/mappo_actions_shifts.png) | **Action shifts of all vehicles in the network** ![](plots_saved/mappo_actions.png) |\n",
    "| ![](plots_saved/mappo_rewards.png) | ![](plots_saved/mappo_travel_times.png) |\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"plots_saved/mappo_tt_dist.png\" width=\"700\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interrupt the connection with `SUMO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
