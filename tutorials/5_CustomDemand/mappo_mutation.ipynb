{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating fleets of automated vehicles (AVs) making routing decisions: Medium traffic network, AV behaviors, IPPO/MAPPO algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This tutorial is based on [Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial](https://pytorch.org/rl/stable/tutorials/multiagent_ppo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "from torchrl.envs.transforms import TransformedEnv, RewardSum\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "\n",
    "# Now you can import the module\n",
    "from routerl import TrafficEnvironment\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"device is: \", device)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 200  # Number of team frames collected per training iteration\n",
    "n_iters = 10 # Number of sampling and training iterations - the episodes the plotter plots\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 1  # Number of optimization steps per training iteration\n",
    "minibatch_size = 2  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4 # Learning rate\n",
    "max_grad_norm = 3.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "policy_network_depth=3\n",
    "policy_network_num_cells = 64\n",
    "\n",
    "critic_network_depth=3\n",
    "critic_network_num_cells = 64\n",
    "\n",
    "# Human learning phase\n",
    "human_learning_episodes = 2\n",
    "new_machines_after_mutation = 100\n",
    "\n",
    "test_eps = 5\n",
    "\n",
    "# number of episodes the AV training will take\n",
    "training_episodes = (frames_per_batch / new_machines_after_mutation) * n_iters\n",
    "\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"new_machines_after_mutation\": new_machines_after_mutation,\n",
    "\n",
    "        \"human_parameters\" :\n",
    "        {\n",
    "            \"model\" : \"gawron\",\n",
    "\n",
    "            \"noise_weight_agent\" : 0,\n",
    "            \"noise_weight_path\" : 0.8,\n",
    "            \"noise_weight_day\" : 0.2,\n",
    "\n",
    "            \"beta\" : -1,\n",
    "            \"beta_k_i_variability\" : 0.1,\n",
    "            \"epsilon_i_variability\" : 0.1,\n",
    "            \"epsilon_k_i_variability\" : 0.1,\n",
    "            \"epsilon_k_i_t_variability\" : 0.1,\n",
    "\n",
    "            \"greedy\" : 0.1,\n",
    "            \"gamma_c\" : 0.0,\n",
    "            \"gamma_u\" : 0.0,\n",
    "            \"remember\" : 1,\n",
    "\n",
    "            \"alpha_zero\" : 0.8,\n",
    "            \"alphas\" : [0.2]  \n",
    "        },\n",
    "        \"machine_parameters\" :\n",
    "        {\n",
    "            \"behavior\" : \"selfish\",\n",
    "        }\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"saint_arnoult\",\n",
    "        \"custom_network_folder\" : \"custom_networks/saint_arnoult\",\n",
    "        \"sumo_type\" : \"sumo\",\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes, int(training_episodes) + human_learning_episodes],\n",
    "        \"smooth_by\" : 50,\n",
    "        \"plot_choices\" : \"basic\",\n",
    "        \"phase_names\" : [\n",
    "            \"Human learning\", \n",
    "            \"Mutation - Machine learning\",\n",
    "            \"Testing phase\"\n",
    "        ],\n",
    "        \"records_folder\" : \"records/saint_arnoult_records\",\n",
    "        \"plots_folder\" : \"plots/saint_arnoult_plots\"\n",
    "    },\n",
    "    \"path_generation_parameters\":\n",
    "    {\n",
    "        \"origins\" : ['-42762428#0', '-1323419247', '71324347#2', '-100525445', '-282689981#0', '-101607967#0', '-101594809#0', '-659278038#3', '-101609498#5', '-120511302', '101416508#0', '-47374680#7', '336863934', '100475365#1', '-101611601', '-101604496#1', '101594836#1', '659281081#1', '-297823021', '-282689985#1', '526438862#2', '71324347#0', '297823019#4', '-47374665#1', '-297823024', '-100488715#4', '868087112', '75421017', '416409192#0', '-282689975#2', '120511932#0', '-101604513#1', '-100468675#0', '-416409190', '297823019#3', '-297823017', '-100468681#0', '-101600741#2', '-101608828', '-101594843', '-101787489', '-1323419243', '-101601559', '120511930', '659283000#1', '526439431#0', '-71324343#4', '-101607098#1', '101611600', '526438862#0', '101417074', '71324343#3', '-101411571#0', '-101749462', '-42762428#3', '71324347#5', '101611060', '-101415865#0', '-100525438#2', '-101418584#1', '71223791#0', '-71223800#7', '120509991', '-1006827381', '-416375813', '-71324343#5', '-101601551', '-416409191#0', '101606547#1', '-101746498#2', '-101601553#1', '71324347#1', '-101746516#2', '1164653913', '71324347#3', '-101415864#1', '71223791#1', '-679068326#3', '-101746498#0', '463830226#1', '-101765339#0', '-101787498', '101784499#1', '101418583', '-47374664', '-101749451', '47374680#2', '619185406', '-101746498#1', '-1200809291', '-282689986', '-619184994', '-101608960', '-100468681#2', '-100488715#6', '-659278038#0', '-100488715#3', '100525438#0', '101607969#1', '-101604497', '-336860316#0', '120829754', '1200809292', '659278036#2', '71223800#6', '526438862#1', '-101752975#1', '336863927#3', '101607964#0', '-101605951#3', '101594822', '-1323419242#1', '101607969#0', '-101606549', '-101605951#5', '416409192#5', '76867740', '-101765343', '-416375814', '-352797377', '-71223800#4', '-479315694', '-336860317#0', '-101606546', '101416508#1', '-101749465', '-282689981#1', '-282689975#3', '-336860317#1', '336863927#0', '1323419244', '120509990', '120511301', '-101746516#0', '101605951#0', '416405090', '-101767911', '-101752970#1', '-101754276#1', '101765339#2', '-416409191#1', '-101784793', '659281081#0', '-101749449#0', '-173532317#1'],\n",
    "        \"destinations\" : ['-100468681#2', '464265154', '416373452', '101606547#1', '-352797377', '-100468681#0', '-101787493', '-101752975#1', '282689983#1', '-372606529#4', '101418583', '526438862#2', '100475365#1', '336863934', '-101418584#1', '-101746498#2', '-101754276#1', '-101608959', '-1323419243', '-101787498', '101767915#1', '-101611601', '-100468675#0', '-416409191#0', '-101594836#0', '-101604513#1', '-71324343#0', '-659283000#3', '-336863927#1', '-619184994', '-101787489', '-101746516#0', '-101601832#0', '100525438#0', '-1200809291', '-101599076#0', '71324347#3', '-101749458#0', '463830226#1', '-47374664', '-1047412913', '-100525438#2', '-101415865#0', '101607963', '416428068', '-101746508#0', '-101601551', '120509990', '-100468681#1', '526438862#1', '-101418584#0', '868087112', '-1323419247', '71324343#3', '-416412034', '-71223800#1', '1047412912#0', '71223791#1', '120509991', '-120511144#2', '-101752970#2', '-282690560#0', '282479125#1', '-101600741#2', '-101752975#2', '71324347#5', '659281081#1', '-101609498#0', '1164659926', '-101608828', '-101604518', '-1323419242#1', '101608824', '101754278#1', '-23887076#2', '526438862#0', '120829754', '-101605951#5', '-416409192#7', '101417074', '-120511144#0', '-100488715#7', '-101609494#0', '-101606549', '-101767922', '-837309289#1', '-416375814', '-101606546', '-336860317#0', '-101411571#1', '1323419244', '-71223800#4', '-101746514', '-101418589'],\n",
    "        \"number_of_paths\" : 4,\n",
    "        \"beta\" : -5,\n",
    "        \"num_samples\" : 10,\n",
    "        \"visualize_paths\" : False\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, the environment initially contains only human agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If the paths are already created then create_paths=False, we don't have to create again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrafficEnvironment(seed=42, create_agents=False, create_paths=False, save_detectors_info=False, **env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.start()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for human in env.human_agents:\n",
    "\n",
    "    inverse = 1 / np.array(human.initial_knowledge)\n",
    "    invserse_normalized = inverse / inverse.sum()\n",
    "\n",
    "    indices = np.arange(len(human.initial_knowledge))\n",
    "    human.default_action = np.random.choice(indices, size=1, p=invserse_normalized)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mutation**: a portion of human agents are converted into machine agents (autonomous vehicles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `TorchRL` enables us to make different groups with different agents. Here, all the AV agents are included in one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = {'agents': [str(machine.id) for machine in env.machine_agents]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True, # Whether to use the mask in the outputs. It is important for AEC environments to mask out non-acting agents.\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False, # Whether the environment’s done keys are set by aggregating the agent keys using any() (when True) or all() (when False).\n",
    "    group_map=group,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code style=\"color:white\">check_env_specs()</code> function runs a small rollout and compared it output against the environment specs. It will raise an error if the specs aren't properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env_specs(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy/Actor network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = False \n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs = env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs = env.action_spec.space.n,\n",
    "        n_agents = env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=policy_network_depth,\n",
    "        num_cells=policy_network_num_cells,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"logits\")],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[(\"agents\", \"logits\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=Categorical,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The critic reads the observations and returns the corresponding value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1, \n",
    "    n_agents=env.n_agents,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=critic_network_depth,\n",
    "    num_cells=critic_network_num_cells,\n",
    "    activation_class=torch.nn.ReLU,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"state_value\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    reset_at_each_iter=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  \n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,\n",
    ")\n",
    "loss_module.set_keys( \n",
    "    reward=env.reward_key,  \n",
    "    action=env.action_key, \n",
    "    sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "    value=(\"agents\", \"state_value\"),\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ") \n",
    "\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "loss_values = []\n",
    "loss_entropy = []\n",
    "loss_objective = []\n",
    "loss_critic = []\n",
    "\n",
    "for tensordict_data in collector: ##loops over frame_per_batch\n",
    "\n",
    "    ## Generate the rollouts\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),  # Adjust index to start from 0\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),  # Adjust index to start from 0\n",
    "    )\n",
    "\n",
    "    # Compute GAE for all agents\n",
    "    with torch.no_grad():\n",
    "            GAE(\n",
    "                tensordict_data,\n",
    "                params=loss_module.critic_network_params,\n",
    "                target_params=loss_module.target_critic_network_params,\n",
    "            )\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  \n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    ## Update the policies of the learning agents\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            ) \n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss_values.append(loss_value.item())\n",
    "\n",
    "            loss_entropy.append(loss_vals[\"loss_entropy\"].item())\n",
    "\n",
    "            loss_objective.append(loss_vals[\"loss_objective\"].item())\n",
    "\n",
    "            loss_critic.append(loss_vals[\"loss_critic\"].item())\n",
    "\n",
    "\n",
    "   \n",
    "    collector.update_policy_weights_()\n",
    "   \n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))  # Get done status for the group\n",
    "\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "\n",
    "\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "\n",
    "for episode in range(test_eps):\n",
    "    env.rollout(len(env.machine_agents), policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Check `\\plots` directory to find the plots created from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The plots reveal that the introduction of AVs into urban traffic influences human agents' decision-making. This insight highlights the need for research aimed at mitigating potential negative effects of AV introduction, such as increased human travel times, congestion, and subsequent rises in $CO_2$ emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  |\n",
    "|---------|---------|\n",
    "| **Action shifts of human and AV agents** ![](plots_saved/mappo_actions_shifts.png) | **Action shifts of all vehicles in the network** ![](plots_saved/mappo_actions.png) |\n",
    "| ![](plots_saved/mappo_rewards.png) | ![](plots_saved/mappo_travel_times.png) |\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"plots_saved/mappo_tt_dist.png\" width=\"700\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interrupt the connection with `SUMO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
