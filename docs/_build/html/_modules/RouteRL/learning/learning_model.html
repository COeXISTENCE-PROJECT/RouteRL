<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>RouteRL.learning.learning_model &mdash; RouteRL 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            RouteRL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../modules.html">RouteRL</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">RouteRL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">RouteRL.learning.learning_model</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for RouteRL.learning.learning_model</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="kn">from</span> <span class="nn">keychain</span> <span class="kn">import</span> <span class="n">Keychain</span> <span class="k">as</span> <span class="n">kc</span>
<span class="kn">from</span> <span class="nn">..utilities</span> <span class="kn">import</span> <span class="n">list_to_string</span>



<div class="viewcode-block" id="BaseLearningModel"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.BaseLearningModel">[docs]</a><span class="k">class</span> <span class="nc">BaseLearningModel</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an abstract base class for the learning models used to train the human and machine agents.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

<div class="viewcode-block" id="BaseLearningModel.act"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.BaseLearningModel.act">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="BaseLearningModel.learn"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.BaseLearningModel.learn">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="k">pass</span></div></div>



<span class="c1">############## Human Learning ##############</span>


<div class="viewcode-block" id="Gawron"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Gawron">[docs]</a><span class="k">class</span> <span class="nc">Gawron</span><span class="p">(</span><span class="n">BaseLearningModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">initial_knowledge</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Gawron learning model.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        params (dict): A dictionary containing model parameters.</span>
<span class="sd">        initial_knowledge (list or array): Initial knowledge or costs for actions.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Extract beta with added randomness</span>
        <span class="n">beta_randomness</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA_RANDOMNESS</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA</span><span class="p">]</span> <span class="o">-</span> <span class="n">beta_randomness</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_randomness</span><span class="p">)</span>

        <span class="c1"># Learning rate components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_zero</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">ALPHA_ZERO</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_sigma</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_zero</span>

        <span class="c1"># Initialize cost array with initial knowledge</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">initial_knowledge</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

<div class="viewcode-block" id="Gawron.act"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Gawron.act">[docs]</a>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Selects an action based on the current state and cost.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state: The current state of the environment.</span>

<span class="sd">        Returns:</span>
<span class="sd">        int: The index of the selected action.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">utilities</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">))</span>
        <span class="n">action</span> <span class="o">=</span>  <span class="n">utilities</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">utilities</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">action</span>   </div>

<div class="viewcode-block" id="Gawron.learn"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Gawron.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the cost associated with the taken action based on the received reward.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state: The current state of the environment.</span>
<span class="sd">        action (int): The action that was taken.</span>
<span class="sd">        reward (float): The reward received after taking the action.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_sigma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">[</span><span class="n">action</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_zero</span> <span class="o">*</span> <span class="n">reward</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Culo"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Culo">[docs]</a><span class="k">class</span> <span class="nc">Culo</span><span class="p">(</span><span class="n">BaseLearningModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">initial_knowledge</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Culo learning model.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        params (dict): A dictionary containing model parameters.</span>
<span class="sd">        initial_knowledge (list or array): Initial knowledge or costs for actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Extract beta with randomness</span>
        <span class="n">beta_randomness</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA_RANDOMNESS</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA</span><span class="p">]</span> <span class="o">-</span> <span class="n">beta_randomness</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_randomness</span><span class="p">)</span>

        <span class="c1"># Learning rate components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_zero</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_sigma</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">ALPHA_SIGMA</span><span class="p">]</span>

        <span class="c1"># Initialize cost array with initial knowledge</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">initial_knowledge</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

<div class="viewcode-block" id="Culo.act"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Culo.act">[docs]</a>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Selects an action based on the current state and cost.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state: The current state of the environment.</span>

<span class="sd">        Returns:</span>
<span class="sd">        int: The index of the selected action.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">utilities</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">))</span>
        <span class="n">action</span> <span class="o">=</span>  <span class="n">utilities</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">utilities</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">action</span>   </div>

<div class="viewcode-block" id="Culo.learn"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Culo.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the cost associated with the taken action based on the received reward.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state: The current state of the environment.</span>
<span class="sd">        action (int): The action that was taken.</span>
<span class="sd">        reward (float): The reward received after taking the action.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_sigma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">[</span><span class="n">action</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_zero</span> <span class="o">*</span> <span class="n">reward</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="WeightedAverage"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.WeightedAverage">[docs]</a><span class="k">class</span> <span class="nc">WeightedAverage</span><span class="p">(</span><span class="n">BaseLearningModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">initial_knowledge</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">beta_randomness</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA_RANDOMNESS</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA</span><span class="p">]</span> <span class="o">-</span> <span class="n">beta_randomness</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BETA</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_randomness</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_zero</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_sigma</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">ALPHA_SIGMA</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">initial_knowledge</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

<div class="viewcode-block" id="WeightedAverage.act"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.WeightedAverage.act">[docs]</a>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">utilities</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">))</span>
        <span class="n">action</span> <span class="o">=</span>  <span class="n">utilities</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">utilities</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">action</span>   </div>

    <span class="c1">#### Check with Onur to save previous reward</span>
<div class="viewcode-block" id="WeightedAverage.learn"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.WeightedAverage.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_sigma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="p">[</span><span class="n">action</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_zero</span> <span class="o">*</span> <span class="n">reward</span><span class="p">)</span></div></div>



<span class="c1">############## Machine Learning ##############</span>



<div class="viewcode-block" id="QLearning"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.QLearning">[docs]</a><span class="k">class</span> <span class="nc">QLearning</span><span class="p">(</span><span class="n">BaseLearningModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">action_space_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Q-learning model.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        params (dict): A dictionary containing model parameters.</span>
<span class="sd">        action_space_size (int): The number of available actions in the environment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Learning parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">ALPHA</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">EPSILON</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay_rate</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">EPSILON_DECAY_RATE</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space_size</span> <span class="o">=</span> <span class="n">action_space_size</span>

        <span class="c1"># Initialize Q-table with zero values for each action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_q_table_row</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">action_space_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">STATE</span><span class="p">,</span> <span class="n">kc</span><span class="o">.</span><span class="n">Q_TABLE</span><span class="p">])</span>


<div class="viewcode-block" id="QLearning.act"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.QLearning.act">[docs]</a>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Selects an action based on the current state using the epsilon-greedy strategy.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state (list): The current state of the environment.</span>

<span class="sd">        Returns:</span>
<span class="sd">        int: The index of the selected action.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">list_to_string</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ensure_row_in_q_table</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>    <span class="c1"># Explore</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>    <span class="c1"># Exploit</span>
            <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">STATE</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">,</span> <span class="n">kc</span><span class="o">.</span><span class="n">Q_TABLE</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span></div>


<div class="viewcode-block" id="QLearning.learn"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.QLearning.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the Q-values for the given state-action pair based on the reward.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state (list): The current state of the environment.</span>
<span class="sd">        action (int): The action that was taken.</span>
<span class="sd">        reward (float): The reward received after taking the action.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">list_to_string</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>
        <span class="n">q_row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">STATE</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">,</span> <span class="n">kc</span><span class="o">.</span><span class="n">Q_TABLE</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">prev_knowledge</span> <span class="o">=</span> <span class="n">q_row</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">q_row</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_knowledge</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">prev_knowledge</span><span class="p">))</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">()</span></div>


<div class="viewcode-block" id="QLearning.ensure_row_in_q_table"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.QLearning.ensure_row_in_q_table">[docs]</a>    <span class="k">def</span> <span class="nf">ensure_row_in_q_table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Ensures the state is present in the Q-table. If not, a new row is added.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state_str (str): The string representation of the current state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">STATE</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span> <span class="o">=</span> <span class="p">{</span><span class="n">kc</span><span class="o">.</span><span class="n">STATE</span><span class="p">:</span> <span class="n">state</span><span class="p">,</span> <span class="n">kc</span><span class="o">.</span><span class="n">Q_TABLE</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_q_table_row</span><span class="p">}</span></div>
      
<div class="viewcode-block" id="QLearning.decay_epsilon"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.QLearning.decay_epsilon">[docs]</a>    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decays the epsilon value to reduce exploration over time.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay_rate</span></div></div>



<div class="viewcode-block" id="DQN"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.DQN">[docs]</a><span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">BaseLearningModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_space_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Deep Q-learning Network (DQN).</span>

<span class="sd">        Parameters:</span>
<span class="sd">        params (dict): A dictionary containing model parameters.</span>
<span class="sd">        state_size (int): The dimensionality of the state space.</span>
<span class="sd">        action_space_size (int): The number of available actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="c1"># Environment and model parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space_size</span> <span class="o">=</span> <span class="n">action_space_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">EPSILON</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay_rate</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">EPSILON_DECAY_RATE</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BUFFER_SIZE</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">LEARNING_RATE</span><span class="p">]</span>
        <span class="n">num_hidden</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">NUM_HIDDEN</span><span class="p">]</span>
        <span class="n">widths</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">kc</span><span class="o">.</span><span class="n">WIDTHS</span><span class="p">]</span>

        <span class="c1"># Initialize Q-network and optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span> <span class="o">=</span> <span class="n">Network</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space_size</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">widths</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<div class="viewcode-block" id="DQN.act"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.DQN.act">[docs]</a>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Selects an action based on the current state using an epsilon-greedy strategy.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state (array-like): The current state of the environment.</span>

<span class="sd">        Returns:</span>
<span class="sd">        int: The index of the selected action.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span></div>

<div class="viewcode-block" id="DQN.learn"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.DQN.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Samples a batch from memory, computes the loss, and updates the Q-network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="k">return</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>

        <span class="n">states_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">actions_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">rewards_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">current_q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">states_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions_tensor</span><span class="p">)</span>
        <span class="n">target_q_values</span> <span class="o">=</span> <span class="n">rewards_tensor</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">current_q_values</span><span class="p">,</span> <span class="n">target_q_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span></div>

<div class="viewcode-block" id="DQN.decay_epsilon"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.DQN.decay_epsilon">[docs]</a>    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decays the epsilon value to reduce exploration over time.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay_rate</span></div></div>





<div class="viewcode-block" id="Network"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Network">[docs]</a><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_space_size</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">widths</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the neural network for the Deep Q-Learning Network (DQN).</span>

<span class="sd">        Parameters:</span>
<span class="sd">        state_size (int): The dimensionality of the input state space.</span>
<span class="sd">        action_space_size (int): The number of available actions (output size).</span>
<span class="sd">        num_hidden (int): The number of hidden layers in the network.</span>
<span class="sd">        widths (list): A list defining the width of each hidden layer and the input layer.</span>
<span class="sd">                       The length of the list must match num_hidden + 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">widths</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_hidden</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;DQN widths and number of layers mismatch!&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">widths</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">widths</span><span class="p">[</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">action_space_size</span><span class="p">)</span>

<div class="viewcode-block" id="Network.forward"><a class="viewcode-back" href="../../../RouteRL.learning.html#RouteRL.learning.learning_model.Network.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the network.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        x (Tensor): The input tensor representing the state.</span>

<span class="sd">        Returns:</span>
<span class="sd">        Tensor: The output tensor representing Q-values for each action.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">hidden_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Anastasia Psarou, Ahmet Onur Akman.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>