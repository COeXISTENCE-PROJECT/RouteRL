{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RouteRL Quickstart\n",
    "\n",
    "We simulate a simple network topology where humans and later AVs make routing decisions to maximize their rewards (i.e., minimize travel times) over a sequence of days.\n",
    "\n",
    "* For the first 100 days, we model a human-driven system, where drivers update their routing policies using behavioral models to optimize rewards.\n",
    "* Each day, we simulate the impact of joint actions using the [`SUMO`](https://eclipse.dev/sumo/) traffic simulator, which returns the reward for each agent.\n",
    "* After 100 days, we introduce 10 `Autononmous Vehicles` (AVs) as `Petting Zoo` agents, allowing them to use any `MARL` algorithm to maximise rewards. In this tutorial, we use a trained policy from the Independent Deep Q-Learning (IDQN) algorithm.\n",
    "* Finally, we analyse basic results from the simulation.\n",
    "\n",
    "---\n",
    "\n",
    "* Establishing the Connection with SUMO\n",
    "\n",
    "* Initializing the Traffic Environment\n",
    "  - Define the `TrafficEnvironment`, which initializes human agents and generates the routes agents will travel within the network.\n",
    "\n",
    "* Training Human Agents\n",
    "  - Train human-driven vehicles to navigate the environment efficiently using human behavioural models from transportation research.\n",
    "\n",
    "* Introducing Autonomous Vehicles (AVs)\n",
    "  - Transform a subset of human agents into AVs.\n",
    "  - AVs select their routes using a pre-trained policy based on the IDQN algorithm.\n",
    "\n",
    "* Analyzing the Impact of AVs\n",
    "  - Evaluate the effects of AV introduction on human travel time, congestion, and CO₂ emissions.\n",
    "  - Demonstrate how AV deployment can potentially increase travel delays and environmental impact.\n",
    "\n",
    "**In this repository we don't recommend adjusting the number of agents, because the policy is trained for 10 AV agents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../_static/two_route_net_1.png\" alt=\"Two-route network\" />\n",
    "</p> \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../_static/two_route_net_1_2.png\" alt=\"Two-route network\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "\n",
    "from routerl import TrafficEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters\n",
    "\n",
    "> Users can customize parameters for the `TrafficEnvironment` class by consulting the [`routerl/environment/params.json`](https://github.com/COeXISTENCE-PROJECT/RouteRL/blob/4f4bc0a90d821e95b7193b00c93d6aaf10b34f41/routerl/environment/params.json) file. Based on its contents, they can create a dictionary with their preferred settings and pass it as an argument to the `TrafficEnvironment` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_learning_episodes = 100\n",
    "\n",
    "\n",
    "env_params = {\n",
    "    \"agent_parameters\" : {\n",
    "        \"num_agents\" : 100,\n",
    "        \"new_machines_after_mutation\": 10, # the number of human agents that will mutate to AVs\n",
    "        \"human_parameters\" : {\n",
    "            \"model\" : \"gawron\"\n",
    "        },\n",
    "        \"machine_parameters\" :\n",
    "        {\n",
    "            \"behavior\" : \"selfish\",\n",
    "        }\n",
    "    },\n",
    "    \"simulator_parameters\" : {\n",
    "        \"network_name\" : \"two_route_yield\"\n",
    "    },  \n",
    "    \"plotter_parameters\" : {\n",
    "        \"phases\" : [0, human_learning_episodes], # the number of episodes human learning will take\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our setup, road networks initially consist of human agents, with AVs introduced later.\n",
    "\n",
    "- The `TrafficEnvironment` environment is firstly initialized.\n",
    "- The traffic network is instantiated and the paths between designated origin and destination points are determined.\n",
    "- The drivers/agents objects are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIRMED] Environment variable exists: SUMO_HOME\n",
      "[SUCCESS] Added module directory: C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(seed=42, **env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Available paths create using the [Janux](https://github.com/COeXISTENCE-PROJECT/JanuX) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p >\n",
    "  <img src=\"../_static/0_0.png\" width=\"600\" />\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  100 \n",
      "\n",
      "Number of human agents is:  100 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  0 \n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reset the environment and the connection with SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(human_learning_episodes):\n",
    "    env.step() # all the human agents execute an action in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Average travel time of human agents during their training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plots_saved/human_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Show the initial `.csv` file saved that contain the information about the agents available in the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travel_time</th>\n",
       "      <th>id</th>\n",
       "      <th>kind</th>\n",
       "      <th>action</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>start_time</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_right</th>\n",
       "      <th>cost_table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.483333</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>-3.483333</td>\n",
       "      <td>-3.483333</td>\n",
       "      <td>0.2217422606191504,-1.514644828413727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>0.2217422606191504,-0.3729781617470602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.550000</td>\n",
       "      <td>2</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>-4.550000</td>\n",
       "      <td>-4.550000</td>\n",
       "      <td>0.2217422606191504,-2.0479781617470603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.916667</td>\n",
       "      <td>3</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>-4.916667</td>\n",
       "      <td>-4.916667</td>\n",
       "      <td>0.2217422606191504,-2.231311495080394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>4</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.2217422606191504,-0.23964482841372692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.066667</td>\n",
       "      <td>95</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>0.2217422606191504,-0.30631149508039357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.083333</td>\n",
       "      <td>96</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.083333</td>\n",
       "      <td>-1.083333</td>\n",
       "      <td>0.2217422606191504,-0.3146448284137269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.216667</td>\n",
       "      <td>97</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>-1.216667</td>\n",
       "      <td>-1.216667</td>\n",
       "      <td>0.2217422606191504,-0.3813114950803935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3.566667</td>\n",
       "      <td>98</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>-3.566667</td>\n",
       "      <td>-3.566667</td>\n",
       "      <td>0.2217422606191504,-1.5563114950803936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.366667</td>\n",
       "      <td>99</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.366667</td>\n",
       "      <td>-1.366667</td>\n",
       "      <td>0.2217422606191504,-0.4563114950803936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    travel_time  id   kind  action  origin  destination  start_time    reward  \\\n",
       "0      3.483333   0  Human       1       0            0          99 -3.483333   \n",
       "1      1.200000   1  Human       1       0            0          58 -1.200000   \n",
       "2      4.550000   2  Human       1       0            0         112 -4.550000   \n",
       "3      4.916667   3  Human       1       0            0         118 -4.916667   \n",
       "4      0.933333   4  Human       1       0            0          31 -0.933333   \n",
       "..          ...  ..    ...     ...     ...          ...         ...       ...   \n",
       "95     1.066667  95  Human       1       0            0          46 -1.066667   \n",
       "96     1.083333  96  Human       1       0            0          50 -1.083333   \n",
       "97     1.216667  97  Human       1       0            0          60 -1.216667   \n",
       "98     3.566667  98  Human       1       0            0         101 -3.566667   \n",
       "99     1.366667  99  Human       1       0            0          62 -1.366667   \n",
       "\n",
       "    reward_right                               cost_table  \n",
       "0      -3.483333    0.2217422606191504,-1.514644828413727  \n",
       "1      -1.200000   0.2217422606191504,-0.3729781617470602  \n",
       "2      -4.550000   0.2217422606191504,-2.0479781617470603  \n",
       "3      -4.916667    0.2217422606191504,-2.231311495080394  \n",
       "4      -0.933333  0.2217422606191504,-0.23964482841372692  \n",
       "..           ...                                      ...  \n",
       "95     -1.066667  0.2217422606191504,-0.30631149508039357  \n",
       "96     -1.083333   0.2217422606191504,-0.3146448284137269  \n",
       "97     -1.216667   0.2217422606191504,-0.3813114950803935  \n",
       "98     -3.566667   0.2217422606191504,-1.5563114950803936  \n",
       "99     -1.366667   0.2217422606191504,-0.4563114950803936  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"training_records/episodes/ep1.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mutation: a portion of human agents are converted into machine agents (autonomous vehicles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total agents is:  100 \n",
      "\n",
      "Number of human agents is:  90 \n",
      "\n",
      "Number of machine agents (autonomous vehicles) is:  10 \n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total agents is: \", len(env.all_agents), \"\\n\")\n",
    "print(\"Number of human agents is: \", len(env.human_agents), \"\\n\")\n",
    "print(\"Number of machine agents (autonomous vehicles) is: \", len(env.machine_agents), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Machine 1,\n",
       " Machine 15,\n",
       " Machine 10,\n",
       " Machine 91,\n",
       " Machine 22,\n",
       " Machine 73,\n",
       " Machine 5,\n",
       " Machine 52,\n",
       " Machine 81,\n",
       " Machine 77]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.machine_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In order to employ the `TorchRL` library in our environment we need to use their `PettingZooWrapper` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = {'agents': [str(machine.id) for machine in env.machine_agents]}\n",
    "\n",
    "env = PettingZooWrapper(\n",
    "    env=env,\n",
    "    use_mask=True,\n",
    "    categorical_actions=True,\n",
    "    done_on_any = False,\n",
    "    group_map=group,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use an already trained policy using the Independent Deep Q-Learning algorith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_explore = torch.load(\"trained_policy.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Human and AV agents interact with the environment over multiple episodes, with AVs following a trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episodes = 100\n",
    "\n",
    "for episode in range(num_test_episodes): # run rollous in the environment using the already trained policy\n",
    "    env.rollout(len(env.machine_agents), policy=qnet_explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Show the first `.csv` file saved after the mutation that contains the information about the agents available in the system after the mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travel_time</th>\n",
       "      <th>id</th>\n",
       "      <th>kind</th>\n",
       "      <th>action</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>start_time</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_right</th>\n",
       "      <th>cost_table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>AV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.100000</td>\n",
       "      <td>15</td>\n",
       "      <td>AV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.783333</td>\n",
       "      <td>10</td>\n",
       "      <td>AV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>-3.783333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.383333</td>\n",
       "      <td>91</td>\n",
       "      <td>AV</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>-2.383333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.900000</td>\n",
       "      <td>22</td>\n",
       "      <td>AV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>-3.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>95</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>-0.533333</td>\n",
       "      <td>-0.583333</td>\n",
       "      <td>-0.5833333333333333,-0.6864890808735301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>96</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.533333</td>\n",
       "      <td>-0.583333</td>\n",
       "      <td>-0.5833333333333333,-0.6989890808735301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>97</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>-0.883333</td>\n",
       "      <td>-0.683333</td>\n",
       "      <td>-0.6833333333333333,-0.79898908087353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2.983333</td>\n",
       "      <td>98</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>-2.983333</td>\n",
       "      <td>-2.916667</td>\n",
       "      <td>-2.916666666666667,-3.036205603551716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>99</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>-0.966667</td>\n",
       "      <td>-0.816667</td>\n",
       "      <td>-0.8166666666666667,-0.9114890808735301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    travel_time  id   kind  action  origin  destination  start_time    reward  \\\n",
       "0      0.700000   1     AV       0       0            0          58 -0.700000   \n",
       "1      1.100000  15     AV       0       0            0          64 -1.100000   \n",
       "2      3.783333  10     AV       0       0            0         116 -3.783333   \n",
       "3      2.383333  91     AV       1       0            0          87 -2.383333   \n",
       "4      3.900000  22     AV       0       0            0         126 -3.900000   \n",
       "..          ...  ..    ...     ...     ...          ...         ...       ...   \n",
       "95     0.533333  95  Human       0       0            0          46 -0.533333   \n",
       "96     0.533333  96  Human       0       0            0          50 -0.533333   \n",
       "97     0.883333  97  Human       0       0            0          60 -0.883333   \n",
       "98     2.983333  98  Human       0       0            0         101 -2.983333   \n",
       "99     0.966667  99  Human       0       0            0          62 -0.966667   \n",
       "\n",
       "    reward_right                               cost_table  \n",
       "0            NaN                                      0,0  \n",
       "1            NaN                                      0,0  \n",
       "2            NaN                                      0,0  \n",
       "3            NaN                                      0,0  \n",
       "4            NaN                                      0,0  \n",
       "..           ...                                      ...  \n",
       "95     -0.583333  -0.5833333333333333,-0.6864890808735301  \n",
       "96     -0.583333  -0.5833333333333333,-0.6989890808735301  \n",
       "97     -0.683333    -0.6833333333333333,-0.79898908087353  \n",
       "98     -2.916667    -2.916666666666667,-3.036205603551716  \n",
       "99     -0.816667  -0.8166666666666667,-0.9114890808735301  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"training_records/episodes/ep101.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results \n",
    "\n",
    ">This will be shown in the `\\plots` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results highlight a critical challenge in AV deployment: rather than improving traffic flow, AVs may increase travel time for human drivers. This suggests potential inefficiencies in mixed traffic conditions due to differences in driving behavior. Understanding these effects is essential for designing better reinforcement learning strategies, informing policymakers, and optimizing AV integration to prevent increased congestion and CO₂ emissions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                                                              |  |\n",
    "|------------------------------------------------------------------------------|---------|\n",
    "| **Action shifts of human and AV agents** ![](plots_saved/actions_shifts.png) | **Action shifts of all vehicles in the network** ![](plots_saved/actions.png) |\n",
    "| ![](plots_saved/rewards.png)                                                 | ![](plots_saved/travel_times.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](plots_saved/tt_dist.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interrupt the connection with `SUMO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
